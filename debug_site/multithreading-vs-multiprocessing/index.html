<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" data-mode="light">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Multithreading vs Multiprocessing — Choosing the Right Concurrency Model" />
<meta property="og:locale" content="en" />
<meta name="description" content="A practical guide comparing multithreading and multiprocessing (Python focus): GIL, I/O- vs CPU-bound workloads, examples, pitfalls, and decision checklist." />
<meta property="og:description" content="A practical guide comparing multithreading and multiprocessing (Python focus): GIL, I/O- vs CPU-bound workloads, examples, pitfalls, and decision checklist." />
<link rel="canonical" href="https://mr901.github.io/posts/multithreading-vs-multiprocessing/" />
<meta property="og:url" content="https://mr901.github.io/posts/multithreading-vs-multiprocessing/" />
<meta property="og:site_name" content="Posts by MR901" />
<meta property="og:image" content="https://mr901.github.io/posts/attachments/posts/2025-10-27-multithreading-vs-multiprocessing/images/multi-threading-multi-processing.png" />
<meta property="og:image:alt" content="Multithreading vs Multiprocessing overview" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-27T00:00:00+05:30" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://mr901.github.io/posts/attachments/posts/2025-10-27-multithreading-vs-multiprocessing/images/multi-threading-multi-processing.png" />
<meta name="twitter:image:alt" content="Multithreading vs Multiprocessing overview" />
<meta property="twitter:title" content="Multithreading vs Multiprocessing — Choosing the Right Concurrency Model" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-27T00:00:00+05:30","datePublished":"2025-10-27T00:00:00+05:30","description":"A practical guide comparing multithreading and multiprocessing (Python focus): GIL, I/O- vs CPU-bound workloads, examples, pitfalls, and decision checklist.","headline":"Multithreading vs Multiprocessing — Choosing the Right Concurrency Model","image":{"alt":"Multithreading vs Multiprocessing overview","url":"https://mr901.github.io/posts/attachments/posts/2025-10-27-multithreading-vs-multiprocessing/images/multi-threading-multi-processing.png","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mr901.github.io/posts/multithreading-vs-multiprocessing/"},"url":"https://mr901.github.io/posts/multithreading-vs-multiprocessing/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Multithreading vs Multiprocessing — Choosing the Right Concurrency Model | Posts by MR901
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/posts/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/posts/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/posts/assets/img/favicons/favicon-16x16.png">

<link rel="shortcut icon" href="/posts/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Posts by MR901">
<meta name="application-name" content="Posts by MR901">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/posts/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Resource Hints -->
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/posts/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="/posts/assets/lib/fonts/main.css">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="/posts/assets/lib/fontawesome-free/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="/posts/assets/lib/tocbot/tocbot.min.css">
  

  
    <link rel="stylesheet" href="/posts/assets/lib/loading-attribute-polyfill/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="/posts/assets/lib/glightbox/glightbox.min.css">
  

  <!-- Scripts -->

  <script src="/posts/assets/js/dist/theme.min.js"></script>

  <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    <script defer src="/posts/assets/lib/simple-jekyll-search/simple-jekyll-search.min.js"></script>
  

  
    <script defer src="/posts/assets/lib/loading-attribute-polyfill/loading-attribute-polyfill.umd.min.js"></script>
  

  
    <script defer src="/posts/assets/lib/glightbox/glightbox.min.js"></script>
  

  
    <script defer src="/posts/assets/lib/clipboard/clipboard.min.js"></script>
  

  
    <script defer src="/posts/assets/lib/dayjs/dayjs.min.js"></script>
  

  
    <script defer src="/posts/assets/lib/dayjs/locale/en.js"></script>
  

  
    <script defer src="/posts/assets/lib/dayjs/plugin/relativeTime.js"></script>
  

  
    <script defer src="/posts/assets/lib/dayjs/plugin/localizedFormat.js"></script>
  

  
    <script defer src="/posts/assets/lib/tocbot/tocbot.min.js"></script>
  

  
    <script defer src="/posts/assets/lib/mermaid/mermaid.min.js"></script>
  









<script defer src="/posts/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script src="/posts/assets/js/data/mathjax.js"></script>
  <script async src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="/posts/assets/lib/mathjax/tex-chtml.js"></script>


<!-- Pageviews -->

  

  



  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/posts/" id="avatar" class="rounded-circle"><img src="https://avatars.githubusercontent.com/u/20877166?v=4" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <a class="site-title d-block" href="/posts/">Posts by MR901</a>
    <p class="site-subtitle fst-italic mb-0">Post on Machine Learning</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/posts/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/posts/timeline/" class="nav-link">
            <i class="fa-fw fas fa-clock"></i>
            

            <span>TIMELINE</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/posts/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/posts/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/posts/attachments/" class="nav-link">
            <i class="fa-fw fas fa-paperclip"></i>
            

            <span>ATTACHMENTS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/posts/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    

    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['mohitrajput901','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="https://www.linkedin.com/in/mr901"
          aria-label="linkedin"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-linkedin"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/posts/">Home</a>
            </span>

          
        
          
            
              <span>Multithreading vs Multiprocessing — Choosing the Right Concurrency Model</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link" aria-label="Search">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    
      

      
      
      

      
    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  

  
  

  




<!-- return -->










<article class="px-1" data-toc="true">
  <header>
    <h1 data-toc-skip>Multithreading vs Multiprocessing — Choosing the Right Concurrency Model</h1>
    
      <p class="post-desc fw-light mb-4">A practical guide comparing multithreading and multiprocessing (Python focus): GIL, I/O- vs CPU-bound workloads, examples, pitfalls, and decision checklist.</p>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1761503400"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Oct 27, 2025
</time>

      </span>

      <!-- lastmod date -->
      

      
        
        
        

        

        <div class="mt-3 mb-3">
          <a href="/posts/attachments/posts/2025-10-27-multithreading-vs-multiprocessing/images/multi-threading-multi-processing.png" class="popup img-link preview-img shimmer"><img src="/posts/attachments/posts/2025-10-27-multithreading-vs-multiprocessing/images/multi-threading-multi-processing.png"  alt="Multithreading vs Multiprocessing overview" width="1200" height="630"  loading="lazy"></a><figcaption class="text-center pt-2 pb-2">Multithreading vs Multiprocessing overview</figcaption></div>
      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://github.com/mr901">Mohit Rajput</a>
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="5428 words"
>
  <em>30 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  
    <div id="toc-bar" class="d-flex align-items-center justify-content-between invisible">
      <span class="label text-truncate">Multithreading vs Multiprocessing — Choosing the Right Concurrency Model</span>
      <button type="button" class="toc-trigger btn me-1">
        <i class="fa-solid fa-list-ul fa-fw"></i>
      </button>
    </div>

    <button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm">
      <span class="label ps-2 pe-1">Contents</span>
      <i class="fa-solid fa-angle-right fa-fw"></i>
    </button>

    <dialog id="toc-popup" class="p-0">
      <div class="header d-flex flex-row align-items-center justify-content-between">
        <div class="label text-truncate py-2 ms-4">Multithreading vs Multiprocessing — Choosing the Right Concurrency Model</div>
        <button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75">
          <i class="fas fa-close"></i>
        </button>
      </div>
      <div id="toc-popup-content" class="px-4 py-3 pb-4"></div>
    </dialog>
  

  <div class="content">
    <p>A program that fetches data from twenty APIs one after another might take twenty seconds. The same program, refactored to fetch them concurrently, might finish in two seconds. That transformation — turning sequential waiting into productive overlap — unlocks the potential of modern computing. Yet the path to that speedup splits into two distinct roads: multithreading and multiprocessing. Each path serves different needs, carries different trade-offs, and in Python, confronts a unique challenge called the Global Interpreter Lock.</p>
<p>This article guides you through both approaches with clarity and depth. You will understand when to reach for threads and when to summon processes, how Python's GIL shapes these decisions, and how to combine both techniques when your application demands it. Whether you are making your first concurrent program or optimizing a production system, the principles here will serve as reliable companions.</p>
<div class="section" id="what-you-will-gain">
<h2 id="what-you-will-gain"><span class="me-2">What You Will Gain</span><a href="#what-you-will-gain" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Concurrency can feel mysterious at first — invisible threads and processes doing work in parallel, sharing state, communicating across boundaries. By the end of this guide, you will see concurrency not as magic but as a set of clear, practical patterns:</p>
<ul class="simple">
<li><strong>Recognition</strong>: You will identify whether your workload is I/O-bound or CPU-bound, and understand why this distinction matters fundamentally.</li>
<li><strong>Clarity on the GIL</strong>: Python's Global Interpreter Lock stops being an abstract concept and becomes a concrete constraint you can work with or around.</li>
<li><strong>Practical mastery</strong>: You will write thread pools for network requests, process pools for computational work, and hybrid systems that blend both.</li>
<li><strong>Confidence in trade-offs</strong>: You will navigate memory overhead, serialization costs, deadlocks, and race conditions with informed awareness.</li>
</ul>
<p>The goal is not just competence but fluency — the ability to reason about concurrent systems and make sound architectural choices under real constraints.</p>
</div>
<div class="section" id="at-a-glance-the-essential-distinction">
<h2 id="at-a-glance-the-essential-distinction"><span class="me-2">At a Glance: The Essential Distinction</span><a href="#at-a-glance-the-essential-distinction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Before diving deep, a compass for the journey ahead:</p>
<ul class="simple">
<li><strong>Use multithreading</strong> when your program spends time waiting — for HTTP responses, database queries, file I/O, or any operation where the CPU idles while external systems respond. Python threads excel here because the GIL releases during blocking operations.</li>
<li><strong>Use multiprocessing</strong> when your program spends time computing — parsing massive files, running simulations, transforming images, training models, or crunching numbers. True parallelism across CPU cores demands separate processes, each with its own Python interpreter and memory space.</li>
<li><strong>Combine both</strong> when your application has distinct phases: threads fetch data over the network, then hand payloads to a process pool for intensive analysis.</li>
</ul>
<p>This simple rule — threads for waiting, processes for computing — will guide most of your decisions. Now, let us understand why.</p>
</div>
<div class="section" id="concurrency-vs-parallelism-two-paths-to-speed">
<h2 id="concurrency-vs-parallelism-two-paths-to-speed"><span class="me-2">Concurrency vs Parallelism: Two Paths to Speed</span><a href="#concurrency-vs-parallelism-two-paths-to-speed" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>These terms often appear interchangeable, but they describe fundamentally different approaches to program execution.</p>
<p><strong>Concurrency</strong> is about structure — organizing your program so multiple tasks can make progress without waiting for each other to finish completely. Imagine a chef preparing a meal: while the soup simmers, they chop vegetables; while those roast, they prepare dessert. Only one activity happens at any instant, but tasks overlap in time, and idle moments become productive. This interleaving transforms sequential waiting into fluid progress. A single-core machine can run concurrent programs effectively because concurrency is about managing multiple tasks, not executing them simultaneously.</p>
<p><strong>Parallelism</strong> is about simultaneous execution — running multiple tasks at literally the same moment on different CPU cores. Imagine multiple chefs working side by side, each preparing a different dish independently. True parallelism requires multiple execution units (CPU cores), and it dramatically speeds up compute-intensive work by dividing labor across hardware.</p>
<p>The crucial insight: concurrency enables parallelism but does not guarantee it. Python threads provide concurrency (task interleaving) but not always parallelism (simultaneous execution) because of the GIL. Python processes provide both.</p>
<div class="mermaid-wrapper">
<pre class="language-mermaid"><code>graph TB
    subgraph "Concurrency: Single Core, Interleaved Tasks"
        A1[Task A: Step 1] --> A2[Task A waits on I/O]
        A2 --> B1[Task B: Step 1]
        B1 --> B2[Task B waits on I/O]
        B2 --> A3[Task A: Step 2]
        A3 --> B3[Task B: Step 2]
    end

    subgraph "Parallelism: Multiple Cores, Simultaneous Execution"
        C1[Core 1: Task A] -.runs simultaneously.- C2[Core 2: Task B]
        C3[Core 3: Task C] -.runs simultaneously.- C4[Core 4: Task D]
    end</code></pre>
</div>
</div>
<div class="section" id="the-python-gil-understanding-the-constraint">
<h2 id="the-python-gil-understanding-the-constraint"><span class="me-2">The Python GIL: Understanding the Constraint</span><a href="#the-python-gil-understanding-the-constraint" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Every Python developer encounters the Global Interpreter Lock eventually, often with confusion or frustration. Yet the GIL is not a flaw — it is a trade-off that makes CPython's memory management simple and safe, at the cost of restricting parallel execution within a single process.</p>
<div class="section" id="what-the-gil-does">
<h3 id="what-the-gil-does"><span class="me-2">What the GIL Does</span><a href="#what-the-gil-does" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The GIL is a mutex (mutual exclusion lock) that protects access to Python objects, ensuring that only one thread executes Python bytecode at any given moment within a process. Think of it as a talking stick in a meeting: only the person holding the stick may speak. In CPython, only the thread holding the GIL may execute Python code.</p>
<p>This design simplifies reference counting (Python's primary memory management mechanism) and prevents race conditions on internal data structures. Without the GIL, every object access would require fine-grained locking, making Python slower and far more complex.</p>
<div class="mermaid-wrapper">
<pre class="language-mermaid"><code>sequenceDiagram
    participant T1 as Thread 1
    participant GIL as Global Interpreter Lock
    participant T2 as Thread 2

    T1->>GIL: Acquire GIL
    activate T1
    Note over T1: Execute Python bytecode
    T1->>GIL: Release GIL (I/O operation)
    deactivate T1

    T2->>GIL: Acquire GIL
    activate T2
    Note over T2: Execute Python bytecode
    T2->>GIL: Release GIL (time slice expired)
    deactivate T2

    T1->>GIL: Reacquire GIL
    activate T1
    Note over T1: Resume execution</code></pre>
</div>
</div>
<div class="section" id="how-the-gil-shapes-your-choices">
<h3 id="how-the-gil-shapes-your-choices"><span class="me-2">How the GIL Shapes Your Choices</span><a href="#how-the-gil-shapes-your-choices" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p><strong>For I/O-bound workloads</strong>, the GIL is not an obstacle. When a thread performs a blocking operation — waiting for a network response, reading from disk, querying a database — it releases the GIL. Other threads can immediately acquire it and proceed with their work. The result: multiple threads can wait concurrently on different I/O operations, and your program finishes faster even though only one thread runs Python code at a time.</p>
<p><strong>For CPU-bound workloads</strong>, the GIL becomes a bottleneck. A thread performing intensive computation holds the GIL continuously (releasing it only briefly every few milliseconds for fairness). Other threads sit idle, unable to execute. Adding more threads to CPU-bound work does not speed it up — it may even slow it down due to context-switching overhead. This is why CPU-intensive Python code gains no benefit from multithreading and requires multiprocessing instead.</p>
<p><strong>Exceptions exist</strong>: Some Python libraries release the GIL when calling into native code. NumPy, SciPy, Numba-compiled functions, TensorFlow, and other performance-critical libraries often release the GIL during heavy computation. In these cases, multithreading can provide parallelism even for CPU-bound work — but this is the exception, not the rule.</p>
</div>
<div class="section" id="the-gil-s-future">
<h3 id="the-gil-s-future"><span class="me-2">The GIL's Future</span><a href="#the-gil-s-future" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>PEP 703 proposes making the GIL optional in future Python versions, allowing true multi-threaded parallelism for pure Python code. This effort is ambitious and will take years to mature. For now, understanding the GIL and working within its constraints remains essential.</p>
<p><strong>Lesson to Remember</strong>: The GIL ensures that only one thread executes Python bytecode at a time within a process. This makes threads perfect for I/O-bound work (where the GIL releases during waiting) but ineffective for CPU-bound Python code (where the GIL blocks parallel execution). Multiprocessing bypasses the GIL entirely by running separate Python interpreters, each with its own GIL.</p>
</div>
</div>
<div class="section" id="when-to-use-multithreading">
<h2 id="when-to-use-multithreading"><span class="me-2">When to Use Multithreading</span><a href="#when-to-use-multithreading" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Threads shine when your program spends most of its time waiting for external resources to respond. The pattern is everywhere: fetching web pages, reading files from disk, querying databases, calling APIs, downloading images. These operations share a common trait — the CPU does almost nothing while waiting for I/O to complete. Threads let you overlap that waiting time productively.</p>
<div class="section" id="recognizing-i-o-bound-workloads">
<h3 id="recognizing-i-o-bound-workloads"><span class="me-2">Recognizing I/O-Bound Workloads</span><a href="#recognizing-i-o-bound-workloads" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Your workload is I/O-bound if:</p>
<ul class="simple">
<li>The program spends more time waiting than computing</li>
<li>Network calls, file operations, or database queries dominate runtime</li>
<li>Adding faster CPUs provides minimal speedup, but faster network/disk helps significantly</li>
</ul>
<p>Examples include web scraping, ETL pipelines reading from multiple sources, microservices making concurrent API calls, or backup systems writing files in parallel.</p>
</div>
<div class="section" id="a-practical-example-concurrent-http-requests">
<h3 id="a-practical-example-concurrent-http-requests"><span class="me-2">A Practical Example: Concurrent HTTP Requests</span><a href="#a-practical-example-concurrent-http-requests" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Consider fetching data from multiple URLs sequentially — each request waits for a response before starting the next. With threads, all requests launch concurrently, and the program finishes when the slowest request completes.</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">import</span> <span class="nn">time</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">as_completed</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">requests</span>
</span><span class="line">
</span><span class="line"><span class="n">URLS</span> <span class="o">=</span> <span class="p">[</span>
</span><span class="line">    <span class="s2">&quot;https://example.com&quot;</span><span class="p">,</span>
</span><span class="line">    <span class="s2">&quot;https://httpbin.org/delay/1&quot;</span><span class="p">,</span>
</span><span class="line">    <span class="s2">&quot;https://httpbin.org/delay/2&quot;</span><span class="p">,</span>
</span><span class="line"><span class="p">]</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Fetch a URL and return its status code.&quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">url</span><span class="p">,</span> <span class="n">resp</span><span class="o">.</span><span class="n">status_code</span>
</span><span class="line">
</span><span class="line"><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
</span><span class="line"><span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
</span><span class="line">    <span class="c1"># Submit all tasks to the pool</span>
</span><span class="line">    <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">fetch</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">URLS</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Process results as they complete</span>
</span><span class="line">    <span class="k">for</span> <span class="n">fut</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="n">futures</span><span class="p">):</span>
</span><span class="line">        <span class="n">url</span><span class="p">,</span> <span class="n">status</span> <span class="o">=</span> <span class="n">fut</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
</span><span class="line">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Completed in </span><span class="si">{</span><span class="n">elapsed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s with threads&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></div></figure><p>Run this code and observe: the total time approaches the slowest single request, not the sum of all requests. That is concurrency at work.</p>
</div>
<div class="section" id="how-thread-pools-work">
<h3 id="how-thread-pools-work"><span class="me-2">How Thread Pools Work</span><a href="#how-thread-pools-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The <cite>ThreadPoolExecutor</cite> manages a pool of reusable threads. You submit tasks (function calls) to the pool, and idle threads pick them up. When a thread releases the GIL during I/O, another thread immediately begins executing. This coordination happens automatically — you focus on what to compute, not when or how threads interleave.</p>
<p>The <cite>max_workers</cite> parameter controls pool size. More threads allow more concurrent I/O operations, but too many threads waste memory and CPU time on context switching. For I/O-bound work, a reasonable starting point is 2–4 times the number of CPU cores, adjusted based on profiling.</p>
</div>
<div class="section" id="threads-vs-asyncio-a-brief-comparison">
<h3 id="threads-vs-asyncio-a-brief-comparison"><span class="me-2">Threads vs Asyncio: A Brief Comparison</span><a href="#threads-vs-asyncio-a-brief-comparison" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>For tens or hundreds of I/O operations, threads work beautifully. For thousands of concurrent connections — think websocket servers or high-throughput crawlers — <strong>asyncio</strong> becomes more efficient. Asyncio uses a single-threaded event loop with non-blocking I/O, avoiding thread overhead entirely. However, asyncio requires async/await syntax and libraries that support it, while threads work with existing synchronous code.</p>
<p>Rule of thumb: start with threads for simplicity; move to asyncio if you need extreme concurrency or your profiling shows thread overhead as a bottleneck.</p>
<p><strong>Lesson to Remember</strong>: Multithreading transforms I/O-bound programs by overlapping wait times. Threads release the GIL during blocking operations, allowing other threads to run. Use thread pools to manage concurrency cleanly, keeping pool size bounded to avoid excessive context switching. For massive concurrency (thousands of connections), asyncio may serve better.</p>
</div>
</div>
<div class="section" id="when-to-use-multiprocessing">
<h2 id="when-to-use-multiprocessing"><span class="me-2">When to Use Multiprocessing</span><a href="#when-to-use-multiprocessing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>When your program's bottleneck is computation — not waiting — you need true parallelism. Multiprocessing creates separate Python processes, each with its own interpreter, memory space, and GIL. These processes run simultaneously on different CPU cores, dividing computational work and delivering speedups proportional to the number of cores available.</p>
<div class="section" id="recognizing-cpu-bound-workloads">
<h3 id="recognizing-cpu-bound-workloads"><span class="me-2">Recognizing CPU-Bound Workloads</span><a href="#recognizing-cpu-bound-workloads" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Your workload is CPU-bound if:</p>
<ul class="simple">
<li>The program spends most of its time computing rather than waiting</li>
<li>Faster CPUs reduce runtime significantly; faster I/O provides little benefit</li>
<li>Profiling shows most time spent in Python code, not in I/O calls</li>
</ul>
<p>Examples include numerical simulations, image or video processing, data parsing and transformation, cryptographic operations, machine learning feature engineering, and compression/decompression.</p>
</div>
<div class="section" id="a-practical-example-parallel-prime-checking">
<h3 id="a-practical-example-parallel-prime-checking"><span class="me-2">A Practical Example: Parallel Prime Checking</span><a href="#a-practical-example-parallel-prime-checking" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Testing large numbers for primality involves intensive computation with no I/O. Running these checks sequentially on one core leaves other cores idle. Multiprocessing distributes the work across all available cores.</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">time</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ProcessPoolExecutor</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">is_prime</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Check if n is prime using trial division.&quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
</span><span class="line">        <span class="k">return</span> <span class="kc">False</span>
</span><span class="line">    <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">        <span class="k">return</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">2</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Check odd divisors up to sqrt(n)</span>
</span><span class="line">    <span class="n">limit</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
</span><span class="line">    <span class="k">for</span> <span class="n">divisor</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">limit</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="n">divisor</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="kc">False</span>
</span><span class="line">    <span class="k">return</span> <span class="kc">True</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span class="line">    <span class="n">nums</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10_000_019</span><span class="p">,</span> <span class="mi">10_000_079</span><span class="p">,</span> <span class="mi">10_000_103</span><span class="p">,</span> <span class="mi">10_000_129</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
</span><span class="line">    <span class="k">with</span> <span class="n">ProcessPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
</span><span class="line">        <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">is_prime</span><span class="p">,</span> <span class="n">nums</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
</span><span class="line">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Results: </span><span class="si">{</span><span class="n">results</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span class="line">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Completed in </span><span class="si">{</span><span class="n">elapsed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s with multiprocessing&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></div></figure><p>Run this on a multi-core machine and observe: the speedup approaches the number of cores used (up to the number of tasks). Each process runs independently, computing in parallel.</p>
</div>
<div class="section" id="how-process-pools-work">
<h3 id="how-process-pools-work"><span class="me-2">How Process Pools Work</span><a href="#how-process-pools-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The <cite>ProcessPoolExecutor</cite> spawns multiple worker processes at creation. When you submit tasks, the pool distributes them to available workers. Each worker receives a copy of the function and its arguments (serialized via pickle), executes the task in isolation, and returns the result (also serialized).</p>
<p>Unlike threads, processes do not share memory by default. Each has its own Python interpreter and GIL. This isolation eliminates race conditions but introduces serialization overhead — large data structures must be copied to each process, which can become a bottleneck.</p>
<p>By default, <cite>ProcessPoolExecutor()</cite> creates a pool with <cite>os.cpu_count()</cite> workers — matching the number of CPU cores. For CPU-bound work, this is typically optimal. Creating more workers than cores leads to context switching without additional parallelism.</p>
</div>
<div class="section" id="memory-and-serialization-trade-offs">
<h3 id="memory-and-serialization-trade-offs"><span class="me-2">Memory and Serialization Trade-offs</span><a href="#memory-and-serialization-trade-offs" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Processes pay for isolation with memory overhead. Each process duplicates the Python interpreter and loaded modules. Passing large datasets to workers incurs serialization costs that can dominate execution time.</p>
<p>When working with large arrays or dataframes:</p>
<ul class="simple">
<li>Use libraries that support shared memory (e.g., <cite>multiprocessing.shared_memory</cite>, NumPy memory mapping)</li>
<li>Partition data to minimize transfer</li>
<li>Consider whether the computational gain justifies the copying cost</li>
</ul>
<p>For small to medium data with substantial computation, multiprocessing delivers dramatic speedups. For massive data with light computation, serialization overhead may negate the benefit.</p>
</div>
<div class="section" id="the-if-name-main-guard">
<h3 id="the-if-name-main-guard"><span class="me-2">The <cite>if __name__ == &quot;__main__&quot;</cite> Guard</span><a href="#the-if-name-main-guard" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Notice the <cite>if __name__ == &quot;__main__&quot;:</cite> guard in the example. On Windows and macOS, Python uses the <cite>spawn</cite> start method by default, which imports the main module in each worker process. Without this guard, each worker would recursively create more workers, leading to chaos. Always protect your multiprocessing entry point with this guard for cross-platform compatibility.</p>
<p><strong>Lesson to Remember</strong>: Multiprocessing achieves true parallelism by running separate Python interpreters on different CPU cores, bypassing the GIL entirely. Use it for CPU-bound workloads where computation dominates. Be mindful of memory overhead and serialization costs, and always guard entry points with <cite>if __name__ == &quot;__main__&quot;:</cite> for compatibility.</p>
</div>
</div>
<div class="section" id="the-concurrency-api-landscape">
<h2 id="the-concurrency-api-landscape"><span class="me-2">The Concurrency API Landscape</span><a href="#the-concurrency-api-landscape" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Python offers multiple APIs for concurrent programming, each suited to different patterns. Understanding which to reach for streamlines development.</p>
<div class="section" id="high-level-executors-recommended-starting-point">
<h3 id="high-level-executors-recommended-starting-point"><span class="me-2">High-Level Executors (Recommended Starting Point)</span><a href="#high-level-executors-recommended-starting-point" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>The <cite>concurrent.futures</cite> module provides <cite>ThreadPoolExecutor</cite> and <cite>ProcessPoolExecutor</cite> — simple, powerful abstractions that handle pool management, task scheduling, and result collection. For most applications, start here:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">ProcessPoolExecutor</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Thread pool for I/O</span>
</span><span class="line"><span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
</span><span class="line">    <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">executor</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">fetch_data</span><span class="p">,</span> <span class="n">urls</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Process pool for CPU work</span>
</span><span class="line"><span class="k">with</span> <span class="n">ProcessPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
</span><span class="line">    <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">executor</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">compute_intensive</span><span class="p">,</span> <span class="n">inputs</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></div></figure></div>
<div class="section" id="lower-level-threading-apis">
<h3 id="lower-level-threading-apis"><span class="me-2">Lower-Level Threading APIs</span><a href="#lower-level-threading-apis" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>When you need finer control over thread lifecycle, coordination, or shared state:</p>
<ul class="simple">
<li><strong>`threading.Thread`</strong>: Create and manage individual threads explicitly</li>
<li><strong>`threading.Lock`, `RLock`, `Semaphore`, `Event`</strong>: Synchronization primitives for protecting shared state</li>
<li><strong>`queue.Queue`</strong>: Thread-safe queue for producer-consumer patterns</li>
</ul>
<p>These are more verbose but provide flexibility for complex patterns like worker threads that run indefinitely or custom synchronization logic.</p>
</div>
<div class="section" id="lower-level-multiprocessing-apis">
<h3 id="lower-level-multiprocessing-apis"><span class="me-2">Lower-Level Multiprocessing APIs</span><a href="#lower-level-multiprocessing-apis" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>For advanced multiprocessing scenarios:</p>
<ul class="simple">
<li><strong>`multiprocessing.Process`</strong>: Explicit process creation and management</li>
<li><strong>`multiprocessing.Queue`, `Pipe`</strong>: Inter-process communication channels</li>
<li><strong>`multiprocessing.Manager`</strong>: Shared objects (lists, dicts) with automatic synchronization</li>
<li><strong>`multiprocessing.shared_memory`</strong>: Zero-copy shared memory for large arrays (Python 3.8+)</li>
</ul>
<p>Use these when you need custom communication patterns, long-lived worker processes, or shared memory for performance.</p>
</div>
<div class="section" id="asyncio-for-massive-concurrency">
<h3 id="asyncio-for-massive-concurrency"><span class="me-2">Asyncio for Massive Concurrency</span><a href="#asyncio-for-massive-concurrency" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>For thousands of concurrent I/O operations, <strong>asyncio</strong> offers single-threaded cooperative multitasking with non-blocking I/O. It requires async/await syntax and compatible libraries but excels at high-concurrency scenarios like websocket servers or large-scale web scraping:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">import</span> <span class="nn">asyncio</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">aiohttp</span>
</span><span class="line">
</span><span class="line"><span class="k">async</span> <span class="k">def</span> <span class="nf">fetch_async</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
</span><span class="line">    <span class="k">async</span> <span class="k">with</span> <span class="n">aiohttp</span><span class="o">.</span><span class="n">ClientSession</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
</span><span class="line">        <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="k">await</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span><span class="line">    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">fetch_async</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">]</span>
</span><span class="line">    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></div></figure><p>Asyncio pairs well with threads and processes: use asyncio for I/O coordination and offload CPU work to a process pool.</p>
</div>
</div>
<div class="section" id="data-sharing-and-communication">
<h2 id="data-sharing-and-communication"><span class="me-2">Data Sharing and Communication</span><a href="#data-sharing-and-communication" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Threads and processes handle shared state differently, shaping how you structure communication between concurrent tasks.</p>
<div class="section" id="threads-shared-memory-with-synchronization">
<h3 id="threads-shared-memory-with-synchronization"><span class="me-2">Threads: Shared Memory with Synchronization</span><a href="#threads-shared-memory-with-synchronization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Threads within a process share the same memory space. This makes communication easy — just read and write shared variables. However, unsynchronized access to shared state causes race conditions. Protect shared data with locks or use thread-safe data structures like <cite>queue.Queue</cite>.</p>
<p><strong>Thread-safe queue example</strong> (Producer-Consumer Pattern):</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">import</span> <span class="nn">threading</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">queue</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">time</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Create a thread-safe queue</span>
</span><span class="line"><span class="n">work_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Producer: add work items</span>
</span><span class="line"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
</span><span class="line">    <span class="n">work_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Consume items from the queue until empty.&quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span><span class="line">        <span class="k">try</span><span class="p">:</span>
</span><span class="line">            <span class="n">item</span> <span class="o">=</span> <span class="n">work_queue</span><span class="o">.</span><span class="n">get_nowait</span><span class="p">()</span>
</span><span class="line">            <span class="c1"># Simulate processing</span>
</span><span class="line">            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
</span><span class="line">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Worker </span><span class="si">{</span><span class="n">worker_id</span><span class="si">}</span><span class="s2"> processed item </span><span class="si">{</span><span class="n">item</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span class="line">            <span class="n">work_queue</span><span class="o">.</span><span class="n">task_done</span><span class="p">()</span>
</span><span class="line">        <span class="k">except</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">:</span>
</span><span class="line">            <span class="k">break</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Start worker threads</span>
</span><span class="line"><span class="n">threads</span> <span class="o">=</span> <span class="p">[</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">worker</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="p">,))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
</span><span class="line"><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
</span><span class="line">    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span class="line"><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
</span><span class="line">    <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All work completed&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></div></figure><p>The <cite>queue.Queue</cite> handles locking internally. Multiple threads can safely call <cite>put()</cite> and <cite>get()</cite> without explicit synchronization. This pattern scales to producer-consumer systems where some threads generate tasks and others process them.</p>
</div>
<div class="section" id="processes-isolated-memory-with-explicit-communication">
<h3 id="processes-isolated-memory-with-explicit-communication"><span class="me-2">Processes: Isolated Memory with Explicit Communication</span><a href="#processes-isolated-memory-with-explicit-communication" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Processes do not share memory by default. To communicate, use message-passing mechanisms:</p>
<p><strong>Multiprocessing Queue</strong>: Similar to <cite>queue.Queue</cite> but works across processes via serialization:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">from</span> <span class="nn">multiprocessing</span> <span class="kn">import</span> <span class="n">Process</span><span class="p">,</span> <span class="n">Queue</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">worker</span><span class="p">(</span><span class="n">input_queue</span><span class="p">,</span> <span class="n">output_queue</span><span class="p">):</span>
</span><span class="line">    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span><span class="line">        <span class="n">item</span> <span class="o">=</span> <span class="n">input_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span><span class="line">        <span class="k">if</span> <span class="n">item</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># Poison pill to signal shutdown</span>
</span><span class="line">            <span class="k">break</span>
</span><span class="line">        <span class="n">result</span> <span class="o">=</span> <span class="n">process</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class="line">        <span class="n">output_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span class="line">    <span class="n">input_q</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">()</span>
</span><span class="line">    <span class="n">output_q</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Start workers</span>
</span><span class="line">    <span class="n">workers</span> <span class="o">=</span> <span class="p">[</span><span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">worker</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">input_q</span><span class="p">,</span> <span class="n">output_q</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
</span><span class="line">    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">workers</span><span class="p">:</span>
</span><span class="line">        <span class="n">w</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Send tasks</span>
</span><span class="line">    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
</span><span class="line">        <span class="n">input_q</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Send shutdown signal</span>
</span><span class="line">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">workers</span><span class="p">:</span>
</span><span class="line">        <span class="n">input_q</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Collect results</span>
</span><span class="line">    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">output_q</span><span class="o">.</span><span class="n">get</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">workers</span><span class="p">:</span>
</span><span class="line">        <span class="n">w</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></div></figure><p><strong>Shared Memory for Performance</strong>: When passing large NumPy arrays or similar data, serialization becomes a bottleneck. Use <cite>multiprocessing.shared_memory</cite> to share data without copying:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">from</span> <span class="nn">multiprocessing</span> <span class="kn">import</span> <span class="n">Process</span><span class="p">,</span> <span class="n">shared_memory</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">worker</span><span class="p">(</span><span class="n">shm_name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
</span><span class="line">    <span class="c1"># Attach to existing shared memory</span>
</span><span class="line">    <span class="n">existing_shm</span> <span class="o">=</span> <span class="n">shared_memory</span><span class="o">.</span><span class="n">SharedMemory</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">shm_name</span><span class="p">)</span>
</span><span class="line">    <span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">buffer</span><span class="o">=</span><span class="n">existing_shm</span><span class="o">.</span><span class="n">buf</span><span class="p">)</span>
</span><span class="line">    <span class="c1"># Work with array...</span>
</span><span class="line">    <span class="n">existing_shm</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span class="line">    <span class="c1"># Create shared memory</span>
</span><span class="line">    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
</span><span class="line">    <span class="n">shm</span> <span class="o">=</span> <span class="n">shared_memory</span><span class="o">.</span><span class="n">SharedMemory</span><span class="p">(</span><span class="n">create</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">nbytes</span><span class="p">)</span>
</span><span class="line">    <span class="n">shared_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">buffer</span><span class="o">=</span><span class="n">shm</span><span class="o">.</span><span class="n">buf</span><span class="p">)</span>
</span><span class="line">    <span class="n">shared_array</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:]</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Start processes that use shared memory</span>
</span><span class="line">    <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">worker</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">shm</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
</span><span class="line">    <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span class="line">    <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Clean up</span>
</span><span class="line">    <span class="n">shm</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class="line">    <span class="n">shm</span><span class="o">.</span><span class="n">unlink</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></div></figure><p>This avoids copying large datasets, providing near-instant access across processes.</p>
</div>
</div>
<div class="section" id="performance-considerations-and-optimization">
<h2 id="performance-considerations-and-optimization"><span class="me-2">Performance Considerations and Optimization</span><a href="#performance-considerations-and-optimization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Concurrency introduces overhead — the question is whether the speedup justifies the cost. Understanding these trade-offs helps you optimize effectively.</p>
<div class="section" id="startup-overhead-threads-vs-processes">
<h3 id="startup-overhead-threads-vs-processes"><span class="me-2">Startup Overhead: Threads vs Processes</span><a href="#startup-overhead-threads-vs-processes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Creating a thread is fast (microseconds). Creating a process is slow (milliseconds) because it involves forking or spawning a new Python interpreter and copying memory. For one-off tasks, this overhead may exceed the task's duration.</p>
<p><strong>Solution</strong>: Use pools that amortize startup costs across many tasks. <cite>ThreadPoolExecutor</cite> and <cite>ProcessPoolExecutor</cite> create workers once and reuse them, making per-task overhead negligible.</p>
</div>
<div class="section" id="serialization-cost-in-multiprocessing">
<h3 id="serialization-cost-in-multiprocessing"><span class="me-2">Serialization Cost in Multiprocessing</span><a href="#serialization-cost-in-multiprocessing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Every argument and return value in multiprocessing must be serialized (pickled) and deserialized. For small data (integers, strings, small lists), this is fast. For large structures (gigabyte arrays, complex nested objects), serialization can dominate execution time, negating any parallel speedup.</p>
<p><strong>Mitigation strategies</strong>:</p>
<ul class="simple">
<li>Use shared memory (<cite>multiprocessing.shared_memory</cite>) for large arrays</li>
<li>Partition work so each process needs only a slice of the data</li>
<li>Preload static data in worker initialization instead of passing it with each task</li>
<li>Profile to identify if serialization is the bottleneck</li>
</ul>
</div>
<div class="section" id="oversubscription-matching-workers-to-cores">
<h3 id="oversubscription-matching-workers-to-cores"><span class="me-2">Oversubscription: Matching Workers to Cores</span><a href="#oversubscription-matching-workers-to-cores" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>For CPU-bound work, creating more worker processes than CPU cores provides no benefit and adds context-switching overhead. The operating system rapidly switches between processes, but each switch wastes cycles.</p>
<p><strong>Best practice</strong>: Set <cite>max_workers=os.cpu_count()</cite> for CPU-bound tasks. For I/O-bound tasks, worker count can exceed core count since threads spend most time waiting.</p>
</div>
<div class="section" id="libraries-that-release-the-gil">
<h3 id="libraries-that-release-the-gil"><span class="me-2">Libraries That Release the GIL</span><a href="#libraries-that-release-the-gil" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>NumPy, SciPy, Numba, TensorFlow, PyTorch, and other performance libraries often release the GIL during computation, calling native code or BLAS routines. In these cases, multithreading can achieve true parallelism for CPU-bound work without multiprocessing's overhead.</p>
<p><strong>Test first</strong>: If your workload heavily uses these libraries, benchmark threads vs processes. Threads may surprise you with comparable speedups and lower memory use.</p>
</div>
<div class="section" id="measuring-and-profiling">
<h3 id="measuring-and-profiling"><span class="me-2">Measuring and Profiling</span><a href="#measuring-and-profiling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Intuition about bottlenecks is often wrong. Before optimizing:</p>
<ol class="arabic simple">
<li><strong>Profile sequential code</strong> to confirm where time is spent (I/O vs CPU)</li>
<li><strong>Measure concurrent versions</strong> with realistic workloads</li>
<li><strong>Compare threads, processes, and asyncio</strong> if the workload has mixed characteristics</li>
</ol>
<p>Use Python's <cite>cProfile</cite>, <cite>line_profiler</cite>, or <cite>py-spy</cite> for profiling. Time measurements with <cite>time.perf_counter()</cite> suffice for coarse-grained checks.</p>
</div>
</div>
<div class="section" id="common-pitfalls-and-how-to-avoid-them">
<h2 id="common-pitfalls-and-how-to-avoid-them"><span class="me-2">Common Pitfalls and How to Avoid Them</span><a href="#common-pitfalls-and-how-to-avoid-them" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Concurrent programming introduces subtle failure modes. Awareness and defensive patterns prevent most issues.</p>
<div class="section" id="deadlocks-circular-waiting">
<h3 id="deadlocks-circular-waiting"><span class="me-2">Deadlocks: Circular Waiting</span><a href="#deadlocks-circular-waiting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A deadlock occurs when two or more threads/processes wait for each other to release resources, creating a cycle with no escape. Classic example: Thread A holds Lock 1 and waits for Lock 2; Thread B holds Lock 2 and waits for Lock 1. Both wait forever.</p>
<p><strong>Prevention strategies</strong>:</p>
<ul class="simple">
<li>Acquire locks in a consistent global order across all threads</li>
<li>Use timeout parameters (<cite>lock.acquire(timeout=1.0)</cite>) to detect and recover from potential deadlocks</li>
<li>Prefer message-passing (queues) over shared locks — queues cannot deadlock</li>
<li>Keep critical sections (lock-protected code) as short as possible</li>
</ul>
</div>
<div class="section" id="race-conditions-in-shared-state">
<h3 id="race-conditions-in-shared-state"><span class="me-2">Race Conditions in Shared State</span><a href="#race-conditions-in-shared-state" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>When multiple threads read and modify shared data without synchronization, the final state becomes unpredictable. Example: two threads incrementing a shared counter simultaneously may only increment it once instead of twice.</p>
<p><strong>Solutions</strong>:</p>
<ul class="simple">
<li>Use locks (<cite>threading.Lock</cite>) to protect shared variables</li>
<li>Use thread-safe data structures like <cite>queue.Queue</cite></li>
<li>Prefer immutable data and message-passing over shared mutable state</li>
<li>In processes, race conditions are rare since memory is isolated by default</li>
</ul>
</div>
<div class="section" id="fork-vs-spawn-platform-specific-start-methods">
<h3 id="fork-vs-spawn-platform-specific-start-methods"><span class="me-2">Fork vs Spawn: Platform-Specific Start Methods</span><a href="#fork-vs-spawn-platform-specific-start-methods" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>On Unix-like systems, <cite>fork</cite> is the default process start method — it copies the parent's memory space. On Windows, <cite>spawn</cite> starts a fresh interpreter. macOS also defaults to <cite>spawn</cite> for safety.</p>
<p>With <cite>fork</cite>, global state is inherited, which can cause surprising bugs (database connections, thread locks, file handles). With <cite>spawn</cite>, the module is re-imported, so unguarded code at module level re-executes in every worker.</p>
<p><strong>Best practice</strong>: Always guard your multiprocessing entry point with <cite>if __name__ == &quot;__main__&quot;:</cite> for cross-platform compatibility. On Unix, explicitly set the start method to <cite>spawn</cite> if <cite>fork</cite> causes issues:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">import</span> <span class="nn">multiprocessing</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span class="line">    <span class="n">multiprocessing</span><span class="o">.</span><span class="n">set_start_method</span><span class="p">(</span><span class="s1">&#39;spawn&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="c1"># ... rest of code</span>
</span></code></pre></td></tr></table></div></div></figure></div>
<div class="section" id="pickling-errors-what-cannot-be-serialized">
<h3 id="pickling-errors-what-cannot-be-serialized"><span class="me-2">Pickling Errors: What Cannot Be Serialized</span><a href="#pickling-errors-what-cannot-be-serialized" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Processes communicate via pickle serialization. Not everything is picklable:</p>
<ul class="simple">
<li>Lambda functions and local closures</li>
<li>Open file handles, sockets, database connections</li>
<li>Thread locks, module-level objects with complex state</li>
</ul>
<p><strong>Workarounds</strong>:</p>
<ul class="simple">
<li>Define functions at module level, not inside other functions</li>
<li>Use <cite>functools.partial</cite> instead of lambdas</li>
<li>Initialize non-picklable resources (like DB connections) inside each worker process</li>
<li>Use the <cite>__getstate__</cite> and <cite>__setstate__</cite> methods to customize serialization</li>
</ul>
</div>
<div class="section" id="global-state-does-not-transfer">
<h3 id="global-state-does-not-transfer"><span class="me-2">Global State Does Not Transfer</span><a href="#global-state-does-not-transfer" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Processes do not share global variables. Modifying a global in one process does not affect others:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">increment</span><span class="p">():</span>
</span><span class="line">    <span class="k">global</span> <span class="n">counter</span>
</span><span class="line">    <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span class="line">    <span class="k">with</span> <span class="n">ProcessPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
</span><span class="line">        <span class="n">pool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">increment</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</span><span class="line">    <span class="nb">print</span><span class="p">(</span><span class="n">counter</span><span class="p">)</span>  <span class="c1"># Still 0 — increments happened in worker memory</span>
</span></code></pre></td></tr></table></div></div></figure><p><strong>Solution</strong>: Return results explicitly or use shared memory (<cite>multiprocessing.Value</cite>, <cite>Array</cite>, or <cite>shared_memory</cite>) with proper locking if needed.</p>
</div>
</div>
<div class="section" id="hybrid-patterns-combining-threads-and-processes">
<h2 id="hybrid-patterns-combining-threads-and-processes"><span class="me-2">Hybrid Patterns: Combining Threads and Processes</span><a href="#hybrid-patterns-combining-threads-and-processes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Real-world applications rarely fit cleanly into &quot;pure I/O&quot; or &quot;pure CPU&quot; categories. Many workloads have distinct phases: fetch data over the network, then process it intensively. Hybrid patterns combine threads and processes to optimize each phase independently.</p>
<div class="section" id="the-two-stage-pipeline-pattern">
<h3 id="the-two-stage-pipeline-pattern"><span class="me-2">The Two-Stage Pipeline Pattern</span><a href="#the-two-stage-pipeline-pattern" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>A common pattern: use threads for the I/O-bound ingestion phase, then hand off to a process pool for CPU-intensive analysis.</p>
<div class="mermaid-wrapper">
<pre class="language-mermaid"><code>graph LR
    A[URLs/Tasks] --> B[Thread Pool: I/O Fetch]
    B --> C[Data Payloads]
    C --> D[Process Pool: CPU Transform]
    D --> E[Results]

    style B fill:#a8d5ff
    style D fill:#ffcba8</code></pre>
</div>
<p>This architecture keeps both your network connections and CPU cores busy, maximizing throughput.</p>
<p><strong>Practical implementation</strong>:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">ProcessPoolExecutor</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">requests</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;I/O-bound: download data.&quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;CPU-bound: parse and analyze data.&quot;&quot;&quot;</span>
</span><span class="line">    <span class="c1"># Expensive computation here</span>
</span><span class="line">    <span class="k">return</span> <span class="n">analyze</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span class="line">    <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://api.example.com/data/</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Stage 1: Fetch with threads</span>
</span><span class="line">    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="k">as</span> <span class="n">tpool</span><span class="p">:</span>
</span><span class="line">        <span class="n">payloads</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tpool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">fetch</span><span class="p">,</span> <span class="n">urls</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Stage 2: Process with processes</span>
</span><span class="line">    <span class="k">with</span> <span class="n">ProcessPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">ppool</span><span class="p">:</span>
</span><span class="line">        <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ppool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">payloads</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Processed </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">}</span><span class="s2"> items&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></div></figure><p>This separation is clean and efficient: threads don't waste time on CPU work (where they'd block on the GIL), and processes don't waste time on I/O (where their overhead is unnecessary).</p>
</div>
<div class="section" id="concurrent-producers-with-process-consumers">
<h3 id="concurrent-producers-with-process-consumers"><span class="me-2">Concurrent Producers with Process Consumers</span><a href="#concurrent-producers-with-process-consumers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>For continuous streaming workloads, run threads as producers feeding a queue, with processes as consumers pulling from that queue:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">ProcessPoolExecutor</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">queue</span> <span class="kn">import</span> <span class="n">Queue</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">threading</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Shared queue between threads</span>
</span><span class="line"><span class="n">data_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">producer</span><span class="p">(</span><span class="n">sources</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Fetch data continuously and enqueue.&quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">for</span> <span class="n">source</span> <span class="ow">in</span> <span class="n">sources</span><span class="p">:</span>
</span><span class="line">        <span class="n">data</span> <span class="o">=</span> <span class="n">fetch_from</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
</span><span class="line">        <span class="n">data_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">consumer_worker</span><span class="p">():</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Process data from queue.&quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span><span class="line">        <span class="n">data</span> <span class="o">=</span> <span class="n">data_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span><span class="line">        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># Shutdown signal</span>
</span><span class="line">            <span class="k">break</span>
</span><span class="line">        <span class="n">result</span> <span class="o">=</span> <span class="n">process</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span><span class="line">        <span class="n">store</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Start producers (threads for I/O)</span>
</span><span class="line"><span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="k">as</span> <span class="n">tpool</span><span class="p">:</span>
</span><span class="line">    <span class="n">tpool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">producer</span><span class="p">,</span> <span class="n">data_sources</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c1"># Start consumers (processes for CPU work)</span>
</span><span class="line">    <span class="k">with</span> <span class="n">ProcessPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="k">as</span> <span class="n">ppool</span><span class="p">:</span>
</span><span class="line">        <span class="n">consumer_futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">ppool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">consumer_worker</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
</span><span class="line">        <span class="c1"># ... coordinate shutdown ...</span>
</span></code></pre></td></tr></table></div></div></figure><p>This pattern adapts to streaming data where ingestion and processing happen concurrently at different rates.</p>
</div>
<div class="section" id="when-to-use-hybrid-approaches">
<h3 id="when-to-use-hybrid-approaches"><span class="me-2">When to Use Hybrid Approaches</span><a href="#when-to-use-hybrid-approaches" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Consider hybrid patterns when:</p>
<ul class="simple">
<li>Your application has distinct I/O and CPU phases</li>
<li>Profiling shows both I/O waiting time and CPU computation time are significant</li>
<li>You want to maximize utilization of both network bandwidth and CPU cores</li>
<li>The data size is manageable (large data benefits from streaming or shared memory)</li>
</ul>
<p>Start simple (threads or processes alone), then evolve to hybrid only if profiling reveals underutilization of either I/O or CPU resources.</p>
</div>
</div>
<div class="section" id="scaling-beyond-one-machine">
<h2 id="scaling-beyond-one-machine"><span class="me-2">Scaling Beyond One Machine</span><a href="#scaling-beyond-one-machine" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Threads and processes unlock the power of a single machine's cores. When that proves insufficient, distributed computing frameworks extend these patterns across multiple machines.</p>
<div class="section" id="ray-distributed-python-with-minimal-refactoring">
<h3 id="ray-distributed-python-with-minimal-refactoring"><span class="me-2">Ray: Distributed Python with Minimal Refactoring</span><a href="#ray-distributed-python-with-minimal-refactoring" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p><strong>Ray</strong> brings a Pythonic API for distributed computing. Decorate functions with <cite>&#64;ray.remote</cite> and call them as distributed tasks. Ray handles scheduling, data transfer, and fault tolerance:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">import</span> <span class="nn">ray</span>
</span><span class="line">
</span><span class="line"><span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span>
</span><span class="line"><span class="k">def</span> <span class="nf">expensive_task</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</span><span class="line">    <span class="c1"># CPU-intensive work</span>
</span><span class="line">    <span class="k">return</span> <span class="n">result</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Submit tasks to Ray cluster</span>
</span><span class="line"><span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">expensive_task</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">]</span>
</span><span class="line"><span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></div></figure><p>Ray integrates with pandas, NumPy, and machine learning libraries. It scales from a laptop to a thousand-node cluster with the same code.</p>
</div>
<div class="section" id="dask-parallel-computing-for-data-science">
<h3 id="dask-parallel-computing-for-data-science"><span class="me-2">Dask: Parallel Computing for Data Science</span><a href="#dask-parallel-computing-for-data-science" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p><strong>Dask</strong> provides parallel versions of pandas DataFrames and NumPy arrays, plus a flexible task scheduler. Dask integrates seamlessly with the PyData ecosystem:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">import</span> <span class="nn">dask.dataframe</span> <span class="k">as</span> <span class="nn">dd</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Read large CSV in parallel chunks</span>
</span><span class="line"><span class="n">df</span> <span class="o">=</span> <span class="n">dd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;large_dataset_*.csv&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c1"># Operations parallelize automatically</span>
</span><span class="line"><span class="n">result</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></div></figure><p>Dask's scheduler can use threads, processes, or a distributed cluster. It excels at out-of-core computation (data larger than RAM) and ETL pipelines.</p>
</div>
<div class="section" id="joblib-lightweight-parallelism-for-scikit-learn">
<h3 id="joblib-lightweight-parallelism-for-scikit-learn"><span class="me-2">Joblib: Lightweight Parallelism for Scikit-Learn</span><a href="#joblib-lightweight-parallelism-for-scikit-learn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p><strong>Joblib</strong> provides simple parallel map and integration with scikit-learn's parallel backends. It's lightweight and well-suited for embarrassingly parallel tasks:</p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
</span><span class="line">
</span><span class="line"><span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">func</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></div></figure><p>Joblib automatically chooses between threads and processes based on workload, and supports progress bars and caching.</p>
</div>
<div class="section" id="spark-and-big-data-ecosystems">
<h3 id="spark-and-big-data-ecosystems"><span class="me-2">Spark and Big Data Ecosystems</span><a href="#spark-and-big-data-ecosystems" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>For massive-scale data processing (terabytes to petabytes), <strong>Apache Spark</strong> and <strong>Apache Flink</strong> provide battle-tested distributed computing with SQL, streaming, and ML libraries. These frameworks handle node failures, data replication, and optimization automatically, but require cluster infrastructure (Kubernetes, YARN, or managed services like Databricks).</p>
<p>Use Spark when your data no longer fits comfortably on one machine and you need robust, production-grade fault tolerance.</p>
</div>
<div class="section" id="choosing-the-right-tool">
<h3 id="choosing-the-right-tool"><span class="me-2">Choosing the Right Tool</span><a href="#choosing-the-right-tool" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<ul class="simple">
<li><strong>Single machine, moderate data</strong>: Threads or processes via <cite>concurrent.futures</cite></li>
<li><strong>Single machine, large data (out-of-core)</strong>: Dask</li>
<li><strong>Multi-machine, Python-centric</strong>: Ray</li>
<li><strong>Multi-machine, data pipelines</strong>: Dask or Spark</li>
<li><strong>Massive scale, enterprise pipelines</strong>: Spark or Flink</li>
</ul>
<p>Start local and scale out only when performance demands it. Premature distribution adds complexity without benefit.</p>
</div>
</div>
<div class="section" id="decision-framework-choosing-your-concurrency-strategy">
<h2 id="decision-framework-choosing-your-concurrency-strategy"><span class="me-2">Decision Framework: Choosing Your Concurrency Strategy</span><a href="#decision-framework-choosing-your-concurrency-strategy" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>When faced with a performance problem, this framework guides your choice:</p>
<div class="mermaid-wrapper">
<pre class="language-mermaid"><code>graph TD
    A[Start: Need Better Performance] --> B{Workload Type?}
    B -->|I/O-bound: network, disk, DB| C{Concurrency Level?}
    B -->|CPU-bound: computation| D{Scale?}

    C -->|10-100s connections| E[ThreadPoolExecutor]
    C -->|1000s connections| F[Asyncio]

    D -->|Single machine| G[ProcessPoolExecutor]
    D -->|Multiple machines| H[Ray / Dask / Spark]

    B -->|Mixed: I/O + CPU| I[Hybrid: Threads + Processes]

    style E fill:#a8d5ff
    style F fill:#a8d5ff
    style G fill:#ffcba8
    style H fill:#ffcba8
    style I fill:#d4a8ff</code></pre>
</div>
<p><strong>Step-by-step decision process</strong>:</p>
<ol class="arabic simple">
<li><strong>Profile first</strong>: Measure where time is spent (I/O waiting vs CPU computation)</li>
<li><strong>Identify bottleneck</strong>: Is it waiting or computing?</li>
<li><strong>Choose strategy</strong>:<ul>
<li><strong>I/O-bound</strong> with moderate concurrency → <cite>ThreadPoolExecutor</cite></li>
<li><strong>I/O-bound</strong> with massive concurrency → <cite>asyncio</cite></li>
<li><strong>CPU-bound</strong> on one machine → <cite>ProcessPoolExecutor</cite></li>
<li><strong>CPU-bound</strong> across machines → Ray or Dask</li>
<li><strong>Mixed workload</strong> → Hybrid (threads for I/O, processes for CPU)</li>
</ul>
</li>
<li><strong>Measure results</strong>: Benchmark the concurrent version against baseline</li>
<li><strong>Iterate</strong>: Adjust pool sizes, chunking strategies, or switch approaches based on profiling</li>
</ol>
<p>No single approach fits all problems. The decision tree provides a starting point; profiling provides the truth.</p>
</div>
<div class="section" id="quick-reference-cheatsheet">
<h2 id="quick-reference-cheatsheet"><span class="me-2">Quick Reference Cheatsheet</span><a href="#quick-reference-cheatsheet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p><strong>Thread Pool (I/O-bound)</strong></p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">as_completed</span>
</span><span class="line">
</span><span class="line"><span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
</span><span class="line">    <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">io_task</span><span class="p">,</span> <span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>
</span><span class="line">    <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="n">futures</span><span class="p">):</span>
</span><span class="line">        <span class="n">result</span> <span class="o">=</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></div></figure><p><strong>Process Pool (CPU-bound)</strong></p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ProcessPoolExecutor</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span class="line">    <span class="k">with</span> <span class="n">ProcessPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
</span><span class="line">        <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">cpu_task</span><span class="p">,</span> <span class="n">args</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></div></figure><p><strong>Hybrid (I/O + CPU)</strong></p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">from</span> <span class="nn">concurrent.futures</span> <span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">ProcessPoolExecutor</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span class="line">    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="k">as</span> <span class="n">tpool</span><span class="p">:</span>
</span><span class="line">        <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tpool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">fetch</span><span class="p">,</span> <span class="n">urls</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="k">with</span> <span class="n">ProcessPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">ppool</span><span class="p">:</span>
</span><span class="line">        <span class="n">results</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ppool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">process</span><span class="p">,</span> <span class="n">data</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></div></figure><p><strong>Asyncio (Massive I/O concurrency)</strong></p>
<figure class="code"><div class="highlight"><div class="table-wrapper"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span></span><span class="kn">import</span> <span class="nn">asyncio</span>
</span><span class="line">
</span><span class="line"><span class="k">async</span> <span class="k">def</span> <span class="nf">async_task</span><span class="p">(</span><span class="n">arg</span><span class="p">):</span>
</span><span class="line">    <span class="k">async</span> <span class="k">with</span> <span class="n">aiohttp</span><span class="o">.</span><span class="n">ClientSession</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
</span><span class="line">        <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="k">as</span> <span class="n">resp</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="k">await</span> <span class="n">resp</span><span class="o">.</span><span class="n">text</span><span class="p">()</span>
</span><span class="line">
</span><span class="line"><span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span><span class="line">    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">async_task</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">])</span>
</span><span class="line">
</span><span class="line"><span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></div></figure></div>
<div class="section" id="further-learning-and-resources">
<h2 id="further-learning-and-resources"><span class="me-2">Further Learning and Resources</span><a href="#further-learning-and-resources" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>The official Python documentation provides comprehensive references and examples:</p>
<ul class="simple">
<li><a class="reference external" href="https://docs.python.org/3/library/threading.html">Python threading</a> — Thread-based concurrency APIs</li>
<li><a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html">Python multiprocessing</a> — Process-based parallelism</li>
<li><a class="reference external" href="https://docs.python.org/3/library/concurrent.futures.html">concurrent.futures</a> — High-level executor interfaces</li>
<li><a class="reference external" href="https://docs.python.org/3/library/asyncio.html">asyncio</a> — Asynchronous I/O and event loops</li>
</ul>
<p>For distributed computing:</p>
<ul class="simple">
<li><a class="reference external" href="https://docs.ray.io/">Ray Documentation</a> — Distributed Python across clusters</li>
<li><a class="reference external" href="https://docs.dask.org/en/stable/">Dask Documentation</a> — Parallel computing for data science</li>
</ul>
<p>For deeper understanding:</p>
<ul class="simple">
<li><a class="reference external" href="https://peps.python.org/pep-0703/">PEP 703: Making the Global Interpreter Lock Optional in CPython</a> — The future of Python concurrency</li>
<li><em>Effective Python</em> by Brett Slatkin — Chapter on concurrency and parallelism</li>
<li><em>Python Concurrency with asyncio</em> by Matthew Fowler — Deep dive into asyncio patterns</li>
</ul>
</div>
<div class="section" id="closing-thoughts">
<h2 id="closing-thoughts"><span class="me-2">Closing Thoughts</span><a href="#closing-thoughts" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
<p>Concurrency and parallelism transform programs from sequential plodding to responsive, efficient systems that make full use of modern hardware. The choice between threads and processes is not arbitrary — it flows naturally from understanding your workload. I/O-bound tasks find their rhythm in threads, where waiting becomes overlap. CPU-bound tasks demand processes, where computation spreads across cores.</p>
<p>The GIL, often seen as a limitation, is simply a design trade-off that shapes these choices. Work with it, not against it: threads for I/O, processes for computation, and hybrid patterns when both matter. Profiling guides these decisions, revealing where time truly goes and whether optimization efforts will bear fruit.</p>
<p>Start simple. Reach for <cite>ThreadPoolExecutor</cite> or <cite>ProcessPoolExecutor</cite> first — they handle complexity gracefully while keeping your code clean. Measure. Profile. Iterate. As your needs grow, the path forward — whether asyncio for massive I/O concurrency or Ray for distributed computing — will become clear from the data.</p>
<p>Concurrency is not magic, but a set of patterns grounded in how computers work: cores that compute, networks that wait, memory that can be shared or isolated. With this understanding, you can reason confidently about performance, scalability, and the architecture of responsive systems.</p>
<p>Go forth and build programs that make waiting productive and computation parallel. The principles here will serve you well.</p>
</div>


  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/posts/categories/python/">Python</a>,
          <a href="/posts/categories/concurrency/">Concurrency</a>,
          <a href="/posts/categories/performance/">Performance</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/posts/tags/multithreading/"
            class="post-tag no-text-decoration"
          >multithreading</a>
        
          <a
            href="/posts/tags/multiprocessing/"
            class="post-tag no-text-decoration"
          >multiprocessing</a>
        
          <a
            href="/posts/tags/concurrency/"
            class="post-tag no-text-decoration"
          >concurrency</a>
        
          <a
            href="/posts/tags/parallelism/"
            class="post-tag no-text-decoration"
          >parallelism</a>
        
          <a
            href="/posts/tags/gil/"
            class="post-tag no-text-decoration"
          >GIL</a>
        
          <a
            href="/posts/tags/cpu-bound/"
            class="post-tag no-text-decoration"
          >CPU-bound</a>
        
          <a
            href="/posts/tags/i-o-bound/"
            class="post-tag no-text-decoration"
          >I/O-bound</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=Multithreading%20vs%20Multiprocessing%20%E2%80%94%20Choosing%20the%20Right%20Concurrency%20Model%20-%20Posts%20by%20MR901&url=https%3A%2F%2Fmr901.github.io%2Fposts%2Fmultithreading-vs-multiprocessing%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=Multithreading%20vs%20Multiprocessing%20%E2%80%94%20Choosing%20the%20Right%20Concurrency%20Model%20-%20Posts%20by%20MR901&u=https%3A%2F%2Fmr901.github.io%2Fposts%2Fmultithreading-vs-multiprocessing%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=https%3A%2F%2Fmr901.github.io%2Fposts%2Fmultithreading-vs-multiprocessing%2F&text=Multithreading%20vs%20Multiprocessing%20%E2%80%94%20Choosing%20the%20Right%20Concurrency%20Model%20-%20Posts%20by%20MR901" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/udemy-course-open-source-llms/">(Udemy Course) Open-Source LLMs: Uncensored & Secure AI Locally with RAG</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/multithreading-vs-multiprocessing/">Multithreading vs Multiprocessing — Choosing the Right Concurrency Model</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/understanding-docker/">Docker — A Complete Practical Guide for Engineers and DevOps</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/understanding-mongodb/">MongoDB — A Complete Practical Guide for Engineers and Data Practitioners</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/a-complete-guide-to-mlflow/">A Complete Guide to MLflow</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/deployment/">deployment</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/jekyll/">jekyll</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/actions/">actions</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/gh-cli/">gh-cli</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/github/">github</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/nosql/">NoSQL</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/pages/">pages</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/performance/">performance</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/rag/">rag</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/reasoning/">reasoning</a>
      
    </div>
  </section>


            </div>

            
              
              






  <div class="toc-border-cover z-3"></div>
  <section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4">
    <h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  
    











            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/posts/understanding-mongodb/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>MongoDB — A Complete Practical Guide for Engineers and Data Practitioners</p>
    </a>
  

  
    <a
      href="/posts/udemy-course-open-source-llms/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>(Udemy Course) Open-Source LLMs: Uncensored & Secure AI Locally with RAG</p>
    </a>
  
</nav>

            

            <!-- The Footer -->
<link rel="stylesheet" href="/posts/assets/css/rst-overrides.css">

<!-- Mermaid Zoom Enhancement (loaded on all pages, activates only when mermaid diagrams present) -->
<link rel="stylesheet" href="/posts/assets/css/mermaid-zoom.css">
<script src="/posts/assets/js/mermaid-zoom.js" defer></script>

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2025</time>

    
      <a href="https://github.com/mr901">Mohit Rajput</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p><!-- Removed Chirpy theme reference -->
    Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
    
  
    
    
    
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/deployment/">deployment</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/jekyll/">jekyll</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/actions/">actions</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/gh-cli/">gh-cli</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/github/">github</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/nosql/">NoSQL</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/pages/">pages</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/performance/">performance</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/rag/">rag</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/posts/tags/reasoning/">reasoning</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div>

    

    <!-- Embedded scripts -->

    
      
      <!-- The comments switcher -->


    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  
  document.addEventListener('DOMContentLoaded', () => {
    SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('search-results'),
      json: '/posts/assets/js/data/search.json',
      searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{content}</p>  </article>',
      noResultsText: '<p class="mt-5">Oops! No results found.</p>',
      templateMiddleware: function(prop, value, template) {
        if (prop === 'categories') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
          }
        }

        if (prop === 'tags') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
          }
        }
      }
    });
  });
</script>

  </body>
</html>

