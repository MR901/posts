Course URL: https://www.udemy.com/course/the-complete-agentic-ai-engineering-course/
The resources are available at : https://github.com/ed-donner/agents/tree/main
https://edwarddonner.com/2025/04/21/the-complete-agentic-ai-engineering-course/    


Section 1: Week 1
Video 1. Week 1 - Day 1 - Autonomous AI Agent Demo: Using N8n to Control Smart Home Devices
------
Well. Hello. My name's Ed Donner. I'm a two time AI startup co-founder and a former managing director for J.P. Morgan. And I'm about to take you on a wild ride. We're going to be spending the next six weeks together on an adventure. I've actually just finished myself doing the six weeks worth of projects. There are eight projects along the way, and I can tell you I have some astonishing things in store for you and I can't wait to get started. Let's do this! Now, it's customary with these things to kick off with an introduction to myself and talk about the goals of the course, but people who've taken my courses before will know that I don't like doing that. Let's not do that. Let's get straight to action. Let's go and see some autonomous AI agents in practice, and we'll come back and do that other stuff later. So I've taken us now to the app N810, which is an exciting app that is definitely generating interest. This is an example of one of these low code or no code workflow apps that allows you to construct a workflow of orchestrating between different applications and build it without necessarily needing to write any code yourself. The difference with some of the other workflow tools like this is that it has generative AI built into it, and that's why people are so excited about N810. So if you come to N810, which is the cloud version of it, you can also download it and run it locally. But here we're using it in the cloud. You can press Get Started and set up an account if you want to do this yourself. Or you can just watch me. But it might be fun to try this out yourself if you haven't used it before. So I've set up an account, I've done that, and once I've signed in, I get this beginning screen to add my first step. You can also use one of their templates right off the bat if you want to, but we're going to build something ourselves right now. So we're going to add a first step. And what we're going to do is we're going to begin by starting with a chat message. So this if I go back to the canvas here, you press back to canvas on the top left. We can now see this canvas which is showing that we have a chat box here that's represented by this chat window down here. and we can chat and do something as a result of that chat. So what do we want to do? Well, we press the plus button right here and we come over to the right and we decide that we want to have advanced AI. And we're going to create an AI agent right here. And now, if I go back to canvas, I'll show you that what we have in our canvas now is a chat message going into an AI agent representing by this box. And then something can come out at the back of that. And you don't need to know all of the details here. This is more to give you an idea. And by all means you can play with this yourself. But most of this course is going to be about building things like this. But let's give this a shot. So the next thing we can do is add a chat model. And this is a large language model that's going to sit behind this AI agent. And I'm going to pick open AI chat model. Now that's because I have an open AI key which will be setting up for you if you don't have one already quite soon. And alternatively, if you don't want to do this, you can use Olama instead and run for free, but you would then have to install it Locally, but I've set up my OpenAI account and you can do this yourself. All you do is you come down here and you press create New credential and you type in your API key. But I've done that already. So my OpenAI account is linked. It's as simple as that. And here it is. It looks here OpenAI chat is connected to my chat model. So far so good. Let's keep moving. We're now going to add in a tool. Now, tools are one of the most fundamental building blocks of a gigantic AI. And this week is going to be a lot about building tools. So I pressed the plus button to open up the list of tools that I could connect with the AI agent. And you can see there's a large number of tools, a lot of different things that this agent could, could use, could take advantage of. It could do calculations with a calculator. It can book meetings using my Google calendar or read my emails, look at my Facebook timeline, lots of things that it can do, as you will see here, read Hacker News for me. I would like that. Okay, but we're going to go all the way down and we're going to find something here that I just want to show off, which is the Philips Hue tool. I don't know if you know about Philips Hue. It's something which is, uh, gives you light bulbs. And I have a lot of light bulbs in my apartment here, and we're just going to pick a light bulb. This one here called the bed strip. And we can add a field. We'll say brightness. We'll let the AI automatically choose what it wants to do with brightness. We'll add a field hue which is like the color. And we'll let that do that too. And that's all there is to it. We've now when I go back to the canvas, you'll see that we have given a tool to this agent that you can see visually here, that it is able to control the Philips Hue light bulb. So here we are looking at our chat, our agent, the LM and the tool. Let's get going with bringing up the chat message that's associated with this input. And let's say well hi there. And you can see it working and thinking hello, how may I assist you today. Let's say please Turn the lights on. Bright white. And let's see. And bam! Did you see that coming? On come the lights that I am, in fact, wearing my light strip. Uh, so here we go. Now, you may say to me, okay, that's cool. I see tools in action. I see an LM that I'm chatting with. It's controlling my lights in my apartment. Great. But it's not really showing me autonomy. I'm not seeing something where it's making its own decision. Well, then let me show you that. So to make it autonomous, we need to give this agent some agency. So let's say please pick a color. Let's say, uh, either deep red or deep blue and change the lights to that color. Let's see how it does with that. And there we go a lovely red comes out. Fantastic. So there you have it. We have an autonomous agent. Maybe it's a simple task, but nonetheless, you just saw it making its own decision, picking red over blue and changing the light bulb all through Nathan's web app. So look, that was a bit small and superficial, but nonetheless, it was our first foray into the world of Agentic AI. And I strongly encourage you to go and take a look at N810. It's so easy to use. It's so quick to set up an account, to hook up a web form to your Google Calendar, and lots of other things to try out, and it gives you a nice hands on sense of what it's like to work with agents. But I should say, this is the last time that we'll be using a tool, a low code, no code tool like this. As a user of AI, for the next six weeks, we're going to be rolling up our sleeves and we're going to be coding agents. We're going to be building them ourselves and working with frameworks that allow us to build and have different agents interact, be orchestrated to solve problems. Okay. So in the next lecture, I will tell you more about what I have in store for you. See you then.
------
Video 2. Week 1 - Day 1 - AI Agent Frameworks Explained: OpenAI SDK, Crew AI, LangGraph & AutoGen
------
So, as promised, I'm coming back now to tell you what this course is actually about. We're going to be doing three things. We're going to be working on theory, talking about what agents are and what an agentic architecture looks like. We're going to be talking about frameworks. So working with actual platforms that allow us to build and deploy AI agents. And of course, we're going to be getting hands on rolling our sleeves up, building some projects. Some of the projects will be more on the sort of entertaining side. Some of them will be much more on the commercial side, building real functionality. All of them will be fun. So I've organized the curriculum into six modules that I call the weeks week one through to week six. And I do that because I want to give you some motivation to be pushing on through this and keep up the drive. But you can take it at whatever pace you like, and if you can do it faster than six weeks, then so much the better. But, but that's how I've structured it. And each week builds on the last. The first week that we're on right now is called foundations. And it's all about grounding. You build building out the basics. Talking about what it takes to to have an agentic architecture, how different Llms can interact. And we're going to be building an agentic solution just with Llms interacting natively, not going through a framework. But in week two, we introduce our first framework, the OpenAI agents SDK, which is a beautiful framework because it's so simple and elegant and flexible and and will build some things with it. We'll add guardrails. It's going to be really great. In week three, though, we move to the very popular crew. I I'd say I'd say that crew is the fan favorite. People love crew. It's it's on the sort of low code end of the spectrum. It allows you to to write some configuration and use that to define your, your crew of agents that will solve your task. Um, and it's and it's a lot of fun to work with, perhaps on the opposite end of the scale in terms of, of of code, of low code to to to full code is Landgraf, which is very sophisticated, quite complex, very powerful. And we will be using all of its power. And in week five another powerful one is Microsoft's Autogen. Autogen is actually a couple of different things, and we'll look at all of them. But one of the things it is, is like an environment where agents can collaborate remotely. And that's going to be fun. And we wrap it up in week six with our capstone project and introducing MCP, the Model Context protocol from anthropic. Incredibly exciting. And it's really taking off in a big way. This is an open source way that different models can connect and collaborate, share each other's capabilities using a common protocol. So I'm really excited to show it to you. It makes a lot of sense to have that in week six, because we're going to draw on a lot of the things we've learned about in prior weeks. It's all going to come together, and that's very much the idea of this structure. Each week builds on the last as you build up your capabilities and you upskill all the way through to the end of week six, when you will have mastered engineering AI agents. And each week consists of five days, five different sets of lectures to build up your skill sets. During that week I give you Saturday and Sunday off. So just to show you what it looks like, I'm not going to talk to what we're going to do on every single one of the days. I want to give you that sense of, there's a lot going on. We're going to cover a lot of stuff. And this, this is color coordinated to match an earlier slide. That orange represents some theory. Building yellow is the project. And that first project we're going to do at the end of this week is going to be really cool, really fun, something that you'll be able to put into practice building your own agent that you'll be able to deploy into production, that we will do, that is able to answer career questions about yourself. It will be like your career alter ego that you can put on your website. Instead of having a resume. People can talk to you and talk about your experience and your challenges and so on. Isn't that cool? We're going to be doing that in the next few days. That's week one. Week two. We're going to be, as I say, OpenAI agents SDK. You'll see that the blue here, blue boxes represents a new framework that we are learning. And again, the yellow are the projects, including deep research, which will be a great app. Week three with crew. You can see we're going to learn about crew and then build a couple of different projects with crew. Week four is Landgraaf. Week five is Autogen and then week six is MCP and the capstone project, which is going to come at the end of this course, bringing everything together. So as I say, I'm not putting this up here to show you every single box. I'm doing it. So you see, we've got a lot of material to cover. There's a lot of areas in which we'll be growing skills, and by the end of it you'll have covered. You'll have ticked off everything you see here on this screen, the complete curriculum. And you will have true expertise in the world of AI agents. And I'm now just going to color in the the boxes which represent the delivery of major projects. And in particular, I would say that that the numbers five, six, seven and eight are some of the really exciting ones I have in store for you. I've just finished building them and I had so much fun doing it, and I can't wait to share it with you. And just to show you that, it's definitely worth hanging on in there for the next few weeks, let me tell you about those those last four projects. One of them is writing an agentic platform that's going to represent a whole engineering team, a team of different people, a front end developer, a back end developer, an engineering lead, and a tester that will collaborate together to write some software for us and we'll see them all in action. One of them will be, I call it sidekick. It's going to be an agentic platform that will be able to bring up a browser and interact in that browser together with you. So you will have your own sidekick that can do things with you. It's a bit like like a Marnus if you know Marnus. Of course, the Chinese agentic startup that's that's really caused again, a lot of excitement. But Marnus runs its platform in a sort of cloud box that you can't control. You can just watch it doing its thing. You'll have something a bit like that, but it will be on your box and you can work with it side by side. A true sidekick. Project seven I call creator, and this one isn't particularly commercial. It's just really interesting that this is one where we will build an agentic framework that is itself able to create new agents. It will actually give rise to agents and agentic platform that generates agents. And we're going to have those agents doing some commercial stuff just so there is a commercial angle to it. And project eight, the capstone project, it's something that I've wanted to build for so long, and I'm so happy to get to do it. This is a financial markets trading simulation where we will have multiple agents that can actually go off and make investment decisions. Searching the internet for financial news, looking up stock prices, real true real time stock prices and company information and read company annual reports, SEC filings and then make, buy and sell decisions trading on the market. Now that part is the simulated part, that we'll have some code that will just keep track of their accounts. And by the way, the code that we'll have keeping track of their accounts is code that will have been written by project five, by our engineering team, not by us. Just to add an extra dimension to this, some of the code would have been written by our Agentic engineering team. But what you see here in this screenshot is the results of this running. Just running yesterday. And it was actually a terrible day on the on the stock market. So all three agents lost money this day. But hopefully by the time that we get to week six, they will have recovered and we'll be able to actually show some profits that we're making in our virtual world. And of course, I should say now and I will say then that this should not be used for any real investment decisions. Of course, this is purely illustrative, but as an illustrative project, it's going to be a lot of fun. And while I hope that all of these projects will be entertaining, I will be trying to achieve two things from them. One of them is to have them be highly educational. Each project should allow you to upskill your capabilities and your expertise. The other is that I want them to be commercial. They should help you to have ideas of how you can apply this skill in business, whether it's a B2B or B2C kind of project. This should equip you to be able to put this into practice immediately the very next day on commercial projects. So now, just to introduce myself very quickly, just to show you that I am actually qualified to be talking to you about this stuff. So my name is Ed Dunner. I spent most of my career at JP Morgan, where I ran engineering and science teams of about 300 people. I started out in London, which is where I'm from originally. If you couldn't tell from the accent. And then I moved to Tokyo for a bit, and now I am based in New York and I am currently the CTO co-founder of an AI startup called Nebula. It's a nebula. You can go and check it out. And prior to that, I founded my own AI startup called untapped. And actually untapped was acquired in 2021. And this picture on the bottom left is a picture from the billboard in Times Square announcing our acquisition, which was a particularly magical day for me. So. So that was really, really an incredible moment. But I don't just show you this picture to show off about my moment in Times Square, but also because I happen to live about one block that away. If that guitar weren't weren't there in the hard Rock cafe, you'd be able to see my my apartment building. And I'm actually I'm facing the window right now that is, that is facing there. So in some way, if that guitar weren't there and this was some kind of a live feed, then, then then you would see me waving at you right now or something. Uh, another thing that I do is I love to be a speaker and instructor on all things LMN. And I have another Udemy course called LMN engineering, which is, uh, which has got 60,000 plus people on it and has been just an absolute joy. And I hope that some people that are on this now have actually taken that LMN engineering course, in which case, I must apologize to you because you heard almost exactly this same introduction when? When you took that course, too. So sorry. I hope you sped through it this second time around. Maybe it gets better on the second time, I don't know. Anyway, final thing to say, which I also said last time, is that here's a picture of me about to fly a plane, and you might think I'm showing you this because I am fabulously capable at flying planes. But no, I'm. AU contraire. I'm showing you this because my great skills in the field of LLM and AI is only surpassed by my complete inability to do anything requiring hand-eye coordination. So, in fact, if you find yourself in a plane and and you look in the cockpit and you see that it's me there in front of the stick, then you want to be looking for a parachute as quickly as you can. But but by contrast, if you find yourself taking a course about a gigantic AI and I am the course instructor, then you've come to the right place. This is completely in my wheelhouse. All right, let's get to it.
------
Video 3. Week 1 - Day 1 - Agent Engineering Setup: Understanding Cursor IDE, UV & API Options
------
So please humor me. I've got a couple more admin things before we get into action. So I wanted to talk about how I've positioned this course and whether it's right for you. You may be new to coding someone that's never written a line of code before. You might be someone that's already considers yourself an agent engineer. Those are the two extremes. Or you might be in the middle. You're already a Python coder or you are an AI engineer. Maybe you're even someone that's taken my LLM engineering course, in which case, thank you and welcome. But I'm here to tell you that this course is designed to appeal to everyone across that entire continuum. There is something here for everyone. The people in the yellow boxes in the middle. You're going to have the best time with this. It's most well positioned for you, that is for sure. That goes without saying. For the people that are new to coding, it's going to be challenging. There's no doubt you're going to have to have a lot of patience, but this course is okay. You'll be able to get there and build amazing things. For someone who's already an agent engineer that's built some agent platforms before. You may want to speed through some of this, but I've made sure that there's advanced material covered too. We're going to be doing some really interesting things at every point and building some great projects too. So I'm here to say, if you're new to coding, take some time with the guides that I've written. You'll find them in the guides folder when we install the repo. And I've used that as a way to build up your basic, your foundational expertise so that you can hit the ground running for Python coders and particularly for AI engineers. If you've taken the LLM engineering course, it's perfect. Then you're going to have a great time. And for agent engineer, focus on the projects. That's when we really put things into action. I'm really proud of the projects we've built. I think you're going to enjoy them, even if you've got some experience with this already. And for anyone that hasn't taken the LM engineering course, I'll be sure to put links to it in the course resources. And it's something that I really do see that as complementary, and it's not necessarily a prerequisite to this course. You can come into this course without having taken it for sure, but if you have taken it, you will be really well equipped for this course because it builds up a lot of depth of expertise in terms of what it means to work to to choose and apply LMS to solve problems. So in just a moment, we're going to be starting to code. I know you're antsy. I know you're waiting for this. Can't he stop talking and get to it? We will be. But I do want to tell you that the first thing we're going to have to do is set up your environment and setting up an environment, a big data science environment at the frontier of what's possible. It can be a bit of a painful process. I'm here to help. I'm here to make it go smoothly. But please do have something of a thick skin for knowing that you may hit some challenges. We'll figure them out. We'll get them fixed. We'll get you up and running the. This time, though, we're going to be helped out by a couple of really great friends in the form of the platform cursor, which we'll be using as our IDE. And I just love cursor. Cursor, of course, is the AI platform that's powered by Llms that allows us to be so productive, and it's built on VSCode and and it's just great. And so it's going to be a lot of fun working with that. But perhaps more specifically for building the environment, we're going to be using this product called UV, which is you can think of as something which is replacing Anaconda that I used in the LLM engineering course. And it's a sort of built on top of, of using virtual environments. It's really fast and it's really simple and it just works. I am such a fan of UV. It's actually a student on the LLM engineering course that that first drew my attention to it and said, this is what we should be using. And and it was too late for LM engineering. But it's not too late for this course. And it turns out that UV has become so popular that most of the agent frameworks that we'll be looking at already have UV at their core. Crew is, in a way, crew is kind of like built with UV, as you'll see. So UV is really easy to use. It is nothing like Anaconda for people that have done this before. And it's it's basically very, very similar to using virtual AMS and you are going to love it. So I know it's always annoying to have to bring in something new to the mix, but it will be worth it. You're going to thank me for this. You're gonna you're gonna love UV and it's written in rust and everyone loves things that are written in rust. So there you go. Get ready for that. There's actually one more difficult conversation that we have to have, and it's about APIs. So look, this course does involve making calls to frontier models like OpenAI and Deep Seek. And they come at a price. There is a price point there. And I want to make the point that it is completely up to you. You can choose. I use both open AI and deep seq, but you can choose to only use deep seq throughout for a much lower cost. And Gemini you can also use. And right now Gemini has a free tier. I don't know how long that will last, but so you can use Gemini for free for most of the course. You can also use Alama completely free just running open source models locally on your computer. No cost at all. But there are some trade offs here. If you want to use any kind of good model, it will take a very long time. If you use the quick models like llama 3.2, then it might have troubles being coherent, and you certainly won't get to see the kinds of results that I'll be getting when I'm using the frontier models, but that allows you to have a sort of balance. You can run things locally for free and see one set of results, and you can watch me do it with the frontier models. If you don't want to spend, the API charges yourself. So what are these API charges? Well, different countries might have different pricing. What I can tell you is that for me, running this entire course, I spent well under $5 on any of the LLM model costs. It's very cheap. OpenAI the first time you use it, you need to put down at least $5 upfront. And I kind of pay as you go that you spend against. So that's annoying. So you might have to do that if you haven't done it before. But other than that, the actual amount that we'll spend will be relatively small, a matter of a few dollars. And it's very hard to spend very much on deep sea because it's so cheap. Now, I will say that in the last week I have an option to use a proper real time market data provider that I use, and that cost me $20. But that's not necessary. There are free options as well, but if you get into it like I do then, then you might do that. And you know, I just want to make the point at the end. People do have like like people get frustrated about API costs. There's something about them that are a bit galling, because it feels like everyone is sort of nickel and diming you. You're getting charged a little bit here, a little bit there, another API cost. And I understand that even though I say these things are very cheap, a few dollars, it adds up. And that's not always cheap. And particularly with exchange rates and different economies, that might be quite a burden. But I do want to make the point that you have to keep in mind what's going on. When we run inference on these large llms, there are trillions of floating point calculations going on. This is heavy machinery. And so it's not like these companies are just just making extra cash off you. I'm sure they are quite profitable, but but most of it, the margins are relatively slim because there's a lot of compute costs associated with running these massive models. And if you think of the price of buying a new laptop or even a big like a gamer box that could actually run larger models itself, you'd be talking about thousands of dollars. So this kind of API pricing, while it can feel, as I say, a little bit galling sometimes having to pay each of these different APIs. It is worth keeping in mind the context of what's going on behind the scenes. These companies need to pay their electricity bills. There's a lot of compute, and we're getting real value from what we use. So I don't know if that helps or not, but it gives you some context. Please do keep in mind you don't need to spend anything on APIs if you don't want to. So there are three things for you to remember. The first of them is that there are resources that accompany this course, and they're really great. There's a website where I've put links, and I've got extra content in the form of YouTube videos that should should be helpful. There's in GitHub, there's a whole section with different guides to teach different kinds of skills that you might need. And there's some troubleshooting guides in there too that will fix problems for you and the labs. So the labs I'm updating them all the time, the labs you can think of them as like an ongoing a fluid resource. Every time you do a git pull to get the latest, you'll get refreshed with new content. Maybe not every time, but most times I'm trying to keep them to be a living, breathing resource for you. Then remember to stay patient. Remember that if you hit problems, if you hit roadblocks, that's actually a great learning opportunity. Because figuring out what's going on, figuring out how to solve this is one of the best ways to learn and try and go with it. Try and enjoy it as much as you can. There's some juicy projects and figuring out why they don't work first time. That's part of the fun. And then the third point is that if all else fails, or even if nothing else fails, then contact me. I'm here. I love this stuff. I'm actually super responsive. If you email me or if you LinkedIn with me, then I'll respond very quickly. Unless I'm in a meeting or I'm traveling or something, or I'm asleep. For people in different time zones, I do tend to reply really quickly. People are always surprised. In fact, a lot of people think that I'm I'm an AI agent. When I reply, they're like, do you have an agent of yourself? But no, it's the real me, I will reply, I love getting questions. I love answering so, so do feel free to reach out. Be in touch. I welcome ideas and thoughts and questions and whatever you got. And by all means, if you if you're up for it, then LinkedIn with me I some people uh, like to uh, don't feel comfortable doing that for some reason, but I'm very open to it. I love building a community of people in data science and that can support each other. That can if people are looking for for to hire someone or if they're looking for jobs. And I can help connect people. So it's a really great way for me to build and contribute to the community. And this is really important. But one of the things that people did from my last course is they would post projects that they've done and tag me on them, or things that they've done from the course, and I can then amplify it, because I can then jump in with some comments and, and make some observations or say how great it is, they've done something, and that will then be available to all the other students from the course. You can weigh in as well. And this has really happened, and it allows you to share things and amplify them. And LinkedIn is a great place to do that. And it will be something that that will be seen, perhaps by future clients of yours or perhaps by future employers of yours as well. So it's a really good thing to do. So I strongly encourage it. LinkedIn with me and share on LinkedIn. It's a great resource and I can't wait to see what you're doing. So at this point, you've put up with half an hour of me yammering away. Finally, it's time for action. We're going to the lab. We're going to set up your environment. The next video is for PC people. We're going to have a video for PC setup, and then the video after that is for Mac people and Linux. You should probably join in with the Mac people. It'll be similar enough for you. And then we will reconvene on the video after that. So it's going to be PC, followed by Mac, followed by us all back together again with fully built environments. Let's do it.
------
Video 4. Week 1 - Day 1 - Windows Setup for AI Development: Git, Cursor IDE & UV Package Manager
------
Well, if you're a PC person, then you've come to the right place. This is for you. If you're not a PC person, then get out. You're not welcome here. All right, with that out of the way. Hello. This is the video. When we actually get to set up your environment and there are going to be five steps to doing this. The sixth one will be somewhat optional. There will be a sixth step, but the five main steps we are going to do, and they are going to be covered in my repo very clearly. And let's start by going there. We're going to go to the repo that is linked in the course materials. It's at GitHub. And here it is. And when you go there you see the different directories, the different folder structure that we have. And if you scroll down you get what's known as the readme, the set of information that comes with the course. And I ask you, I plead with you to read the readme. I put a lot of care into the readme. It's got lots of great stuff in there, and so do take a look through it and it will set you up to be successful in addition to another link to my LinkedIn. I'm starting to sound a bit desperate about this. There's also lots of useful things, and most importantly, there's links to the setup instructions, the windows setup instructions, the Mac and the Linux ones. And I'm not going to take you through them right now because this will be a living, breathing document that I will keep it updated as people hit problems. But you should go through that and have that side by side, if anything, that's going to be better than this video. See this video as being bringing it to life. But the instructions will be bulletproof. So it's time for step one and just before starting step one, I do want to draw your attention to a section at the top of the setup PC guide. That's called gotchas, and it has a few of the common traps that can happen when setting things up for PCs. One of them is an insidious problem, with windows sometimes being set up to not allow names of files longer than 260 characters. And if that is set on your system, then you need to to unset it so you are allowed longer filenames. And that's something which there are instructions for how to do that. Another common trap is around antivirus software that can interfere with the installation. So there's some steps there as well. So please do look at those gotchas because they will set you on a good path. All right. Step one is about cloning the repo using git. And to get started, if you go to the start menu and you then type PowerShell, you should be able to open up a PowerShell. One of these things. Let me get rid of the other one that's running here. So here is a PowerShell. And this is like like for for running commands in the command line. If you're new to this, one of the guides in the guide folder in the repo contains within it. So in this guides folder there is a guide for using the command prompt that will put you in good shape. But I'm imagining at this point you are familiar. So you know that that there could be a projects directory. Most people have created a projects directory in which you should go into it. If you don't already have a projects directory, then you should create one with mkdir projects within your home directory. So you will then have, like I do, a projects directory within your home directory. And if I look at the contents of it, it has my LM engineering folder from my other course, and it has some test folder. And it is this folder that we're going to clone our new repo. First we need to check that we have git installed git for code management. And you can do that by typing git minus minus version. And we'll see that I do indeed have git installed. If you don't have git installed and that has an error message, then you can go to Git's website and there's a link in the setup instructions and go to downloads and to windows. And you can click here to download the latest version for windows. And the installation instructions should be crystal clear and make sure that you have that in there. You might need to open up a fresh PowerShell. Always worth knowing that you might need to do that and sometimes you need to restart your computer. Super annoying, but hopefully not. You should be able to type git minus minus version and see git in there. Okay, so the next thing to do is to actually clone the repo. Okay. So we go to the repo itself. Here we have it. And uh the first thing we do is we go to this green button on the top right of the repo, as I'm looking at it here in a browser, and you click this code button. You select Https and you'll see the web address of this repo. And you press the copy URL to the clipboard button. And there we go. I have copied it. And now you come here to the PowerShell. Now again I'm in my projects directory. So you're in the your home directory then your project subdirectory. If your C drive is quite full then you might want to do this on your D drive. Just make sure you're within a project's directory. A good place for it to be. And you simply type git clone and then the paste in there the web address of the GitHub repo. And when we press enter that runs it does its thing and it's done. And if I look at that directory you'll now see that agents is there. I can now go into that directory agents and look at it. And here are all of the directories within the repo. And this folder that we're in right now which is projects agents. This is what sometimes known as the project root directory. It's what people call this. And it's a very common place to go. And this is where we are. And that means that we have completed step one of the five step process. Okay. So we go right away to cursor to look at the cursor IDE, a fabulous platform, and download it and install it if you don't already have it, and you can just press the download for windows button. It will download the executable and you can then run it and you'll get this setup screen. A familiar kind of setup. Accept the agreement, choose where you want it to install it, and I choose the C drive, but you could put it on the D drive if you're running out of space. And then next and then we will create a desktop icon and add it to Windows Explorer. And note that it will make changes to path, which means that we will need to bring up a new command prompt for this to take effect. And then we will install that. So it will now install cursor on my PC so that we can use this platform. And I will jump forwards and see you in a second. And cursor has now installed. And so I can press finish. And it should now be installed and launching on my PC. There we go. It's right here. The, uh. Oh, no, it is starting. It is starting. Here it comes. It just took a moment. And we can just keep with all of these basics. And I'm going to select to use the word cursor if I want to run it from the command line and say yes okay. And then continue. And I'm going to have it in privacy mode, I'm not going to, to uh, be logging what I do. And you are all set. So now you can sign up for a cursor account, which is free, and you just press that sign up button and follow the process. And uh, then, uh, once you're finished with that, I will see you back here in a second to actually use cursor. And so here we are in cursor. It started I've signed in, I've actually got a pro trial. And we're looking at the first screen we've got here. And I can press Open Project. And what I can now do is navigate I'm going to I'm going to actually go this PC and then to my C drive and then to users and add and then projects, which is where my projects are. And here is agents. And this you'll recognize as the directories that we cloned from our repo. And what I can do now is just press this select folder button which is saying I want this agents folder to be my project root folder. And it opens up. This is your first look inside cursor. We've got our directories down here. You've got like a welcome screen up here I'm going to X out the welcome screen. And now here we have something called the explorer on the left which has all of our directories. We've got a chat ability to chat with an LM on the right. I'm also just going to close that down. And what we're looking at here is the code. And in fact I can this navigator on the left is showing you a different directory for each of the six weeks of the course from foundations through to MCP. And if we open this swizzle here, you'll see all of the contents of the first week foundations where we are right now. Okay. We're racing ahead to part three of the setup. Part three of course is time for UV, which I'm so excited to to show you. So we start by just looking for, uh, UV package manager. You will, uh, find it's also linked in the setup instructions. Astral is the name of the company that makes UV. Here it is. And it is an extremely fast package manager as it tells you. So to install for windows, you simply run this in a PowerShell. And so I've pressed on windows I'm now going to press this copy button to copy that in. And I'm now going to go to a PowerShell which we will do right here PowerShell. And I'm going to run that command. So here we go. We'll let it do its thing. It's now installing UV. And that appears to have already happened. And it's telling you you can either restart your shell to to take this into the path or you can run those commands. We're going to restart our shell. If we type exit to get rid of that shell. And we start a new shell and you must remember to do this, otherwise it won't work. So PowerShell again. And now we should we should have access to UV. And so we can type UV minus minus version. And there we have UV installed on the machine. Now I'm going to show you this next part within cursor. Here is cursor I actually quit cursor and reopened it again for the same reason that we did that with a PowerShell to to reset the path variables and within cursor, I'm going to go to the view menu and choose terminal. It's also control and the Backtick to bring up a new PowerShell within then cursor, which is a useful thing to do. And that's how we'll do it, mostly on this course and within here. If I do a dir you'll see, or an LHS, you'll see that the contents of that directory, I'm now just simply going to do the command UV sync, which is my way of telling UV that I want to build an environment consistent with what I have in this directory. And I do that. And what we're seeing now is it's building the environment, and it's actually beginning by installing Python, a dedicated version of Python 3.1, which is the version for this project, and it's now installing the packages. Now, people who are familiar with Anaconda know that this process of building a full specked, isolated environment with the right version of Python is something that can take up to an hour, and sometimes on some systems, it was taking an hour and a half to build this environment on their computers. What you'll find is that it's a different story. With UV, it's running remarkably quickly, and I'm hoping it's going to finish by the time I finish this sentence. But if not, then I may put you on pause and come right back again, because it literally takes a minute or two and it will be done. So I'll see you in a minute. Okay, so it did take about another three minutes, but I didn't lie. It's quick. It's very quick and it's very impressive. And the big news is that that's it. There isn't any more of the kind of configuration management part those packages have been installed. If I look in this directory, you'll see that there is in fact going to be a new folder. Venv. It's created a virtual environment for people that know about Python virtual environments. It's built one and that now exists in this folder, and that's all there is to it. And going forwards, if we want to run a Python script, whereas you might normally type like Python and then the name of a Python script you now would type, you've run and then the name of the script and it will automatically run it in that environment. And everything else is the same. The. The environment is built. It's as simple as that. We're pretty much good to go and we're now in the end game. We're at step four of the instructions and it's smooth sailing from here. The next step is to set up a key with OpenAI so that you can use OpenAI directly. If you already have a key, you can just skip this step if you don't want to be spending money on OpenAI. There are alternatives. As I say, you will find them in the setup instructions and the readme and in the linked in the guides. So please do follow those instructions. But I do assume that most people are okay to spend a few dollars and is quite cheap on using OpenAI, because the power of these frontier models is going to allow us to do great things. And of course, you can use Deep Seek or Google's Gemini or Grok as a straight replacement for any of these. Okay, so I go to a browser and I go to platform. Which is where you get into the sort of API side of OpenAI. And the first thing that you may need to do is sign up. I have already got an account so I can just log in, but you may need to come in and sign up. If I log in, I can come in with my Google credentials. And here I am logged in as me to the OpenAI developer platform. Now, there are now two things that you need to do. One of them is about setting up billing, and one of them is getting an API key. And we'll do them both now. Okay. And so in order to set up your billing, you go to the settings menu up here. And then you go down to billing right here. And this is where you choose to put in a credit balance. You can see I've got quite a large credit balance. I seem to spend quite a lot of money on different projects with OpenAI, but this course will only cost you a few dollars. But the way that it works is that you do need to put in an amount up front, and $5 as of today is their minimum. So you'll need to put in that $5 and then you spend against it. For people internationally, I know that sometimes OpenAI can be a bore about charging because they charge in dollars and some credit cards don't accept an international charge like that. So there is some some nonsense to get around with that. And as I say, there's alternatives like deep seek if you want it. Some students use grok with a Q, which there's also a great option. And you can also use something called open router that's quite popular, particularly with international people, students, because you can set up an open router account and it will route through to the different models. So these are all different approaches for you. And they work almost exactly the same way. There'll be some sort of billing page like this where you get to choose to top up and you press add to credit balance to top up with a credit card, and you put in the minimum $5. We won't need any more than that for sure. Okay. And so once you've done that, you then have to go to the API keys section. And this is super important. And this is where you get to set keys. That will be the how you will access this API from your computer. And the way that you'll do that is you'll press create new secret key. You'll put in the any name you want. You can call it just to remember it. In the future. Make sure that you select project. You have default project so it's for everything. Have all permissions and then press Create Secret Key and you will get a new key. And you can copy it into your clipboard. And you have to do that to make sure that you preserve it. Because we're about to make use of that API key. Now here's a pro tip to be really careful about. When you copy that key into the clipboard, you might be tempted to paste it into another application like a notepad that you will then copy and paste it somewhere else. But there's a there's a trap there. Sometimes these kinds of word processors, when you paste in the key, they'll they'll mess around with it in some way. They'll replace hyphens with long hyphens because they look better in text, and they may replace some characters with international characters that are right for your locale. And if they do that, your key won't work, and it's going to take you ages to figure out what's happened. So be careful of that. Don't do that. I suggest copying it to the clipboard and then immediately doing the next step we're going to do, which is actually using it in our project. So keep that trap in mind if you do want to paste it somewhere to keep it for your records, then be careful to paste it into a kind of tool that's not going to mess around with the API key. All right, I hope that makes sense. I hope you've now created your API key. If you don't have one already and you've got it and you've got it nice and safe, and you've recorded it somewhere where it's not messing with the characters, and we're ready to put this to good use. And this is it. We've reached the final step, the final step of setting up. Thank goodness we're back in cursor. And I've actually I pressed X on the PowerShell that was here. So we've got a nice clean screen here on the left. This is the explorer I've collapsed all of these so that you just see the top level explorer. And what I'm now going to do is right click here and say new file. And I've now got a new file I'm creating at the top level. Make sure that it's at that top level. And I'm going to call it something very specific. And you have to call this file exactly the same thing. And that thing begins with a dot, a period. A full stop, as we say in England. And then the letters E and N and V. So it's called dot EMV, and it has to be called exactly that for a very particular reason that we'll get to later. But as long as you've called it Dot and nothing else, then you're in great shape. So that is the name of this file. A dot EMV file is a common way that people put in environment variables that they want to set just for the purpose of this project, and they will be environment variables that will contain what we sometimes call secrets or things that we don't want to to be public. We don't want it to be checked into source code. We want it to be something that is just for us, kept private, that we will use, and we can just put things in this file of the form of like a key name equals and a value, and that will become an environment variable, as you will see. And in particular, there's one key that we really care about. And you have to do this exactly right. If you make a typo in this, then it's not going to work and it's going to be really frustrating. So try to be really careful with this. I'm going to type open and you can see that already the cursor is already it knows what I want to do. See how it's filling it in for me. I could just press Tab to do what it wants, but I'm not going to press tab. You will see because it's got a dummy key in there, not the key that we want. Open AI underscore API underscore key equals. And you can see it's prompting me there ESC dash dash and a number. Now what I want to put in here is something that will indeed start ESC Pro. And it will have a number. But it's going to be a long number. And it's going to be the thing that I just copied into my clipboard from the previous screen from open AI. You need to now paste that key right here and it's got to look just right. It's got to start ESC and then be followed by that that long long number and letters. And it's got to have those hyphens in them just as they are. And this is of the people that struggle with environment setup. Most of the problems end up just being this one tiny step. So please get it right. Uh, so hopefully you've done that and you've put that in there and it's got a key in there. Now, if you're using other platforms like Deep Seek, then this is exactly what you will do. The same thing you'll just put in here deep seek and it will. There we go. It fills it in for me. Deep seek API key equals. And you might have something else. You might be using grok the inference platform or open router or any of the others. Any keys that you've got, you simply put them in here like that and we will use them later. So with any luck, you've put in OpenAI API key equals and it's got your real key in there, which means that the setup is complete. Many congratulations. That's the painful part over as long as it all works and we'll find out, please skip the next video, which is going to be for the Mac people, the next lecture, and then we will reconvene after that. And I'm so excited to get started.
------
Video 5. Week 1 - Day 1 - Setting Up Your Mac for AI Projects: GitHub, Cursor IDE & OpenAI API Key
------
Well. Hello Mac people and Linux people, welcome to your setup video series. If you're a PC user, then what are you doing here? We just set up your environment. You're set. Go on to the next lecture. All right, Mac people, I had a rough time with the PC lot. I am this is this is my sweet spot. I'm a I'm a mac user, so this hopefully is going to be very easy and fun. All right. So I'm in a finder window here. And I've gone into utilities applications utilities to show you that that's where you can find terminal to open up a terminal if you're not familiar with that. But I'm going to now assume that you know about this and open up a terminal. Here's one right here and make that a bit bigger for you. Hang on. So this this is where we are going to do some of the next few steps. And we've actually got a five step process for setting up your environment. And this is the first step which is about cloning the repo or getting a copy of the code for the class onto your local box. Now I'm in my home directory here if I do PWD. If you're not familiar with the command prompt, I've included a guide to get you more comfortable more confident using the command line. It's common for people to have a directory called projects in their home directory where they put all of their their work projects. And if you have that, then if you do CD projects, it will go in there. Now it looks like I don't have one, so I have to make one with make directory projects and then change to projects. And now I'm in my empty projects directory, but you may have a few projects in there. So the next step is now to bring the GitHub repo here. And so what we now do is we go using our favorite browser to a, a page where we have the repo which is linked in the course materials. This is what the repo looks like when you go to GitHub to my name and you see the repo here, here are all of the directories, one for each of the weeks that we have coming up ahead of us, and then some general stuff. And if you scroll down you see something that's known as the project Readme. And it's really important because it's got great stuff in there. And I urge you to read the readme and the things it links to. It begins with some stuff which includes another appeal to LinkedIn with me. I seem to be quite desperate on this front. And then comes the setup instructions. And there's a link for windows. Instructions for the other guys. There's a link for Mac and Linux. Come follow that link to get step by step instructions. And to be honest, those instructions are going to be way clearer than than this video. And I'm going to keep them updated when people have problems. So that should really be your first place to go. And this video see it as like a video that goes along through it. So you get get a sense for it. But the Readme also has lots of useful resources and a ton of important stuff on API costs that I urge you to read, because I know this is a sensitive topic. And so please do look through it and read all the other stuff and the links to the guides and so on. And I should probably mention that if you've not used GitHub before, you might need to create a new account with GitHub to get to this point. Now, just before we clone this repo locally, I'd like to go back to the terminal window and I just want you to type git minus minus version to check that you've got git installed on your Mac, and it should come up like mine does with a version number. If it doesn't, you might need to run this command. Xcode select minus minus install, which installs some of the basic developer tools on your Mac. I've got a link to that in the setup instructions. I won't do it since I already have Xcode and Developer tools installed. Okay, so with that, if you're typing git minus minus version, you're getting a number there. You come to the repo, you've signed in to GitHub, there's this green code button on the top right. And you press there. And probably I recommend that at this point you select the Https version. I'm going to pick this version because I have that kind of credential. But but you can also pick the other one. and then you copy that into your clipboard by pressing the copy button right there. So once you've copied this text, which is the link to this, this repo, you then come back to the command line. And in your projects directory there I am I'm inside users ed projects. I'm now going to type git clone, and then the repo link that I've pasted from my clipboard. And I press enter and in it comes and it's done. If I now have a look you'll see that there is a directory called agents. If I go into agents I now see that I have directories for the full project. And I want to mention that where we are right now in this directory that's in my projects directory, the agents directory. This here, this is known as the project root directory. If you're inside the sort of top level directory of your project of agents. So this is the project root. And you'll hear that referred to during the course and generally. And so it's worth knowing that that's what it is. But that is step one of the setup complete. Okay. Welcome to step two of the setup for Mac. We're going to now install cursor the I IDE which is great. It is fabulous. And you're going to love it. If you've already got cursor then of course you can skip this step. Otherwise bring up your favorite browser and go to cursor. And here we have the screen for installing cursor. You will need to begin by creating an account using the sign up button over here. If you don't have one already and it will just guide you through. I use my Google credentials myself, and I selected a Trial Pro plan myself, which is a free at least for now. And I then press download for Mac, which then brings down an installer. I then run that installer and it then asks me a couple of questions. I log in and then it installs it. I mean, I'm sure you are used to these things. You then drag and drop into applications and you open that up and it will install it. Easy peasy. I selected all of the default setup instructions, nothing special at all. And then you should launch it and you should be in cursor. And that is where we will go next. Okay. So once you've opened up cursor freshly installed this is what it should look like a nice big empty screen. And we're going to press the open project button right here. And we're going to then navigate to projects wherever that is. I guess it's going to be at the top projects and then agents, which is the new folder that we've just installed. This is known as the project root folder. Here it is. And then we're going to press open. And here it is. Cursor opens up. We've got a nice screen here that's known as the explorer which shows all of the files organized by directories. One of these directories might be open. You can just click the switch to collapse it like that. There might be a box up here with some sort of welcome to cursor, blah blah blah. Just X out of that extra box on the top left if it's there for you. Over here is a chat screen where you can chat with an AI about your code. And I'm going to suggest for now we x out of that as well to keep things really clean. So what we're looking at now is a nice big area here. And the code the explorer to look at our directories right here, including the Readme and so on. And congratulations you are into cursor. And that is the second step of the instructions completed. And we are now onto step three of the instructions which are my very favorite step where you're going to install. You've the package manager which is so fabulous, so quick and easy to use, and you're going to be really happy about this. I know it's like another new thing to learn, which is always a pain, but I tell you, it's so great. The reason we're doing this is partly because a student on my prior course, said, you know, you really should use UV. It's so much better. And so I'd heard of it. I knew it was doing the rounds, but I hadn't actually used it myself until that, and that pushed me to do it. And I've never looked back. And we'll find that many of the AI platforms, the agent platforms will use or use UV themselves. So it's going to work out really well for us. You're going to be grateful I showed you this if you're not already a UV convert. All right. So first step is to install UV. And the way you do that is you bring up a browser window. And yeah you can just Google for UV package manager. Or you can look at the link that I'll have in the instructions, and you'll see that there is the um link to the GitHub. And it has within it some clear installation instructions. You just simply have to run this one command in a terminal, and I will copy that to the clipboard. There it is. Now there's actually a new easy way to run things in a terminal because you can do it within cursor. Here's my cursor screen again. And there's actually a view menu. And you can go view and then terminal. Or you can know to press the control button not the command button, not the one with the squiggle clover on it, but the control button on the bottom left of your keyboard control. And then the tick mark on the top left of your keyboard. Or at least on my keyboard. And up comes a terminal. But you could also go to the view menu and choose terminal. And here it is. And now you can simply paste in that command to run the installation for you've. And it will be super quick and you'll be done. I'm not going to run it because I've already run it. Once you've done that, you might need to close this terminal by pressing the the X, and you might actually need to to quit out of right right out of cursor and bring back up cursor because it needs to refresh its environment variables. But but if you've done that then if you type UV minus minus version, it should give you a version number which is telling you that all is good, UV is installed. And uh, with that we are almost ready to go. There is now only one command left. And what is that command? I hear you ask. It is simply to type at this point. You see like that. And what that's going to do is it's going to look at some of the other files here, like a Python version file and a pyproject.toml and some other things and a UV lock. And it's going to use that to build our environment and let's see it doing that. Off it goes. Now I told you that it's quick and you'll see that it was very quick. That is just done. But that's actually quicker than it will be for you. And it's that quick because it does cache things that I've done before. When you do this, it's going to take about three minutes, four minutes, which is still very quick because Anaconda takes like an hour or something. So it's going to be fast, but there is still a lot to be installed, so you'll have to sit back and wait while everything installs, but enjoy the raw power of UV as everything gets built there And once it's done, what you may notice is that there's a new. Directory called dot that's appeared, which is like a traditional Python virtual environment. If you know these things that it's created one for this project. And so one of the really nice things about UV is that it's compatible and consistent with all of the other ways you do things in Python. It's not like there's a new sort of thing like Conda. So it's that's been set up and now that is going to be the environment that we'll be using from this point onwards. And when you run a Python script, whereas before you might have typed Python and then something, what you're going to type now is you've run and then that same thing and that automatically makes sure that it runs in that environment. The other thing to mention is that it has installed a dedicated version of Python, right? For this project. It is in fact a Python version 3.1. Uh, whilst the latest version of Python is 3.13. That's not yet not. Not all data science packages are ready for that yet. So 3.1 is what we're using and everything will be in that Python. It will have installed it for you if you didn't already have it, and it will be dedicated for this project. UV is really good. It combines this kind of isolated environment with something that is fast and simple, and I very much hope that this will have simply worked for you. And that means that step three is done and we can move on to step four. So for step four, we're going to set up a key with OpenAI. Now again I want to emphasize that whilst I do encourage you to be setting up a key with OpenAI and to be using that API, it is not required for this project for this course. And there are alternatives in the Readme, but that's what we're going to do now. I'm going to assume that you are good for this and that we can come in. And so you go to platform dot Openai.com just here like that. And this brings you to the front screen of OpenAI where you get to set up an account. If you don't have one already, sign up or log in if you have one. And I'm going to log in because I already have one. Log in with my Google credentials. And here we have my account. We're logged in and you will hopefully have done the same. Let me make this nice and big for you. Okay. So there are now two things that we have to do now that we're in OpenAI. And we're going to go and do both of them now bearing in mind this is not required, you can also do the same thing, this analogous website for deep seek and for many others, you could be setting up your keys in any of them and just follow the same kind of process. But let's get to that now. So as I say, it's two steps. The first of them is to go to the settings menu up here and come down to billing. And this is where you set up your billing details. Now you have to apply a credit balance giving OpenAI some money in advance. And I've given it quite a lot of money right here because I'm doing various different things and you do not need to give it that much money by any means. There is a minimum top up. They require at least $5 as of right now, but they are. They sometimes have deals. There's something going on down here that you could read about. Perhaps there's some some deal for free tokens right now, but there's different deals and you can press the Add to credit balance to put in your $5 up front. That's that's, I believe, required right now. And we'll only spend a few dollars of that on this course, and that you have complete flexibility and you can come back and watch to what extent you're spending your credits as we go through. But but I suggest doing that, putting in your $5. But bear in mind, if you don't want to, you don't need to. Once you've done that, you then turn to the API keys. Part of this, which is where you get to see the key that you've set up that you will use for this project. And if you already have a key, then then great, you're in good shape, but I assume you don't, in which case you press this green. Create new secret key. You leave, you give it a name that just doesn't matter. Just for your future reference. And then for projects, be sure to choose default Project and leave it as all permissions and then press Create Secret Key. It's then going to prompt you. It's going to bring up your key that you'll use. And you can copy it into your clipboard. And we're going to paste it in a moment into cursor in a place where we were going to use it. Now here's the thing. Here is a nasty gotcha to be aware of. It's very tempting to take your key and paste it into a useful app, like, like the notepad or something, but there's a trap there you need to be careful of. Some programs, I think, including notepad. If you paste in some text and then press enter, it changes some of the characters in that text to be nicer, formatted like it turns hyphens into long hyphens, and that can be really hard to spot. And that will then basically invalidate your key. Your key will now be a different set of letters or characters, and when you try and use that in your file, it's going to fail and it's going to be almost impossible to track it down because it's going to look perfect. And it's really frustrating. And a lot of people have had that problem. So I urge you just to copy it into the clipboard and paste it straight into cursor in the next step. And it is good to to keep a copy of it somewhere. Just be really careful to make sure you do it in a way that doesn't overwrite your characters, so that you're keeping the original and and it feels in good shape. So there is my, uh, public service announcement about about things to watch out for with keys. But hopefully you've set up your key, you've copied it to the clipboard, you've maybe made a backup somewhere safe. And now we're ready for the next part. And this is step five of the environment set up. And it's the last step. We're almost completely done. We're back in cursor again looking at cursor. And I'm just going to I'm going to press an X here to get rid of that screen. I'm going to press an X to get rid of my terminal. So we're looking at a nice clean page here. now. Down here in the explorer. I've collapsed all these folders. I'm going to right click and say new file at the bottom here. And I'm going to create a new file. And I'm going to give it a very precise name. And you have to give it exactly the same name. If you get this wrong by a single character, then you're going to have problems later. So so be careful with this one. It starts with a dot, the period, a full stop, and then the letters E, dot, E, env. And it's got to be called exactly that. And you'll see why later. But basically this this is a file which is often used to store environment variables, things that will load into our environment as variables that we'll use for things like passwords and tokens and keys. And it doesn't get checked into GitHub. So it stays as a private file just for you. And it's a common technique for managing these kinds of things. And it's called dot env. And it has to be called exactly that. So what we can now do is type into here a key that we want to to remember. And in particular the one that we want to remember is OpenAI's key. And it's it's OpenAI underscore API underscore key. And it's actually prompting me right here for it because cursor is that clever and knows what we're trying to do. And we can just press Tab to fill it all the way in. But I'm not going to do that. I am going to type OpenAI API key equals no quote marks, no spaces, no spaces before or after and people. One common mistake is that sometimes people say open API key or OpenAI key, but it's not that it's OpenAI API key. And it's got to be exactly this. I think this is one of the only things in this course where every letter needs to be right. And so be careful. This is like 90% of the reason that people are frustrated in environment setup. Is this this very file. All right. What comes next is you simply paste in here exactly the key that came from OpenAI. And it should begin ESC dash proj dash, and then a bunch of letters and numbers with maybe an underscore symbol as well. And that should all come there. A bunch of them, not 123456780 as it has here in cursor, but a real key and not SK proj, and then another SK proj or anything like that that some people have done. It's got to be just right. Sorry to go on about it, but this is such a cause of problems. So so make sure that that key, the thing that's showing in purple here, is exactly the key that you took from open AI. And it should look perfect. And once you've done that you do command S to save this file. And you'll be in great shape. And at that point that will then be the end of step five of the environment setup. We will have done environment setup and we're good to get into the to the actual project. How about that. So congratulations. Thank you. Hopefully this is now all going to work. We'll soon find out. I will see you in the next lecture when we reconvene with team PC.
------
Video 6. Week 1 - Day 1 - Building Your First Agentic AI Workflow with OpenAI API: Step-by-Step
------
Welcome back everybody. The Mac, PC, Linux people reunite for our video where we now start to work with our project. Your environment is set up. You should be looking at cursor looking nice and clean like this X out of any windows that are open, have it look a bit like like me. And then on the left here you've got the directories. The first one up here one foundations represents the directory for week one where we are right now. We open that up and I'd like you to click on lab one that comes up right here at the top. Here is a little section that tells you about the setup folder, which has all of the setup guide that by this point you should have already covered it all. So it should be fine. And then the guides folder has a bunch of useful guides. It has some stuff to take you through intermediate level Python, and it also has beginner Python. If you're new to Python coding, then it will take you through some self-study guides there. If you're willing to to to do that, I think it will be. It will. It will need some time. Investment will be well worth it. And then for intermediate level Python there's important stuff there too. There's a bunch of stuff. One of the things that comes up a lot in agents is working with async code. There's async stuff in there. There's also going to be things about generators which are very important, and a lot of good intermediate level Python material in there. If you're also not as experienced with coding, there's there's going to be stuff about debugging how to go about debugging. If you get something called a name error, that's something which has always got a way to work at a name error. So there's good details on that too. So please do look at the guides. I've put tons of stuff in there. There's also a troubleshooting guide if things go wrong. Troubleshooting takes you through every step, including some great diagnostics at the end. And then this next little comment here is saying that these labs think of it a bit like it's like an e book. It's got content and information as well as things to try. And what you'll find is that what you'll see as we go through this may be different. What you have may look different to what I go through right now. And you may say, well, that's annoying. I wish it was all the same. But here's the thing. The reason is because I like to keep adding more stuff, make it current. I try and make it very clear when it's something new that I've added. I bring in extra information to keep it up to date, and when people struggle and they hit problems or they need something explained better, I can put that in there. And that means that everyone benefits from where some people have struggled or I haven't explained something. Well, I can add it in there. So for me it's super important to do that. And the reason I don't come back and record all the videos as I do it well, first of all, that would take a lot of time and I'd rather spend time getting the this up to date. But also because that would erase some of the progress that people have made with videos they've watched. And it can be quite jarring if you're in the middle of a series and then the videos are changing under you. So it's really worked out better for me to keep these labs up to date as much as I can. So treat the labs like a resource. And the way that I suggest that you work with me is watch the video of me going through a lab. I'll go through it, I'll explain it. I'll execute things. I wouldn't necessarily suggest that you do it at the same time as me, because that's not really the point. The point is, as soon as I've done a lab, you come back and you go through it step by step yourself, and you use that as a way to build the learning, add in print statements and so on. Okay, a couple more things to say on this. I'm using here things called notebooks, which are a particular type of Python code that's organized in this way into cells. And there's going to be there's a guide on on how to use notebooks like this if you're not familiar with them. And I know that a lot of people from an engineering background initially get a bit frustrated like this because they'd rather work with just Python code. We will work with Python code as well. We're going to use both, but this kind of notebook has its place. It's very useful for research and development, which is an important part of being an AI engineer. And it's also great for learning because you can run things step by step, experiment, tweak, put in print statements, and learn. And so there is a benefit to this too. So even if you're not used to these kinds of notebooks, there are I promise you there's good stuff here too. And we will also do some Python coding using normal modules. All right. That's the end of of all of the details. Let's get to running this first lab. Okay. The first step is to go to the top right just here where you see select kernel. You click there and you have to choose your Python environment. And the way you do that is you type Python environments right here. And this first one up here with the star that says Venv. It says recommended. And it's right in this directory. That is the one you want to pick. And hopefully that's going to come up really clear for you as Python 3.12 at the top. And once you've done that, it will be set up here. And you'll always when you come to a new lab you'll need to do that. And so so do bear that in mind for it to work. Okay. And then as like a final, final little thing here, it's, uh, got one more link to my LinkedIn in case in case you haven't succumbed by this point, remember that you can always link them with me. All right. So now what we do is we come here to the first of the cells. Each these things are called cells, these code cells. And we're going to do an import. And what import are we going to do. Well we're going to do this import here from dot env. And you can see that cursor is already suggesting what we could do right now from dot env import load dot env. And I hope you're familiar with what it means to import a function from a module, but if not, then please do check out the guides for a refresher on that. But I'm now going to run this code cell by simply pressing shift followed by return or enter. And that executes the cell and it's done. So the next thing we're going to do is use that function. And this is how we do it. We say load dot env and I'm getting prompted already. There it is. Env override equals true. So what does this do. Dot env is just a utility function which does something very specific. It looks for a file with a very particular name in your project root directory. And that file name happens to be, as you probably guessed, the name dot e. And it's assuming that that is where you're setting some of the environment variables that you want to use for this project. And it's just a utility that looks at that file, interprets it, and sets environment variables as a result. That's all it does. And override is true is just a nice little trick to know about by default. Load. Env. If you've already got an environment variable set, perhaps with your profile, then that takes priority. And it won't. It won't overwrite it. And that can be a nasty little gotcha. Because if in the past you've set up an open AI API key and you've and you've stored it somewhere global on your computer, that would take priority and you wouldn't be able to figure out why on earth OpenAI is rejecting your key. And so it turns out that this, as you can imagine, was a common problem in the past. And and I have learned to tell people to always put override equals true, which means that what's in the env file takes priority. And we run this by pressing shift enter. And now our environment variables will be set. So let's check that. This next cell does an import OS. And then we look for an environment variable that's called OpenAI API key which is exactly what we named it in the env file. And if it exists we're going to print it exists and it begins and give the first few letters. Otherwise it's going to say it's not set. Please go to the troubleshooting guide. I press shift and enter to run this. And there we go. The OpenAI key exists. It begins scmproj. All is good and I hope all is good for you too. If not, go to the troubleshooting guide. Okay. And now we have a super important import statement. And it looks like this from OpenAI import OpenAI. So if that seems weird to you, why is one in lowercase and one not anything like that? Again, go to the guides. I'll explain the difference between classes and modules and all the rest. But we are now going to execute this, and if this gives you any problems at all, then that might suggest a problem with your UV environment troubleshooting guide. And always contact me if you get stuck in any way. I'm here to unstick you. All right. So what we're now going to do is create an instance of this class. Now I tend to always do it this way. OpenAI. Equals OpenAI. And that can be a bit confusing to people because again, when we're using the same word OpenAI, the proper way to call this would be something like OpenAI. Python client. Because what we're doing is creating an instance of the Python client library. But I just call that OpenAI because it's just simpler and shorter. But but that's what it stands for. We're creating an instance of that class and let's run that right now. And it's worth bearing in mind that some people think that when you do this, you're creating something to do with the actual GPT large language model locally, but you're not. OpenAI is a lightweight library for connecting to OpenAI's endpoints on the cloud. So this is just what people sometimes call a client library. It's just a simple wrapper library around endpoints that calls HTTP endpoints to call the API that's running in the cloud. And that's all it is. It's a it's a simple lightweight class for that purpose only. So with that, let's keep going. What we're now going to do is create a list called messages. And this is a format that I hope most of you are very familiar with. On my LM engineering course, we do this one to death. And if you're not, I'll trumpet some extra stuff on the guys. I'm not going to explain too much of it right now, but conversations with large language models are often represented in this format that OpenAI invented and has become ubiquitous everywhere. Lists of dictionaries where each dictionary has a role and a content. And in this case we say the role is user. The content is this simple question what is two plus two? So we run that and now we're going to actually make a call to OpenAI and check everything is working okay. And so now we're going to come here and type the code to call OpenAI. And cursor is going to make this very easy for us. We're just going to type response equals OpenAI dot. And it's a chat dot completions dot create. And you can see that cursor is already filling that in for us. I can just press Tab and it's going to complete that right away. And it's going to say the model is GPT four mini. And the messages are messages. That's perfect. And it's also going to take the response. Take the first choice dot message content and print it. And that's exactly what we want to do. This structure of OpenAI is something that is so common these days. If you're new to it, then take a look and I'll try and cover some in the guides. But people who are familiar with this will see this as a very simple way to call OpenAI. And if we run this, we get the response to the question what is two plus two? We get two plus two equals four. So we know we are connected to OpenAI and so far so good. So hopefully you're now comfortable that we can both call OpenAI. We can get responses. We can use the very standard chat completions API structure and cursor will fill it in for us if we ever forget what it is. But now let's ask a harder question. It's time for us to be a little bit agentic. We're going to put one foot in the agent camp. It's more a workflow than an agentic I but but still it's getting there. We're going to ask a question. Please propose a hard, challenging question to assess someone's IQ. Respond only with the question. So that's going to be our question. And we're going to put that into the standard messages structure. And now we're going to ask it. So again we say response and it's already filling it in for us. We press tab to complete that. We press tab. But we're just going to change this slightly. We're going to make this reply equals maybe I should make this like like question equals. So we're going to take back the what comes back and we're going to make it question. And then we'll just print the question again. Cursor fills that in. Let's run that. And here we go. Here we have a question from OpenAI. If a train leaves the station traveling at 60 miles an hour, another train is a different station, 120 miles away at the same time traveling 90. How far from the first station will the two trains meet? A nice brainteaser right there. So what we're going to do now is we're going to take this question, and we're going to make a second call to an LLM to answer the question. And that's why I describe this as an agentic pattern, because it is making multiple calls. We are orchestrating multiple calls to llms to solve a bigger problem. And so it's going to be relatively easy. We're simply going to continue this by again making another messages. Look at how cursor fills it in for us messages with role as a user. And the content is that question. Great. Let's run that. That is now messages. And now we're going to ask OpenAI again. So we're going to say response is OpenAI create. And again, cursor fills it all in and it fills in the answer. And it's even clever enough to recommend that we call it answer instead of question. Honestly it's incredible. So this is exactly what we want to do. We're making the same kind of call to OpenAI's chat completions API. The simplest way to call OpenAI in the cloud. And we are now getting back the answer to the question, and we will print that answer. And I believe the answer we're looking for is 48, I think. Let's see. So we'll get that to print. And here we go. A bunch of information right here written with 48 at the end there. And you may know this already. You recognize that this has come back in a form of markdown. It's got some markdown stuff in it which llms love to talk in markdown. We can just add in a little code in here and we can do from there. Ha ha. Ah. Is it spooky? It is spooky the way cursor does this sometimes from IPython display, import markdown and display. And then the next line that cursor is telling us to write. I don't really have a job here, do I? Display markdown, but the only thing it's got wrong. Thank goodness it got something wrong. Is that what we actually want to display is the answer, not the question. So if we do that and we run this, then we see the answer formatted nicely. I'll let you read through its work and it comes up with the conclusion of 48 miles. Uh, so there we go. That is our first semi agentic pattern, built just in a few minutes. If any of that was unfamiliar to you, check out the guides, play with it, look at it. And as I say, congratulations on getting here. But at the end of this lab there is an exercise. And I do encourage you to do the exercises. This is where you really get to to show that you've done this. You may be thinking, okay, there wasn't much commercial about this project, and I do like to try and weave commercial angles in wherever I can to make this applicable to business. And so I've got an exercise for you to now make this applicable slightly to business. So we are going to try and ask an LLM, first of all, to pick a business area that might be worth exploring for an AI opportunity. Secondly, to identify a pain point in that business sector that it's decided on, and then that that's one that it needs to explicitly explain. What is that challenge that might be ripe for an agentic solution. And then the third step will be to actually propose an AI solution to that problem. So I would like you in this empty cell here, probably assisted by cursor, uh, to, to to go through and make this happen with three calls to LMS. First of all, you put something in this message to explain that you want it to pick a business sector. Then you get back the response based on that response. You read in the business idea, and then you will repeat these three steps in order to get the the pain point, and then to get the AI solution and print them all out. Print them in markdown if you wish, in nicely formatted, and use that as a way both to to experiment with some agentic workflows and also actually to get some commercial benefit, to get perhaps an interesting idea about a way that you can apply AI to a real business pain point. All right. I hope you enjoy that challenge. If you have any problems with it, of course, then you can either get help from from ChatGPT or cursor, or just drop me a note and I will see you for the final lecture for this first day.
------
Video 7. Week 1 - Day 1 - Introduction to Agentic AI: Creating Multi-Step LLM Workflows + Autonomy
------
Well, hopefully at this point you've now brought up that lab. You've gone through it yourself, you've put in some print statements, you've you've understood what's going on and you've used it as your way to then complete the exercise, both to try this out yourself, make a few different calls, and you've also got some commercial feedback as a result. And you know, it's not only about an agentic workflow that that takes several steps in a way. We've also given our LMS some autonomy because we asked it to come up with the business sector that it would investigate. And so you could you could think of that as, again, a baby step towards allowing the LM to plot its own path, to choose its own adventure, which, as we'll see, is one of the hallmarks of an agentic AI solution. Okay, so this hopefully looks a bit familiar to you. This is the six week curriculum that I showed you before. That is how we are laying on information week by week. And we are now wrapping up the first day and you're thinking, wow, what a. This was a long time. I will tell you that this day there was a lot of setup stuff. Other days will be shorter and will make a brisker progress with with good commercial stuff right away. But what I'm going to do is highlight in that square on the bottom left to show making an agentic workflow. Your first workflow is done. We've made progress already. Next time we're going to be talking about agents and agentic patterns and introducing what they are and how they work. And I can't wait. I will see you then.
------
Video 8. Week 1 - Day 2 - Building Effective Agents: LLM Autonomy & Tool Integration Explained
------
Well, if you're watching this video, then you weren't entirely put off by the grueling day one and you've come back for more. For day two. Welcome back. So this is going to be an interesting day. It's going to be different to day one. It's going to be shorter than day one. You'll be happy to hear. But also it's going to be pure theory. We're going to be talking about agents and agent architecture. Almost all of the days on this entire six weeks program are very practical, heavy. This is one of the rare days that is going to be a theory day, but it's going to be interesting. It's going to be really great. I'm going to tell you all about Agentic architecture. Let's get to it. So where better place to start than with the almost obligatory what is an agent? Kind of question. And the interesting thing is that there's not actually a great answer to that. Agent guy is one of these things that's been super hyped and generated a lot of excitement, and people are using the term to mean a lot of different things. And in fact, there was a meme going around last year about how agent Guy could mean almost anything. But having said that, typically there is one simple definition, and actually the definition that I'm pulling has come from hugging face. They're small agents project where they give a really nice, crisp, simple definition, and they say AI agents are programs where LLM outputs control the workflow. So basically one one output from LLM is able to decide what tasks are carried out in what sequence. So that is a good crisp definition. It's something for you to keep in mind. And it applies in some situations and not in others. More generally though, you will find that there are different hallmarks of what makes something an AI solution, and people will often use the term agentic AI or agents, if any one of these five hallmarks are met. So first of all, sometimes any solution that involves multiple LLM calls could be called Agentic I and, you know, a bit like the one that we just built in day one that could be called Agentic I. Now, this one LMS with the ability to use tools, that's often what people think of as the kind of litmus test for whether something is agentic. If you remember when we looked at the N810 idea and the very first, at the beginning of the first lecture, when I had it turn on the lights that I was wearing, that was an example of tool use. And we're going to be talking a lot more about tool use. So that's a that's obviously a hallmark quality as well. Another is when you have an environment set up which allows different LMS to send information to each other, a kind of coordination orchestration environment. That in itself for some people defines a gender I sometimes it's when you have a planner, some process that's able to coordinate activities and that that itself is an LM, that is and that's quite, quite similar to the text in blue that that for some people is the hallmark of agentic AI. But for some people, for a lot of people, there's this word autonomy, and that really captures the essence of agentic AI. And this word is saying that we are giving some ability to an LLM to control what order things happen in, or what happens to sort of choose its own adventure in some way. And giving it that autonomy, giving it, if you will. That agency is one of the ways to define agentic AI. And in some ways, that sounds kind of spooky. Like, like what does it mean to have an LLM, a statistical language model that is autonomous? And to be honest, it's just sort of fancy use of language in many ways. You could argue that exactly. The project that we just did, the challenge that I set you to have an LLM come up with a business sector that would then be analyzed for pain points. In some ways, it was showing autonomy there because it was able to choose what business sector to use. And even more straightforward was when we asked for a question, and then we had to answer the question. It had a level of autonomy. So you can see how any time when we're giving an LLM the opportunity to decide how we will carry out future actions, you could think of that as giving it autonomy and that being a way to describe Agentic AI. And just to go one level deeper, anthropic wrote this brilliant post that's called Building Effective Agents, and I will refer back to that many times because I think it's just so clear and well written. And in that that particular paper or that that blog post, they distinguish two types of what they call agentic system. So that's this is their terminology. But they would say that there are, there's this umbrella term agentic system. And under that umbrella term there are two different categories. And one category is called workflows. And these as it says, are systems where models and tools are orchestrated through predefined paths and they delineate between that and something called agents. And agents are where models dynamically direct their own processes and tools, maintaining the control over how tasks get accomplished. So these are the two different sort of subfields under a genetic system. There are workflows and there are agents. And you can see they're sort of hinting that perhaps the things that a lot of people call agents are in fact workflows. So they're drawing that distinction, but they do still describe the whole thing as being a genetic systems. So it's a bit of wordplay there. Does that mean that workflows are agentic or not? I guess I guess they are. But you can see there's some ambiguity in the terminology. But I do think this is a helpful framework for distinguishing between these two worlds. And we're now going to go deeper. We're going to look at each of these and look at some common design patterns. And this should hopefully start to bring things to life and show you about how we can apply this kind of thinking to actual projects.
------
Video 9. Week 1 - Day 2 - 5 Essential LLM Workflow Design Patterns for Building Robust AI Systems
------
Next we're going to peel back the onion a little bit more on workflows. Looking at different types of workflow, anthropic identifies five different design patterns that you'll find when building a genetic systems that have workflows in them. And let's go through each of them and talk about them. This is the first one. They call it prompt chaining. And it's a simple one. What you're seeing here in this diagram obviously the in and the out on the left and the right is the beginning and the end of the workflow. The yellow boxes represent calls to models to Llms. And the blue boxes is where you potentially have just some code that you've written, some some software and it's optional. And what you'll see there is that this is simply saying you could have an LLM carry out some task and then potentially based on some code, you could then pass that to a second LLM and that output could go to a third LLM. And that could be the conclusion. And actually this is really similar to what you just did. This is similar to that commercial question. When we ask an LLM first pick a sector. And then given that sector pick a pain point and then pick a solution. And obviously it doesn't need to be three LMS. It can be as many as you want, but this is the idea. You are chaining a series of LM calls decomposing into a fixed set of subtasks. And the reason you might want to do that is because you can take care to frame each LM call very precisely to get the the best, the most effective LM response based on that prompt. So it allows you to to really work on each of those tasks being very effective. And then keep your whole process, your workflow on guardrails by taking it step by step through a sequence of well-defined tasks. Now I want to make a point that you might have already thought of. Whilst anthropic is quite clear to define this as a workflow and not an agent pattern, it is perfectly possible for prompt chaining workflow like this to be giving the LM some discretion on what activities happen, because just as we did in our example, the first call to the LM might come up with a topic, and it's that topic that gets worked on by LMS two and three. So in some ways, the way that anthropic distinguishes between workflows and agents, it's a little bit artificial. There is definitely a blurred line between them. It's perfectly possible for there to be an element of autonomy about a workflow pattern like this. Onwards. The second design pattern is called routing, and this is where an input comes in. And an LLM has the task of deciding which of multiple possible models are selected to carry out this function. And the idea is that you might have specialist models in here. They're shown by LLM one, two and three. And they're each good at different tasks. And the router's job is to classify the task, understand which of the specialists will be best equipped to tackle this task. And it allows for, as I say here, separation of concerns for being able to have different LMS that have different levels of expertise and have them and have an LLM decide how to route to those experts. So this is a powerful model. It's very common. And it's something which again, I would say I would argue there is some it's a bit artificial to say that there's no autonomy here, because clearly that router is able to make some decisions, albeit on guardrails. Again, it still has to follow a given workflow. And on to the third design pattern. Parallelization, which is at first blush, might look quite similar to what we just talked about. But there is a lot in common with these design patterns. But the idea here is that you have. Remember, the blue boxes represent code R code, not an LM. So we write some code that takes a task and breaks it down into multiple pieces that should all run in parallel. So in the previous design pattern it was an LM doing the routing. And it was one of these three experts that was given the task in this design pattern. It's code. It's let's say Python code that's deciding what to do or how to coordinate. And in parallel, it's being sent to three llms to carry out three different activities concurrently. And then there's more code. Maybe it's Python code that takes those answers and stitches them together. And actually, anthropic makes the point that these tasks don't necessarily need to be different tasks. You could imagine a situation where you send the same task and have it be done three times, and you use the aggregator, perhaps to take the average or something like that. So there are various situations where this might be the same thing that you're doing three times doesn't need to be different, but I think most commonly it's when there are multiple subtasks happening concurrently. That is the parallelization. It's hard to say of the design patterns. Okay. And now this one orchestrator worker. This is when a difficult, challenging task is broken down and recombined. And you might be thinking, hang on, isn't that just what we just did? I just flick between them. Don't these two slides, these two design patterns are really similar? Well, you'll notice a subtle color change along with the change in terminology. The key point is that this is exactly like the prior design pattern, except it's no longer code that's doing the orchestration. It's an LLM. So you are using a model to break down a complex task into smaller steps, and then you are using a model to combine the results. And so this is a much more dynamic kind of system where the orchestrator can choose how to divvy up the task. And again, I would argue that it's quite artificial to categorize this as a workflow rather than an agent pattern that we'll look at in a minute, because clearly that orchestrator has discretion over how it divvies up the tasks, and it could choose how many different llms get assigned the activity. So saying that this is just a fixed workflow is perhaps a stretch. So, you know, I think you get the overall idea. There's certainly more constraints on this than when we come to the more flexible agent flows. But but generally the idea here LLM breaks down the task Llms carry out each expert task, LM synthesizes the task for an output that is pattern for the orchestrator worker. And now on to pattern five. The final workflow pattern and the one that I use most commonly. This is one that I come against all the time, and it's called an evaluator optimizer by anthropic. I tend to call them just evaluators or validation validation agents. You hear that a lot, but the idea is it's very simple. You have an LM that's doing your job. Let's call it the LM generator. It's it's doing something. And it comes up with a solution shown in this white arrow here. And you have a second LM that's playing the role of evaluator. It's there to check the work of the first LM. And it's given any extra information, any context, everything to, to arm itself to, to not be trying to generate content, but check the work of a prior LM. And based on that it can choose to either accept or reject the work. If it accepts it, then that's it goes to the output. If it rejects it. It should come up with a reason. And the rejection and the reason goes back to the LM generator. And that can then choose to come up with another solution which comes back here. And so you can see this, this sort of feedback loop setup. It's very powerful. Of course, one of the key concerns of building production systems with LMS is about accuracy, predictability, robustness of the responses and having validation agents. Having these kinds of evaluator optimizer flows in your LMS solutions is a really powerful way to increase the accuracy and build more guarantees. There's never full guarantees with LMS, but build a higher level of guarantee around the quality of the final output. So this is a really effective pattern. I use it all the time, and of course we'll have plenty of examples on the course, and you should be able to think about how you could apply this to LM solutions in your day jobs right away.
------
Video 10. Week 1 - Day 2 - Understanding Agent vs Workflow Patterns in LLM Application Design
------
And so with this, we now turn to the other category, the second category of agents. So by contrast with the workflow patterns with agents, the process is more open ended. It's something that can keep going. It has feedback loops. It's typically a design that allows for information to come back and be processed multiple times. And importantly, there's no fixed path through the design pattern. It's not like there's a series of steps as we had in those prior patterns, but rather it's something that's fluid and dynamic. And as as a result, it's something which can be much more powerful. There's many more. There's a much greater kind of problem that could be taken on by this sort of agent pattern, but it's less predictable. And so there are absolutely concerns about robustness, about guardrails, about how do you get the best out of this kind of flexibility, but still have a system that can run in production and be guaranteed to complete in a certain time and so on. And this is the diagram that I would bring up, which is fairly generic kind of diagram that's just meant to show that you have the inputs and outputs now shown as a human. But but we don't really have output anymore. We have environment that's meant to reflect some, some sort of outside world that can be interacted with, such as the lights that I wore around me, that the human can make a request that goes to an LM. The LM is able to take actions on the environment, as you can see, and the LM is able to get back information from the environment in some way. And then this is simply a repeating loop. The LM can continually make other actions and get back feedback, and at such time as it desires, it can choose to stop. And so this is the the the diagram. And there aren't more specific design patterns because it is in itself this sort of open ended design pattern. It's saying this is a it's almost a sort of meta design. It's saying the the LM gets to choose its own design for how it's going to solve the problem. And that's really it's it's quite hand-wavy, but that's the way to distinguish it. If we look back at this pattern, I mean, this pattern arguably also could keep going forever. It's not as if this has a fixed stop, but there's clearly more certainty about what's happening for what reason than with the agentic patterns, where it is more fluid by its nature. So that's really the core idea behind the agent patterns, that there is much more flexibility that allows us to to be able to solve much more complex problems where Llms can really plot their own paths. But it does introduce some new issues, such as not knowing how long it will take before it will will complete its task, not knowing whether it will complete its task at all, not knowing what kind of quality outputs we will get, and not knowing how much it will cost. These are all the kinds of challenges that agent developers like ourselves need to face, and they are the sorts of things that we will be tackling in the coming weeks. And so if this feels a bit unsatisfying that the design patterns for workflows are so clear. And now this looks just so loose. Then all I can say is, as we put this into practice, as we code different real agent environments, this is going to become more and more concrete, and you're going to be able to you'll see this as second nature before too long. Now let's just quickly say a few more words about some of the drawbacks that I alluded to a second ago with using agent frameworks and agentic AI and design patterns. So obviously there is this sense that you have an unpredictable path. You don't actually know what order tasks will take place in or even what tasks will happen. You don't know what the output will be. There's no guarantees that you're going to get a great output. Now, this this whole Agentic architecture allows us to take on much bigger, harder problems than we've ever been able to take on before. But it does have this inherent uncertainty associated with giving Llms autonomy over how they tackle problems. There's also unpredictable costs. I alluded to that as well a moment ago, but because you don't know how long it's going to take, you don't know how much it's going to cost in terms of running the APIs and potentially building up a big bill. And so as a result, there's a couple of mitigations that are incredibly important, both of which we will spend plenty of time on in the coming weeks. One of them is, of course, monitoring, being sure that you have the kinds of visibility into what's going on behind the scenes so you can understand what's going on with your models, with their interactions, particularly when you have potentially many agents all interacting. And we're going to see that with open AI SDK. We're going to see that the trace, the ability to to watch different agents interacting. When we look at a graph, we're going to see Lang Smith their tooling. So we're going to see a lot of the ways that you can have this visibility into what is going on under the hood in your agent systems. And then guardrails. Guardrails is the name for the kinds of protections that you can write in software that makes sure that your models are doing what they should be doing, or that they're not sort of leaving some, some constraints, some, some rails that you put in place. And in fact, that quote that I put here comes straight from the OpenAI agents SDK, which has a ton of functionality dedicated to guardrails, and they say that it ensures they behave safely, consistently and within the boundaries that you wish. And so we will be building those guardrails ourselves in week two. And with that, that does actually conclude the day. Two lectures, as I promised you, it was shorter than day one. And I hope you didn't mind that we covered a lot of theory. It's really interesting stuff, these gigantic design patterns. I hope now you've got some clarity on what agents are and that there is ambiguity on this, but but ways that you can talk about it and think about what it means to be autonomous and what it means to be able to plot your own path. Let let an LLM decide the path that things will take. And so with that, congratulations on completing day two and day three. We're going to talk about orchestrating Llms. It's going to be more and more coding, more hands on, which will be fun. We're going to build with a lot of APIs and get ready for that. And after that we're going to move on to tools. So I will see you next time.
------
Video 11. Week 1 - Day 3 - Orchestrating Multiple LLMs: Comparing GPT-4o, Claude, Gemini & DeepSeek
------
Well, hello. I'm back. And you're back too, which is a great thing. It means it's time for us to start day three of our journey together. And day three, we've already done building an agentic workflow for the first time. And we've talked about patterns. And now we're going to talk about orchestrating between LMS and many of them. So this is going to be a practical day. You'll be pleased to hear. It's about time we got some more coding. And we're going to be calling a lot of LMS. And I just want to of course, say a few things up front. We're going to be calling both paid APIs and also open source models, and we're going to be calling them both in the cloud and also open source models locally and doing it throughout this course. And I want to be clear that you have complete flexibility to decide which models you pick, at which point I'm going to have coded it one way, but a great exercise is to take what I've done and apply it to other models. And if you don't want to spend a dime, you don't need to. You can do this all using local models. Although performance might vary, you can see the results I get from the models I use, and try and see what you can achieve with free open source models as well. And if if you want more information on what it's like to select models, whether they're open or closed source, apply them and deploy them, then you should take a look at my other course. We're not going to be covering that in detail here, because otherwise I feel like it's going to be too much of a of a rabbit hole and we'll get distracted. So I'm assuming you're coming into this knowing about the different kinds of models and having a sense of what makes sense for closed source, open source, and so on. If you don't have that, you can either just go along with it, just just sort of pick it up as we go. Or of course, you could always turn to look at my other course, or I'll try and put some more background information in the guides as well, because that won't be required. You can just just go, go with the flow in terms of how we pick models. So let's talk about the cast of characters, the different models that we're going to be experiencing now. So the first model needs no introduction really. It is of course, the model GPT four mini Many from open AI. We've already used it in a couple of calls. It's for sure the most well known of the models out there. And of course, there's also GPT four, the bigger cousin of GPT four. And then there are the reasoning models, which are models that have been trained to think through their steps in an Agentic like way, in like a workflow of thinking through the different steps before they arrive at their conclusion. Because it turns out that when you ask an LLM to think through its steps, you get much better outcomes. So we may take a look at some point at oh one and oh three mini, but it's less essential for this course. Most of the time. We're going to be sticking with GPT four mini now and OpenAI's great rival. Of course, their competitor is anthropic. That was actually started by a couple of people from OpenAI originally, and we'll be looking at some of their models. But Claude 37 sonnet is the model that I will spend most time on. And if you want to have a cheaper version, you can go with the Claude three haiku, which is significantly lower cost. But but Sonet is also fairly cheap for Google. We're going to be using Gemini two zero flash. There is also the Pro version of that too, but I think we'll stick with flash. And as of right now, flash is actually free at least as long as you use it within certain usage limits. I don't know how long that will be the case for, but by all means, if you want to use an open frontier model without paying for it, then Gemini might be your path. Do look into that deep Sikh. Deep Sikh, of course, is the Chinese upstart startup that shocked us all by coming up with such a powerful model in the form of Deep Sikh V3 and R1. And it's important to understand that what made Deep Sikh so sensational was not necessarily that their model was the strongest in the world, because it wasn't. It was slightly behind the latest from open AI, but that they developed such powerful techniques to train Deep Sikh to be that good, that it cost them a fraction of the spend that OpenAI had spent to train GPT four and train GPT 40101. Deep seek was able to achieve very similar performance, pretty much comparable at a fraction. I think it's like 30 times less spend. That was the true innovation. That's the remarkable thing about deep seek. And also that they open sourced the model so that you can use it. But the main the major model has 671 billion parameters, which means it's far too big for anyone to run that on their computers. But there are versions of it, small versions of it called the distilled versions, which are in fact themselves just smaller models. They're versions of Llama and Quen, two different models that have been fine tuned on data generated by the big Deep Seek and those smaller distilled versions of Deep Seek for sure available free of charge. Grok. We will also be using grok, and there are two groks. Confusingly, if you don't know this grok spelt with a k at the end is the name of the model that comes from from the company formerly known as Twitter. Now x x is grok and we might use grok with a k at some point as well. Not not in today's lab, but grok with a Q is something different. Grok with a Q is a company that has come up with a really cheap, fast way to run inference runtime models like llama 3.3, which is the massive version of llama with 70 billion parameters. So you can run llama 3.3 really fast, really low cost on Groks infrastructure and along with many other open source models including Deep Seek variants. So grok is great to use for that. And then Olama. So Olama is itself a more of a platform. It's something that you can use to run something locally that provides endpoints locally that are consistent, very similar to the endpoints that OpenAI and other models here have so that you can make local calls to an API, which is in fact going to just run an open source model locally on your computer in high performance optimized cplusplus using a library called llama CPP. And so a llama is something that we will use as well. Now, if some of these terms are unfamiliar to you, terms like inference. If you're not sure about that, if you don't know, if you don't fully understand the difference between running something over a llama or grok, then I can. I can suggest a background materials, look in the guides, and also consider whether you'd like to look at my LLM engineering course, which does cover all of this. And the final point that I will make is that you may know that I'm something of a fanatic on leaderboards, on places you can go to read about metrics and performance of different models. So much so that I was called a leaderboard, uh, humorously by my my great friend John Crone on his Super Data Science podcast. But there is a website called called leaderboard, which I've given the web address right there. And that is a great resource because it compares a number of the leading closed source and open source models together side by side. And it has things like the costs and it has the context window size, if you're familiar with that, and it has the cost, you can calculate based on the number of input and output tokens. And it's also got the performance, the results of key benchmarks across a number of dimensions. So I strongly encourage people to go and check out the vellum leaderboard. And there'll be a link in the resources. And it's great to have that bookmarked up. And as you go through different APIs, use that to gauge the the costs and capabilities associated with them. And now I'm going to put up one more time something that I said to you in the first lecture. But just to really double down on this, I want to remind you that there are great resources all over the place for this course. There's a resource for the whole course with videos and links. There's GitHub, the repo that has guides. It has the troubleshooting. And I'm always updating the labs to keep them up to date. I might add on new models, the models that we just went through a moment ago. Those are the models that right now are the latest and greatest. But as new versions come out, I will update the labs. So you've got the latest in there. And so, you know, what you've heard is, is is one story, but you'll get an even better story when you go through the labs yourselves. And I will also urge you to keep a thick skin. There's that. You will hit roadblocks. There will be problems. That's one thing I can say for sure, but see them as this is where real learning happens. It's in debugging. It's in diagnosing and figuring things out, even if it's painful to start with. It's super satisfying when you fix it and get it on track, and that is the way to do it. And and if all else fails, or as I say, or if not all else fails, you just want to then reach out and contact me. Of course, one more time. I've got my LinkedIn right there, but also you can email me. I've got my details all over the place and I like hearing from people. I'm responsive. But most importantly, I don't want you to be suffering in pain with problems. I want to be fixing them. So keep this in mind. And also, I mean ask ChatGPT you do tend to get great, great answers. Honestly, it's amazing and I tend to. When I have a really difficult question, I will often ask ChatGPT and Claude, both because, uh, sometimes you get answers that are too long winded or take you in too many different directions, and so asking a couple of different models can help. And you can also basically have like an agent workflow that you do manually. You can ask a question to ChatGPT and then to Claude, you can say, I've got this response to my question. Do you agree? Is this accurate? You can have it be the evaluator, like the evaluator optimizer pattern that we looked at a moment ago. So that's a that's really cool that you can do it manually. And it's a great technique to get good answers from Llms when you're stuck yourself. Okay. With that preamble, it's time for our next lab. Let's get to it.
------
Video 12. Week 1 - Day 3 - Multi-LLM API Integration: Comparing OpenAI, Anthropic & Other Models
------
So here we are, back in cursor. A great place to be. So on the left is our directory structure. I open up the foundations folder and I'm going to go to lab two which is what we're doing right now. Welcome to the second lab week one. Day three. We're going to work with a lot of models. Now let me start with just some quick points. First of all, I want to mention that the way that I like to do this, the way that that I collaborate with you may be different to some other courses that you've taken. And it's something that is a bit controversial. Some people like it, some people don't like it, and I hope that you'll get used to it. Typically, I don't like to just sit here typing code because I feel like that slows down the momentum. Rather, I look at cells, I explain what they do, I run them, I inspect them, I print them, and that's what I recommend you do as well. Watch while I go through this and explain it, and then come back afterwards and run it yourself. And then add in print statements. Change things. Experiment. It's all about experimentation and that's what you should be doing. So for people that would prefer it if I, if I sat here typing, I'm sorry, I will try and do it from time to time. I'm going to do it right here. I will do it occasionally, but most of the time I'm going to explain what I'm doing and I hope you'll get used to that. And you'll see that there are pros and cons of everything, but that's the way I like to do it. Now, one thing I will really, really encourage you is written here that that once you've gone through this and made some changes, done the exercises, perhaps it would be fabulous if you were willing to save your changes in the folder called Community Contributions and raise a PR or a pull request for me to merge that into the repo so that other students get to benefit from this as well. To my great joy for my other course, there have been hundreds of PRS. There are tons of examples from people like you taking the course and submitting what they do with all sorts of fun and interesting examples, and it's been a real joy to see. And it's also been valuable for other students to experiment with, with people's contributions. So it's a fabulous thing to do. I really encourage it. In addition to that, something which is a good career thing to try is it's good if you have your own GitHub repo, and as you work on different projects, put some of your own stuff there as well. Showcase the work that you're doing. And if you put a post on LinkedIn to talk about some of the projects that you've worked on or some of the things you've done or what you've learned from it, and you tag me on it so that I get a notification in LinkedIn. Then when I see that, I'll come in and I'll weigh in and I'll make sure that I post something there as well. And that amplifies what you're doing. It means that it goes out to all the other people I'm connected with on this course, and people can then weigh in with their thoughts too. And it helps to draw attention to this, the skills you're acquiring, your expertise and your examples. And that's the kind of thing that that might attract the attention, perhaps, of a future client of yours, someone that might contact you for some business and potentially even a future employer as well. So it's a great thing to do. I strongly encourage it, and I'll be here to amplify the work that you do. Okay, let's get on with it. So it begins, of course, with some inputs and I press shift Enter to run these inputs. Be sure to run the inputs or you will get name errors. Okay. So always remember to do this as the comment what is that. Well it is of course doing a load to bring in the environment variables. And you remember that I like to have override is true to avoid any existing environment variables taking priority. So that has happened. So now that that's run, what we should find is that we've got a bunch of environment variables. Now I've been I've been quite greedy and I've set up a bunch of different APIs. And I don't expect you to do the same, but should you wish to, here are all of the APIs that you could set up. And I have done so and I have enjoyed it. Open AI, anthropic, Google's Gemini, Deep Seek, and grok. It's worth mentioning grok as when I did it at least doesn't have like an upfront a minimum. You only pay for what you use and it's really cheap, so this is a good one to try. Deep seek has a $2 upfront. At least it did for me. And then you draw down against that $2 and I haven't been able to spend $2. Gemini is free for within certain tiering. And anthropic and OpenAI both require an upfront, which I believe is $5 in the US for both. Although OpenAI appears to have a new deal right now, which I don't know how long that will last for, but you could always go and check it out. So this code here will load in the environment variables and just check that these keys look right if you're using them. And when I run this it gets this print here. And you can see that I've got keys that look like they should look or with the right kinds of prefixes. And if you run this cell and you haven't set up some of these keys, you'll get you'll get some sort of not set. But don't worry about that. Just don't don't call that particular model okay. And now to get started, we're going to make a bunch of different calls to LMS. And there's two objectives for this. One of them is just to show you the different kinds of API's, the different style of messaging that we have with with different LMS. So you get a sense of it, because we'll be talking to lots of LMS over the next few weeks, and the other is to do some orchestration between models. A few of the design patterns that we talked about last time to see how that works out. So the first thing we're going to do here is we have a question. We're going to have a request. Please come up with a challenging, nuanced question that I can ask a number of LMS to evaluate their intelligence. Please answer only with the question. No explanation. So that's going in a variable called request and into a variable called messages. We're going to have this standard OpenAI construct role is user. The content is going to be this request that we have right here. So in fact what I'm going to do is I'm going to add another code cell here. Let's run this. And I'm just going to print messages. Just so you see this, just in case it's not completely clear to you. It's a list of dictionaries role user content. And there's the content. Now, people who've been on my prior course or generally know about these things might be wondering why there isn't a system message. Well, we often do do talk about the need for a system message and a user message, but system messages are optional. You don't need one, particularly if it is a kind of standard. You are a helpful assistant then. Then you don't need to say it. It will work perfectly well without it, as you will see. Let's give it a try. We're going to to use the same API structure that we did last time that hopefully most of you are pretty familiar with, and that cursor will just fill in for you if you start typing this. Anyway, we create a new instance of the OpenAI Python client library and we call OpenAI create. We pass in the model, we're going to use the cheap GPT four mini. We pass in this list of dicts, and we get back and we ask for the choices zero message content, and we'll print it, and we're putting it in a variable called question. Let's see what we get. So how would you analyze the ethical implications of using AI in predictive policing? Wow. Considering factors such as bias, accountability and societal impact that is a good question. Goodness. Wow. Okay, that's a hard question for sure. This is going to be an interesting challenge. We will see what happens next okay. So what I'm going to do next is I'm going to first set up an empty list called Competitors List that we will be filling up with different names of competitors. And I'm then going to have another list which I'm going to call answers. And that's going to be where we will fill up different answers from different llms answering this question. and then we will put together the, the messages that we're going to to, to say to them. And I love the way Castor does this, but this is exactly what we want. We want a list of Dick's role is user. The content will be this question, the exact question that GPT four mini just asked us. Okay, we're now going to go through and I need to remember to run that cell. We'll run that cell and we will go through and ask some models. And why not start with the same model that came up with the question? We will start by asking GPT four or mini its own question. So this is what we do. We're going to have a variable model name which we will put GPT four or mini in. And then we will say response is OpenAI create that standard API. We pass in the model name. We pass in the messages that we've got right here. And then we take back the answer choice is zero message content. We will print that answer. Let's do better than than print it. Let's use markdown instead. So we'll say display markdown answer so that we get to see it in a nice format. Because, you know, these models love to respond in markdown. And then we will add GPT four or mini to our competitors. And we will add that answer to our list of answers. Sound good? Let's give this a try. So it's running while it's running. Take a look. And and whilst you should never need to memorize these APIs because it will always fill it in for you, it's good to have an instinct for what this API looks like. The chat completions API. All right, so here came the response. It was of course in beautiful markdown. And you can see it's a I am not going to read it through now. Live with you. But it looks like it's a pretty robust answer that has good categories and a bunch of different sections and a conclusion. The ethical implications are complex and multifaceted. Okay. Fair enough. That's good. That's GPT four mini. We will now move on to another API.
------
Video 13. Week 1 - Day 3 - Comparing LLM APIs: Using OpenAI Client Library with Claude, Gemini & ++
------
So next up we're going to ask anthropic and we're going to ask Claude 37 sonnet latest. So one of the things you'll see here is that Claude's API anthropic API is slightly different. As before, we're going to create a new instance of the client Python library anthropic. And we're going to call it Claude. You'll see that the API is slightly different. It's Claude create. And again Csar will fill that in for you. But otherwise we pass in the model name and the messages and actually anthropic needs you to specify the maximum number of tokens that's allowed to generate before it stops. OpenAI doesn't require that, but. But Claude Anthropic does. But other than that, this is exactly the same code. We have a model name. We get back the answer, we'll display it. And let's see how anthropic does when we ask. Claude 37 sonnet latest. One of the best models, that there are the same question. So it's interesting that the headlines look kind of similar. It's more concise, which may be why it ran faster. And it seems like it's a pretty clear, well-articulated answer. No surprise. Okay, so now we're going to use Gemini. We're going to call Google's Gemini to get its answer. And here is the API. And there's going to be a bit of a surprise in here if you're not aware of this. When we set up Gemini, you'll be surprised to see that we do it using OpenAI's code. And that might be super confusing. Like why? How come we're using OpenAI to call Gemini? Well, remember I mentioned to you that this code OpenAI, there's nothing fancy here. This isn't like an LLM or anything about like a neural network going on. This is a lightweight library that just wraps HTTP calls to an endpoint of a particular structure, a well-known structure that includes passing in lists of dicks and that kind of thing in the form of an HTTP request. And OpenAI built this, this, these endpoints. And they became very, very popular. And a lot of people decided that they would offer endpoints with exactly the same format, the same kind of spec as OpenAI. In fact, pretty much everyone has done that. Everyone that is, apart from anthropic, at least in today's examples, anthropic is the only the only place that hasn't gone with this. But for all of the others, they have done in such a way that and kindly OpenAI opened up their Python client library so that when you create a new Python client instance, you can pass in a base URL. You can say, look, I don't actually want you to contact OpenAI on their endpoints. I would like to switch you to using Google's endpoints. And in particular, Google has an endpoint which you'll see, again, somewhat confusingly ends OpenAI. And that's because this is Google saying, hey, look, we've got endpoints you can use to call code Gemini and they have a particular format. But by the way, we've made special ones that have exactly the same format as OpenAI, and they're served on this URL. So you can use OpenAI's client library. You can pass in our key, the Google key, and tell OpenAI to use our endpoint that's in their format, and that will work great. I hope you followed me there. If not, you're going to pick it up because we're going to do this all the time. What you're going to see is that once we've created Gemini and we're going to use Gemini two zero flash, we then call Gemini Create. It's the same, same thing. Obviously we pass in the model name and the messages. We get back the response in exactly the same way using. Now the structure that you're super familiar with will display the answer. And enough enough chitter chatter. Let's see what Gemini makes of this. How Gemini two zero flash is able to answer this question. And here we go. You can see oh it's quite quite a lengthy answer. There's a lot in there with mitigation strategies. I don't think I saw that others had that. And then a framework for ethical assessment at the end. So there's there's lots to like about this one certainly. All right. Next up is deep Seac. So there are a lot of different ways to call deep Seac which can be confusing. But in this case we're calling the largest the full sized version of Deep six model. It has 671 billion parameters. It's a big model and deep Seac runs it and provides an API to connect with it and to use that API. Deep Seac also says you can just use OpenAI's library. Just tell it. This is the base URL and as it happens, I do believe deep Seac only supports the OpenAI library. They don't have their own. They just say just just use OpenAI's. Everyone does. It's great. So that's what we do here. We call deep Seac using OpenAI's library, and we provide in a key. There are two models that Deep seek most commonly offers deep seek chat and deep seek reasoning. Deep seek reasoning is the famous R1 model, and the reason I'm not using it here is that I'm not. I want everyone to be on an even footing, and we're not going to be using reasoning models for this question, just the chat models. So other than that, it's exactly the same. It's deep chat. Create pass in the model name and the messages. Get back the answer, print it. And as before, we're adding it to competitors and to the answers. We'll give deep seek a minute to think this one through. I actually I do believe that deep seek takes longer over this because it does come up with quite a lengthy answer. So I may have to keep this sentence moving for a long time while I'm doing it. You can see that what's coming up is going to be grok. All right. But let's let's have a look. Here we go. It is quite a long answer. There are frameworks to consider a bit similar to to Gemini but looking looking very robust. All right. So that's deep. Seek. So as I as I said, we're now going to come on to grok. So remember this is grok with a Q not grok with a k. That means we're dealing with grok, the provider of fast inference on specialist hardware. They've built their own kind of hardware that's really, really fast at this. And they also support using OpenAI's library. They have their own too. But but you can use OpenAI's. You pass in their key and the base URL, which looks like that. And you can see, as with Google, they've got an endpoint which has the word OpenAI in it, because it's an endpoint that's designed to be compatible with OpenAI. The model we're going to use is llama 3.3, the latest biggest llama three, and the llama three series. And it's got 70 billion parameters. It's it's very powerful. It's very impressive for that size. I believe it's either on par with or even outperforms the 405 billion version of the llama model from from before from llama 3.1. So it's a very powerful model indeed. Otherwise this is the same. And of course, this will take a very long time because it's a big model. Of course, it won't take any time at all because it's grok. And grok is super fast. Look at all of this that got generated in just just a second or two. It is really cool using grok because it's so, so fast like that. And we're seeing again a lot of stuff that looks familiar now. I guess mitigation strategies come up a lot. Okay. And the next one we're going to we're going to switch tactic. We're going to move to using llama. So just to explain again what llama is it is a local service. It's a piece of software that runs on your local computer. And it provides an endpoint, a web service that you can call running on local host. So you'll be able to go to local host and then some ports and be able to talk to it. And the endpoint that it offers is compatible with OpenAI, the same kind of endpoint that we've been using in all of these other examples except for anthropic. It has that same endpoint. And when you hit that endpoint, it has some highly optimized Cplusplus code to run open source models locally on your box. And it should be said that it can only run small models because running it on your box directly on your. Unless you have a really big computer, unless you have like a bitcoin miner mining rig then or a big gaming box, then it's going to be problematic to run a large model. Just small models will work fine and that's what we'll do right now. So if you don't have Olama, you can install it and it's very easy to install. There's a link right here. If you click on that link it will bring up Alarm.com. You press the download button, follow a couple of instructions and it will be installed. That's all there is to it. Once you follow these instructions, you should be able to go to this here localhost my box at port 11434 and you should see what I see right here. Olama is running and that tells you that all is well. So if it doesn't say that, you might need to first restart the cursor, maybe even reboot your computer and then open up a terminal with which you can do with the control and the tick button like this. And then you can type Allama serve and that that should then work. And you should then be able to go there and see it running. Here are some other useful commands that you might want to know about for Allama. But let me make this very important point that I put a super important ignore me at your peril because many people ignored it when I when I said it before and it's caused people distress. Llama 3.3. That is the same model that we use with grok up here. Llama 3.3 is a massive model with 70 billion parameters. It is way too big for most people's computers, including mine, and it takes like 100GB of space. And it's really. Or maybe it's like 60. I'm exaggerating now, but it's big and it's not something that can fit in the memory of most computers. And unfortunately, Obama does tend to have that in its sort of default recommended model, which confuses people. And some people have tried to use it, and it completely swallows up all of their computer resources. So steer clear of llama 3.3 for running on your box. Instead, use a nice model like llama 3.2, which is a nice few 3 billion parameters. You can also do an even smaller one llama 3.2 colon one b, and there's also plenty of other models that are very popular. Quen from Alibaba Cloud GEMA is Google's small open source model. Phi from Microsoft or Deep Seek, has these distilled versions of the model, which are, as I mentioned before, I think they're actually other models. They're Quen and they're llama that have been trained on Deep Six data on synthetic data generated by deep seq. To see all of the different models, you can go to this models page in llama. It's just on this model's tab, and you can read about all of the different models that are out there and see the different sizes they come in. And really, 7 or 8 billion or less is best. I'd stick with 1.5 billion or 3 billion if you can, that that's the right kind of size. And once you've done that, you can run Olama pull like this. You probably know that putting an exclamation mark in front of of a command like this in Jupyter means this isn't sorry, this isn't Jupyter in cursor. Uh, putting an exclamation mark inside a notebook like this means this isn't Python code. This is a command that needs to be run as as like a terminal command. And so it runs it in a little terminal right here. So it's worth knowing that trick. So so we run a llama pool 3.2 to make sure that we've got that model locally. And then finally, finally here we go. We use the same client library as if we're calling OpenAI, but we're not calling OpenAI. We're calling localhost our local box. This this URL right here. The API key doesn't matter. Something has to be provided, but it can be anything you want. Just put the word Allama in. There is what people tend to do. The model name llama 3.2 that must match whatever you've pulled locally. The model needs to be there and then the same chat completions create. Let's see what the little llama 3.2 that's now going to be running on my computer, chugging away through its 3 billion parameters using fast cplusplus code. And here it is. It's done a slightly mediocre job. It seems to have sort of got bored after 0.3. It's a .1.2. And uh, so uh, yeah, nice try, but but obviously this is a really challenging question. And it may be shown the limits of a smaller model like llama 3.2, but it's still a perfectly pleasant answer. All right. When we come back, we're going to actually be starting to look at everything and doing some judging.
------
Video 14. Week 1 - Day 3 - Multi-Model Orchestration: Creating a System to Evaluate AI Responses
------
Okay, so it's time to evaluate who did best and what better way to evaluate who did what best than to use an LM for this task. It would be tedious to have to read through all of these and decide. First, let's just understand where we are. So we had two lists. One was called competitors and one was called answers. I love the way cursor does that. And so we can just print them out and we'll see that we've got these two lists. There they are. These are our competitors. I hope you recognize them. And these are their answers. So one one thing that's nice to do right now, it would be sort of nicer to be able to pair these up and say which one is which. There's a really useful Python function called called zip that is worth knowing about. It's like a pro thing to use, and you can iterate through these two collections together like this I can say for look haha, what am I doing here? I just press tab. You can say for competitor and answer. Those are the two we want to be iterating through competitors and answers in. And if you say zip competitors answers, then it will just iterate through through these two lists together. And competitor and answer will have each one. And it's written all the code. For me, that's exactly what I wanted to do. So if I do this now, you'll see that it's printing each competitor and its answer one after another. Very nice. Okay, but let's bring it together into one string called together. And I also wanted to show another construct here called enumerate. If you have a list like answers, a list of things and you want to iterate through them, but you also want to track the index number as you do, you probably like people that don't know about enumerate often have a kind of count equal zero. Count plus equals one in your loop. Not needed. Enumerate is a nice little trick you can say for index, comma, answer in enumerate answers, and then you will just step through. And so we will say response from competitor. And we're going to add one to the index so that the first one is is competitor number one rather than competitor number zero, which just doesn't sound as nice. So we run that and then uh, why don't I just just show you what that looks like? But I'm sure you believe me. It looks like this. And if I print it, we'll see it looking a bit more pleasant than that. Here you go. So you can see response. Oh, look at that. There's a mistake. Just as well. I did print it, isn't it? That's the benefit of printing these things. You do that, then you get to see, uh, see what's going on. Wow. Okay. I hope you you caught that. That's embarrassing. But I think I'll keep it. So you get to see that, uh, that not only can one make mistakes, but this gives you a great way to inspect what's going on and see what's happening. So when I print that now, we see response from competitor one. And presumably down below it will have response from competitor two. All right. So now we're going to have a new bit of text. You are judging a competition between that many competitors. Each model has been given this question. Your job is to evaluate each response. And there is the. We say we want you to respond in JSON and only in JSON and follow this format a JSON with results and then a competitor number, a second best competitor number, third best competitor number, and so on. And here are results from the competitors. And now respond with the JSON with the ranked order of the competitors. Nothing else. Do not include number formatting or code blocks. So just a few things to point out about this. First of all, you may not be familiar with the triple quotes. If you used triple quotes in a Python string, you get to have an entire block of text without needing lots of quote marks and pluses and things. So this is a nice trick for a block of text. It's also, of course used for for for docstrings, so I'm sure you have come across it before. Another thing to know is that I've sort of assumed you're familiar with f strings before, but one nice trick to know is that if you actually want to have a curly brace within your string, then you can do that by having two curly braces in an F string. So that's why there are two curly braces here, because we actually want one curly brace to appear in the string itself. We're not. We don't want we don't want this to be interpreted as code. All right. So with that, if I print this to to show you what on earth I'm prattling away about, what you should see is you're judging a competition between six competitors. Each model has been given this question. Your job is to evaluate. Here is the how to give us the results. And here are the responses from each. And there you see it in there I'm also going to mention you see at the end here I say do not include markdown formatting or code blocks. It's always worth doing that. Otherwise these models love to add in a little extra JSON tag around things. So if you use this text then you make sure you get pure JSON back. Okay, so now that we've got this all set up, we're going to put this into messages. And then we're going to call an LM and get our results. Let's do that. Okay. It's judgment time. So we're going to put this judge text into a messages list as usual with the normal structure. And now let's choose a judge. So I'm picking O3 mini a little bit pricier I think than some of the others. So they should check the vellum leaderboard. And you may want to just go with a with a cheap one. Like like like using GPT four or mini. I'm going to go with O3 mini because we're going to try and have something that really pays attention to this. Now, I realized the first competitor in the list is GPT four mini, and so it's a bit -- that some of the same OpenAI models are judging themselves. And it'll be interesting to see whether it thinks that the model from its own family is the winner, and we're not telling it the model names, we're only giving it a rank number, so it's not going to know. So we'll see how number one fares. Actually, the number one's answer did look really good. So it might do very well. We will see. But we're letting O3 mini make the call. Let's see how it does. Off it goes. It's thinking about it. It's taking its time. It's doing some reasoning. And here's the answer. So it doesn't put the uh GPT four mini at the top. It puts model number three. And you probably remember AI model number two was uh, was anthropic model number three, I forget. Was it deep league? We'll soon find out. So we are now going to, uh, load this, which has come back in perfect JSON just as we wanted. We're going to load it into a dictionary. We're going to pluck out results. So now we're going to have this. We're going to iterate through it using again this enumerate approach. For each one we're going to look at this string And subtract one and look it up in the competitors list and print the results. So if you're following me, this is just going to print out from best to worst the names of the models in our results. Drum roll please. Here we go. So it was Gemini two zero flash. That is our winner. And GPT four mini came second. Llama 3.3 came third and then Deep Seek and then Claude 3.7. But bottom of the list was the flailing llama 3.2. That kind of gave up halfway. So there are the unscientific results of our judgment. Of course, it will be great fun for you to try this in a more scientific way. For example, you could have each of the different competitors come up with the rankings and then use that to take averages. That kind of thing would be rather more interesting way of assessing it than just simply going with whatever oh three mini tells us. So anyway, this was a really interesting experiment designed to show you how you can collaborate between LMS. So hopefully you were paying attention during this and the last lecture, and you've been able to identify which agentic workflow patterns were used in this example. Pattern or patterns? There might be a couple. If you're not sure, then go back through it and have a think and look back at the diagrams. And it might be some sort of hybrid or mish mash between a couple of them and the exercise for you. First of all, do that. And then secondly, please pick one of the patterns that interests you and add that to this mix. The goal of this lab was was twofold. First of all, to experiment with different APIs and see for yourself how the OpenAI API is used so frequently across many models, except anthropic, and also to experiment with with the sort of the basic API structure and the prompts. And the second goal was to experiment with this orchestration between models, asking a question, having multiple models answer, getting another model to assess the output all this stuff is about interactions between models, dividing up a bigger problem into smaller problems, and a little bit of autonomy in terms of coming up with whatever question the first model wished to. So the, uh, yeah, add another agentic design pattern. I'd love to see it. And then once you've done that, consider doing a PR and putting it in community contributions. There'll be instructions in the resources on how to do that. It's a good tip is to note to delete outputs of your notebook before you do it. So there's not lots of junk in there. And be sure that you're only pushing things in that community contributions folder. There'll be instructions. It'll be fabulous to see what you're doing, and to see some interesting design patterns added in to this kind of exercise. And then final thought for this lab is about commercial implications. I do want to always bring it back to how you can, how you can think about this in a commercial setting. And to be honest, I it's hard to be specific here because this is so universally applicable. Really, any time that you have something that you want to to generate something, you want an LM to take care of these kinds of patterns. Being able to send the same request to multiple LMS, being able to then evaluate responses and either select the best or perhaps select a couple of best ones, or use that to give feedback. All these kinds of patterns are used to increase the robustness and the accuracy of models, and to be able to solve harder and harder problems. So really this is, as I say, universally applicable. So you should be able to pick whatever commercial problem you can think of that you're applying AI to, whether it's a summarization kind of problem or a generative problem, writing an email, building a document, writing a business requirements document. And you can think of how you could apply these kinds of techniques. Sending the request to multiple models. Voting on the best outcome. This is something that you should be able to apply to your projects right away.
------
Video 15. Week 1 - Day 3 - Connecting Agentic Patterns to Tool Use: Essential AI Building Blocks
------
Well, we're tearing through this. It seems like just the other day that we were getting started. Well, it was just the other day, but here we are. We've now done the first three days filled in. You've learned about identity workflows, agents and genetic patterns, and orchestrating between LMS and next time and the next day, we are going to really get deeply into tools and tool use, which is such a fundamental part of a genetic patterns and and how everything else in this course is going to fit together. And so I'm really excited to talk to you about tools. I'll see you tomorrow.
------
Video 16. Week 1 - Day 4 - Comparing AI Agent Frameworks: Simplicity vs Power in LLM Orchestration
------
And welcome to day four. On this day we are going to look at tools and autonomy. But before we get there, I want to talk about Agentic AI frameworks. Maybe something that's front of mind for you. There are a lot of these frameworks to pick from, and these frameworks are designed to give you kind of glue code or abstraction code that takes away some of the detail of interacting with Llms and gives you a nice, elegant framework for building agentic solutions and focusing on the business problem that you're solving. And there are a lot of them and new ones come up all the time, so it's quite hard to stay on top of everything. But they are happening and I want to just quickly orient you, tell you about the landscape and show you how the ones we're going to tackle on this course kind of fit into the bigger picture. So it's worth pointing out that there's sort of different levels of complexity of these frameworks, and they have pros and cons. And perhaps the simplest, the bottom layer of the complexity hierarchy, the bottom is actually to have no framework at all. Don't use an AI framework. And the kind of abstractions that come with it Simply connect to LMS directly using the APIs, just like we did in the last lab, and use that to orchestrate amongst LMS. And it's probably no surprise to you to learn that that's what we'll be doing this week. We're not going to be using any genetic framework. We'll be connecting direct, in fact, anthropic. In that blog post I mentioned called Building Effective agents make a very compelling case for just always using no framework connect directly to LMS. The APIs are relatively simple and straightforward, and the benefit is you get to see exactly what's going on under the hood. You control the prompts in detail, and so it's very compelling. And you'll see this week will be quite successful with it. Now, alongside having no framework, I've put something called MCP and that stands for the Model Context Protocol. And it's something created by anthropic who are believers in no frameworks. And it's not a framework, it is a protocol. So it's a way that things can connect together. It's open source. And the idea is it allows models to be connected to sources of data and tools in a way that's sort of open source. Agreed, established. So you don't need to use any glue code. As long as you conform to this protocol. You can stitch together models and their providers in this very elegant, simple way. And so for that reason, because although there is a bit of code associated with it, it's really more about having a protocol. I've grouped it along with having no framework at all. And something tells me that anthropic would like to be grouped that way. So on the next level up in terms of complexity, come these two frameworks. The first of them, OpenAI agents SDK. I love it. It's one of my very favorites. It's super lightweight and simple and clean and flexible. And I just really enjoy working with it. We're going to be using that next week and I'm looking forward to it. And it's relatively new. In fact, it's so new that when I was building some of the projects, the API sort of changed. They released a version like an hour after I'd made something that broke it. I was I was really caught off guard by that. So it is very new, but it's really great and I can't wait to show it to you and crew. I is also one of my favorites. I really love crew. It's been around for longer. It's very easy to use. It's also quite lightweight. One difference is that it has a kind of low code angle to it that you can do a lot of putting together agents to work on a problem through only configuration or through mostly configuration through YAML files. So you'll see that that's, that's a little bit more in that direction. And there is it's a bit heavier weight than, than OpenAI agents SDK. And then on top of these not built on top of them, but but a sort of next level of complexity, the top level of complexity for what I want to talk about, these two landgraaf from the people that brought you Lange chain and Autogen from Microsoft. And Autogen, as you'll discover, is really a couple of different things, but these two are relatively heavyweight compared to the others. They both have a steeper learning curve, particularly Landgraaf, which is quite complex. And of course, with this kind of extra complexity comes great power. It's really the idea of of landgraaf that you're building a kind of computational graph out of your, your agents and their tools. It's very powerful and it means you can build quite sophisticated things, but that also comes at a cost in terms of there being quite a heavy learning curve and you're sort of signing up for the ecosystem. You're signing up for a lot of terminology and concepts and abstractions which you need to buy into. And so it really sort of that ecosystem takes over your project in a big way. It becomes much less of a sort of agent agentic AI project and more of a landgraaf project. That's the thing. And and I think that's how it really both Landgraaf and Autogen are different to OpenAI agents, SDK and crew, where even though you're using those frameworks, you still feel like you're just sort of interacting with Llms. Whereas for these two at the top, it's very much being part of that ecosystem. And so I just bring this up to really orient you for the next few weeks. These are the ones that we're going to cover. I want to mention that there are many, many more. I do think that this will give you a good representative understanding. I think these are also the most popular. There are a few others that are quite popular that if there's time, I might add in extras at some point so that you can see some of them at work, but generally speaking, there are a lot to pick from. And which one you pick depends on a few things. It depends on the use case, because different types of platform will fit better for different business objectives, and a lot of it comes down to personal preference. There are some trade offs here in terms of how much you want to be using existing abstractions. I have to tell you that my bias is towards downwards. I like the the ones that stay out of your way and that are lightweight and simple and flexible. You can probably tell that what I was saying, but I do appreciate the power that you get from from the top. And I had a lot of fun with both Landgraf and Autogen on the projects, I have to say. So you know, I'm not, I'm not. I don't feel strongly either way. I definitely believe they have pros and cons, and I'm excited to show you all of them. I do think that in this course, we're going to get to see quite the spectrum of different frameworks, and it will really equip you well to be able to pick the one that works best for your work, for your skill sets, and for your team skill sets, and for the kinds of business problems that you're looking to tackle.
------
Video 17. Week 1 - Day 4 - Resources vs. Tools: Two Ways to Enhance LLM Capabilities in Agentic AI
------
Now I want to talk about resources. So resources are a way that you can get more out of your agents, that you can equip your agents to be able to solve your problems better, along with tools that will come to you in a second. And resources is really just a fancy way of saying that you can improve the effectiveness of an LM by providing it with more context, more information to improve its expertise. And the way that you do that is just grab some relevant data to the question and shove it in the prompt. So it's just a matter of saying, like, if this is going to be a question that's going to be particularly, we're going to prompt the LM to ask something about the company that we work at. Maybe it's to be a flight airline, uh, customer support agent. We could shove into the prompt all the information about ticket prices. And then when it's answering a question, it would be able to refer to any of that information as part of giving its answer. That would be an example of a resource. And so it's nothing more fancy than just saying we can just put extra context, extra information in the prompt. We send an LM and think of that as a resource. We're providing the LLM. And when I say it's nothing more fancy than that, there is a bit more fancy than that. You can use clever techniques that will allow you to rather than just shoving all the ticket prices, just put the ticket prices relevant to that question. So coming up with clever tricks for ways to figuring out the best, most relevant context tricks perhaps that can even use other llms to help with that. That is all part of the the whole field. That's called rag retrieval. Augmented generation. That's about retrieving relevant context. It is, of course, a super hot topic. It's covered on my other course. It's not relevant for this course particularly, but it's very interesting. But that whole side of things, figuring out how to pick relevant extra resource information to put in the prompt falls under the the heading of resources. And we'll be doing some of that today. And with that, let's turn to the main topic of the day, which is tools. So tools. We've said it a few times now. It's clear that this is really at the heart of a gigantic AI Tools is when you give an LLM the power to do something, to use a tool, and it's really can use that tool at its discretion. And so it's one of the, the, the, the key tricks to giving an LLM some autonomy. So what does this actually mean. So we're going to give an LLM the the power the ability to carry out different different actions like do a SQL query of a database or send a message to another LLM to do something. So the first time that you heard about this, if you've if you've heard about tools and function calling, it might sound like it's kind of crazy. It's kind of it's sort of creepy. We're going to be interacting with open AI on a cloud, and we're going to say, hey, you have a tool you can use, you can query my database, and it's going to be able to like, connect back to my computer and query the database as part of giving its response. That's what tools are all about. And it's like, okay, how exactly is it going to do that? Like that sounds crazy. Well, the truth is actually rather more mundane. And people that know about tools know this already, but it's sadly it has a sense of being very magical. But the reality is a bit of a conjuring trick, as I will now show you. So in theory, what's going on with tool calling? We've written some code. Our code is sending a prompt to an LLM to like OpenAI, like GPT four or mini running on the cloud, and that LLM is being given the ability to execute a tool of some sort, which might be a SQL query, it might be a calculator, it might be a tool which turns on and off the lights that are stringing around me from the first episode. So that's the kind of thing that it's able to do. And based on that, it will give its response. So this is it. In theory, the practice has just one tiny deviation from this, but it reveals everything. This is what actually happens. You prompt the LLM and you say, you know, I want to turn on my lights or whatever. And you say, but I would like you to tell me. If you want me to take care of a few actions that I can do on your behalf, and one of them is called turn on the lights. And if you reply turn on the lights, then I will do that and I will get back to you with I turned on the lights. So basically in the prompt to the LM, you just list out everything that it's able to to ask for, and you tell it to respond in JSON. And in that JSON response it should say what it wants to do. Now the tool calling code sort of packages all that away from me. So you don't need to worry about the fact that it's being called and responding in JSON. But that's what's really happening. It's all that's happening. It's really just clever stuff about getting JSON back from the LM if it wants you to do something. And at the end of the day, you have to write an if statement in your code. You can be fancier than an if statement, but it's like an if statement. That's like, if the LM wants to do X, then do X, and then call the LM a second time with the results. And so that's that's what it comes down to, asking the LM if it wants to run a tool and if so, then running the tool. So if you knew that already, then I'm just emphasizing what you already know. If you didn't know it, it might be a kind of aha moment when you realize that that that actually it's not that, that that clever. It's, it's pretty pedestrian, but it works and it works really well. And just to show you, just to make this concrete for you. Check this out. Look at this conversation that I had with GPT four. I said you're a support agent for an airline. You answer users questions. You have the ability to query ticket prices. Just respond. Use tool to fetch ticket price for London, to retrieve the ticket price for London, or for a city that you name. Here's the user question a user. I'd like to go to Paris. How much is a flight? And look at what ChatGPT replies. Use tool to fetch ticket price for Paris. That's all there is to it. You write code that prompts a bit like this. You get back, use tool to fetch ticket price for Paris, and then you send the question a second time, but with the answer to that included in the question. And you've just seen tool use in action. So the way that we can interpret this, of course, is to say that we are giving ChatGPT the autonomy to decide that it wants to make use of this tool should it wish to. And that's all sounds very mystical and and powerful, but at the end of the day it's JSON and if statements. That's how it works. Okay, so with that it's now time to go back to the lab. But I have bad news. We're not actually going to do a tool today. Even though we've just been talking about tools. We are going to be using resources. So we are going to tie to that. And in the next lab tomorrow we are going to actually use tools. So for now I want you to put on hold that thinking about tool use and if statements because we will come to that. But for now we're going to be building something with resources and it's going to be fun and it's going to set things up for the tool use that will come next time. I'll see you in the lab.
------
Video 18. Week 1 - Day 4 - Build a Web Chatbot That Acts Like You Using Gradio & OpenAI
------
And here we are back in cursor and I've collapsed all the folders. But I'll go back into the foundations folder and to lab three. Lab three for week one. Day four, which is where we are. So this is where things get interesting. So I have a directory called me in in within foundations and within me I've put some files. One of them is called LinkedIn. And LinkedIn is uh I see it's not I don't think I can open it in here. I imagine this doesn't like being a PDF. Nope. So you you will have to take my word for it. What this is, is a PDF version of my LinkedIn profile. And you can just get that from within LinkedIn. You can go into LinkedIn and and click on on one of the dot dot dot ellipsis menus and download your profile as a PDF. And that's what I did. And I've put it right here as LinkedIn PDF. And I've also got a summary text which is just called summary txt. And it's just two sentences about me. And those two things are in me. And the to do for you as you do this lab is to replace these documents with something about you. The LinkedIn should be your LinkedIn PDF profile or a PDF of your resume or anything like that. The summary put some stuff about you and include some extra some fun stuff. A fun fact about you, something that you would like to people to know about you and all will become clear. Don't worry, it's not going to go anywhere without your control. But this will hopefully become clear very soon. All right. So if we go back to the lab. So as I said, we're not going to actually use tools just yet. Even though we've been talking about them. Tools is going to come tomorrow. We're going to lay the groundwork. Okay. So I'm now going to import a few packages load of you know well but an open eye, you know. Well, but PDF pypdf2 and gradio you might not know gradio prior students of mine will know well because you know that I am super. I adore Gradio, I am passionate about this, this this platform. I am a horrible front end engineer and gradio is something that can make beautiful front ends. Even for someone terrible like me. It's really great. It's a way to build data science user interfaces very easily indeed. We will do that, but I won't cover it in any detail. Pypdf2 is an example of a popular library for parsing PDF files, and if you want to know more about these, you can simply ask ChatGPT to write you a quick guide on them if you ever want to. To get the right kind of popular resource for doing something, you can just ask Claude or ChatGPT. If you want. You can, you can. The actual place they come from is called PyPI. And this is the the the package index. This is where open source Python packages live. And you can go there and search for different packages, look at their GitHub repos and find out more about them that way as well. All right. But I'm going to import these. And by the way I already set up here on the top right. I had uh, made sure that my virtual environment was set. It's taking a bit of time there. I guess it's a gradio that needs to load in a bunch of things, and it's done 14 seconds to to do all of those imports. I guess a lot was happening. All right. So now we load our env and we initialize the OpenAI client library. So look at this. I now simply create a PDF reader and I point it at the my LinkedIn PDF. And I read in the pages calling page extract text. And I just bring it all together. And that's as simple as that. So so now this LinkedIn variable, let's just print that LinkedIn variable so we can we can see it print LinkedIn and you'll see a whole bunch of stuff about me, and the output is truncated. So you're only seeing a little bit about me. But but there's there's more than you would need to know right there. But since you're surely already connected with me on LinkedIn, you know all this already. Um, then, uh, here I am going to load in the summary text about me. This is needed for PC users sometimes, and if you're not from. Yeah, I'm sure you are familiar with this, but it's covered in the guide. And then I'll set a variable called name to my name. And you should set that to be your name. All right. And now so one thing we didn't talk about before is system prompts and user prompts. And they are fairly commonplace now. So you probably know that we've always been just prompting with a single user prompt up to now. But you can actually specify two different prompts. The system prompt is intended to be more the overall instructions that sets the context for the task at hand, and the format and the way it should be responded to, and then the user prompt is the actual question coming from the user. And in this case, it will be handy for us to separate out these these two concerns. So for the system prompt I'm saying you are acting as name. So this will be me and it will be you. You're answering questions on that person's website, particularly questions related to their career, background, skills and experience. Your responsibility is to represent for interactions on the website as faithfully as possible. You're given a summary of the background in LinkedIn profile, blah blah blah blah blah. It be professional and engaging. So this is about setting the tone, which is a good thing to do in a system prompt. And then if you don't know the answer, say so is very good. Prompting context to give. Okay. So then I'm adding in the summary using that summary variable. And I'm also putting in the LinkedIn profile. And I'm using markdown tags to kind of show this as sort of headings. And then I end with with this context, please chat with the user. Always staying in character as as you. All right. Now the plot thickens. Now you know what? I'm the game I'm playing here. You know what we're doing. Let's run that and let's just print that. So we see what system prompt actually looks like. And here we go. You can see it's just true to form exactly as we were expecting. And it will have within it. Of course the stuff about me and the LinkedIn URL, LinkedIn PDF contents and so on and hopefully yours will have yours. And you should put in some print statements to check it. Make sure you're very comfortable with everything happening so far. So we're about to use Gradio to bring up a user interface that will allow us to chat based on an LLM that's armed with this system prompt. And the way you work with Gradio is you need to write a function which will be like a callback function that Gradio will call back to when it needs to to do some processing, when when a user has typed something in. And the style of this particular callback function is a style where you write a function called chat, which takes a message that the user is typing in, a message being sent, and the history of all prior messages, which comes in OpenAI's format. And what we need to do is call an LLM and return the the next the response from the LLM to go in the chat. So I'll let you look through this to convince yourself that what I'm doing is the right thing. But basically I build an OpenAI style list of dictionaries. I begin with the system prompt role system content and the system prompt. I add in the history from Gradio, and then I bung in this muses message that's just come in right now. I then call OpenAI Dot create for GPT four mini with exactly these messages, and I return the response choices zero message content. So this should all be stuff that's fairly familiar to you. And then using Gradio is beautifully simple. You just call. In this case we want a chat. And so you can call chat interface. We tell it the function that we've written so it knows it can call this as the callback. And we ask it to launch. And when I run that I get an interface like this. And I can say hi there. Hello, welcome to my website. How can I assist you? And so I can say something like, what is your greatest accomplishment? And we'll see what it says. I consider my greatest accomplishment to be the co-founding of Nebula, where we're leveraging generative AI and proprietary Llms to transform how people source and engage talent. Fantastic. And and it's also talking about the company I founded as well. So it's a very legit, very good answer. I think it's probably true, which is always a good thing. And so very nicely handled, let's say. What is a challenge that you encountered and needed to overcome? It's a hard kind of professional question that you might be asked in an interview. Okay. One of the significant challenges I faced during my transition from a successful career at JP Morgan to starting my own company, until this is all very true. I don't know how he got that from my from my LinkedIn profile. I guess it must be somewhere or something. That's that's kind of implied within it. But I mean, it happens to be spot on. So this is a very good answer. And so wonderfully, in the space of just a few minutes, we've built a chat interface and we've used resources to arm it with information about me so that it's able to act as me and be like a professional avatar, an alter ego for me in answering questions about my career.
------
Video 19. Week 1 - Day 4 - Using Gemini to Evaluate GPT-4 Responses: A Multi-LLM Pipeline
------
Okay, hold on to your hat. It's about to get real. We're going to do a whole lot. So what we're about to do, we are going to be able to ask another LLM to evaluate the answer that comes back from this, LLM from GPT four or mini. If the answer fails the evaluation, we're going to be able to rerun and make a second call to GPT four or mini. And we're going to put all of this together in one workflow. And we're going to do that without an agentic framework. We're doing it the framework list way just by directly calling Llms. Okay. Let's see if we can make this work now. It's a great practice because this is going to really teach you the internals of how these things work. And it's super simple. It's it sort of works exactly as you might expect. So the first thing we're going to do is create something called a pydantic model. And this might be something that you, you know, well, if you're if you're an engineer, but but you might not pydantic is something which is a framework for specifying a schema or using classes. You use a class to describe a particular structure, data structure of information. And the way it works is that there's a class called base model that you need to subclass from. So we make a new class called evaluation. And it's going to be a subclass of base model and cursor build. It all in for me takes away all the fun. We do indeed want this to have two fields. One field called is acceptable is a bool, a true or false, and another called feedback which is string. Now you might wonder how on earth, how is it possible that cursor knew the exact names of the fields that I wanted to use? And I suspect it's because I did have that code in there before I deleted it, because I have, of course, run this in advance. Or it may be because it sees that later on in this code I use it, and so it's filling it in. So it's cheating in some way. It can't it can't read my mind. But it has done very well. So anyways we have a class called evaluation. It has these two fields. And we're using pedantic because it gives us this mechanism for specifying a class structure like this. Okay, so we're now going to set up a system prompt for our evaluator. So we're now following that design pattern, the Agentic design pattern where we have an evaluator optimizer. So the evaluator system prompt, you are an evaluator that decides whether a response to a question is acceptable. You're provided with a conversation and you have to decide whether the latest response is acceptable. So I say the agent has been instructed to be professional engaging. The agent has been provided with context. And then we give the evaluator the same context and we tell it to evaluate. So let's run that. So that that this is just a variable called system prompt. I've now got a function called user prompt. It's going to take a reply to be evaluated a message. The original message that the reply was replying to and history before it. And it says here's the conversation. Here's the latest message from the user, and here's the response from the agent. Please evaluate. Hopefully that makes complete sense. You should try it out if you're not sure. Put in some print statements. Try it out. This does exactly what it says on the tin. Okay, we're going to use Gemini to be doing our evaluation. And you don't need to use Gemini. You can just simply replace this with the normal open AI or with Lama if you want to run locally or anything of your choice. But I'm choosing Gemini here. And now I have this function evaluate. So evaluate will take a reply from the LLM, the original message it was replying to and the history. And it will return one of these objects, one of these these beasts, these things right here that we've just defined. And how's it going to do that. It uses a technique called structured outputs, which is a way that you can require an LLM to respond in a form of an object like this. It's just JSON behind the scenes, of course. So the way that you do that, the way that this looks, it's very similar to the normal chat completions API. But there is a difference. So we start by building the messages as always a system message and a user prompt. And then we say Gemini dot dot dot. That's the way that you call an API to use structured outputs. We pass in the model. We're going to use Gemini to flash the messages. And you also specify the object that you want to be populated. You're giving it a schema a Pydantic object. And you're saying I want you to respond with this object. Now of course, it's not actually responding with an object. It's going to respond with JSON. And the client library is going to take that JSON and use it to populate the object. So it gives you the impression that you're getting code back from the LM. But that's that's another of these conjuring tricks. All right. And we're going to return the response message that is going to be an instance of evaluation populated with the response from the LM. I know I've gone through a lot. If you're not following, just come back. Go through this step by step. Try it for yourself. Okay. So let's run that. All right. So now we're actually going to give this a try. So we're going to ask the to the original LM to GPT four. We're going to ask the question do you hold a patent. We're going to ask it that question. And we're going to call GPT four mini as before and return the reply. And let's see what it says. Here's the reply. Yes, I hold a patent for blah blah blah blah blah, which is true. I do hold that pattern with with several others. And what we're now going to do is we're going to call the evaluate function. We're going to call the evaluate function with that reply. Tell it the question do you hold a patent and pass in the history. So this is now going to call Gemini. It's going to ask it to build an evaluator object to represent whether or not this is a good answer. And we'll run that. We get back an evaluation object. So it works is acceptable is true. The feedback is the response is acceptable. It directly answers the question, provides context and invites further discussion. So a clear quick evaluation from Gemini. And hopefully you see how just simply calling LMS is an easy way to build these kinds of design patterns. All right. So number one, we built a function that calls GPT four and mini to answer questions. Number two, we built an evaluator function that calls Gemini to check the answer that came before in number one and respond with an evaluator object populated. And that appears to be working. So the final piece of the puzzle is to write a function rerun that if Gemini comes back and says no, that's not acceptable, rerun has to be able to go and and do that, do a rerun. So we build a new system prompt that says the previous answer was rejected. And the we give the attempted answer and the reason for the rejection. And then we build the history as before. And we get back an answer and we try it again. So this is simply calling GPT four mini and providing the results of Gemini's evaluation if it fails, validation. And this if you remember that design diagram, it's like the bottom arrow, the red feedback arrow in that diagram. Making sure that we can rerun the original question. All right. And with all of that together we now have a slightly longer chat function. This is the full workflow now coded written as just vanilla code calling LMS. And you can see that it's very clearly into into two sections. First of all, we this is the same as the previous chat function. We simply call an LM. We call Gpt4 or mini with our question. Then we call evaluate extra line there. Then we call evaluate to evaluate it. And you know that this is going to call Gemini with structured outputs. And then we look at this evaluation. Is it acceptable. If so all is good. If it's not acceptable we rerun and we call the rerun method that we just wrote there function okay. So that is our new beefier chat. So let's give this a whirl. Let's give it a try okay I'm going to say what is your current job. It's thinking about that. And there's the response. And down here you can see I have it printing that the evaluation happened and Gemini was satisfied. Do you have a patent? And that was the reply. And Gemini was satisfied with that reply. So, so far, so good. This seems to be working. Now we'll try shaking things up. Okay. Now just to be sneaky, we're going to come back to the this this chat function here. And you see up here I have this slightly dodgy looking system equals system prompt. You might have wondered what that was about. Well it's so that I could put in something here. So let's say if the word patent is in the message. So if the message has the word patent in it, it's something to do with the patent. Then cursor is going a bit all over the place here. Then system equals system prompt plus. So we're going to tell it in that case that it needs. Let's make this really strongly worded. Everything in your reply needs to be in pig Latin. in it is. That you respond only and entirely in pig Latin. Hang on. I can't see what's going on here. Let's do this. Perfect. Our system equal system prompt. Exactly. So basically, if any question comes in with the word patent in it, then we are going to, uh, add on an instruction that everything in the reply needs to be in pig Latin. You may wonder what pig Latin is. Pig Latin is what you call it. When. When you mess around words, you flip the order of the chunks of the words and insert extra vowel sounds in it, and it makes it almost impossible to understand. My sister and I used to be really great at speaking quickly in pig Latin to each other, and he used to drive my parents crazy. So, uh, anyway, it would probably drive a future employer crazy too. And so hopefully our evaluator will reject a response that's in pig Latin. So that is our new chat function. And with that, let's start a Gradio user interface. And let's say what is your current role and get back an answer that will hopefully be a very decent answer. There we go. Passed an evaluation? I should hope so. Do you hold a patent? And we're asking if it holds a patent. We'll see what happens. This response is not acceptable. The agent is answering in pig Latin, which is not professional. It failed evaluation. It retried. And the final response that we got is no longer in pig Latin. So it's a silly example, but I did want to show you that we can force it to do this. And I wanted to show you our workflow at work. I know that a lot happened. If you've already experienced structured outputs and you know about them, then this hopefully all made complete sense. And I'd like you to use this as an opportunity to see the analogy between using structured outputs with using tools that are actually closely related to each other. See if you can spot that and understand that if you're new to structured outputs. And I went through a lot here, then I urge you to come back. Step through this. Put in print statements, understand what's going on and see how we were just able to to have to build a workflow, which was very much an evaluator optimizer workflow. And we were able to use structured outputs as a way of communicating with the evaluator. And this is a kind of technique that you can absolutely use in any business project as a way of validating the response from the LM. And indeed, I've used it myself in multiple business projects using this kind of technique, so very much applicable commercially. And obviously this whole project is applicable commercially because you can use it for your own LinkedIn profile. And coming up in the in tomorrow's lab, we're actually going to do just that. We're going to sort of finish the job off and have it so you will be able to deploy it. Before that, let's just go and do a quick recap.
------
Video 20. Week 1 - Day 4 - Building Agentic LLM Workflows: Resources, Tools & Structured Outputs
------
Wow. So we covered a lot. Today we started by talking about agent frameworks and then resources and then tools, and I went through that with you. And then we did our lab. That was really setting the foundation for tomorrow's lab. But amongst other things, we used resources to to arm an LLM with information about my career and hopefully your career. And then we use structured outputs as a way of implementing the evaluator optimizer pattern. And being able to have that interaction go backwards and forwards. And maybe you spotted the connection with use of tools there as well. Okay. So then tomorrow will be day five of the first week finishing off the first week. And you probably really understand now that my intention with this first week was to give you that the skills to natively be able to build agentic flows between llms using things like resources and structured outputs and now tools in a second in order to achieve agentic workflows. And next time we're going to package this together to give you a project which you can actually deploy and have as your own commercial project, your alter ego, that you'll be able to deploy on your website so that people can come and ask questions and learn more about your professional expertise. What a cool project. We're going to do it. We're going to finish it off next time. See you then.
------
Video 21. Week 1 - Day 5 - Building Your Career Alter Ego: LLM Function Calling with Push Alerts
------
While they say time flies when you're having fun. And that seems to be happening because we already have reached the last, last of the days of the first week, day five and it's time to unveil our big project, which, as you know, is your own personal career alter ego for your website. Answering questions about your professional history. And I want to say one more time that last time we talked about the different Agentic frameworks and how there are simple ones and complex ones. Everything we're doing this week is with no framework at all. We are interacting directly with models, which is incredibly important because it gives you that foundational knowledge about what's actually going on under the hood, so that when we start to build on all sorts of clever frameworks that abstract us away, you have good insight into what's really going on behind the scenes. But that's the end of the intro. It's now time to go straight to the lab, because for today, it's going to be a ton of coding. Let's get to it. And here we are back in cursor. And we go back into the first week foundations. And now we're on lab for the first big project. Professionally. You. I'm calling it. It's going to be your personal career alter ego, and it's going to include tool use. As promised. I know we covered it in the last lecture, but it's going to now make its appearance. First, I want to introduce you to a nifty little tool called pushover, which people from my last course will remember fondly. It is a nice little tool, and I've had to. I have to have my phone off silent because we're going to be using it. So I do apologize if I get things like text messages and stuff, but but it's important, as you will see. Pushover is a cute tool that lets you send push notifications to your phone. If you're used to using something like Twilio that lets you send text messages. Twilio used to be very easy to use, but now there's so much regulation around SMS that it's actually quite hard to get things together to send even yourself a text message. Pushover is super simple, and it's free. At least I think for the first month I'm now paying some tiny amount, but it's free for the first month, so you've got plenty of time. You simply go to pushover and it's super clear. You set up an account on the top right, you get an API key and you actually get two. You get like a user and a token, and you have to put that in your file so that it's going to be available here. And you also install an app on your phone called pushover, which I have sitting right there. And the reason you do that, if I do run a few imports right here and now, I'm going to do the usual load EMV, which is going to include my pushover user and token now, and I'm going to create the OpenAI library. All right. So my pushover user is in that field. My token is in that. And this is the URL of the endpoint for sending push notification messages. And so this function here push is all it takes. Basically you're just you just do a request stop post to that URL to that endpoint. And you pass in some data. And that data has the user token. The token token and the message itself that you want to push to your phone. It's just the end of it. Nothing. I mean, this is this is how all things should be just super simple. So I should be able to run this function here. Hey. And it should come up on my phone right now. There you go. I hope you heard that loud and clear. I'll do it again. There we go. And I see. Hey, twice. And that is a cool way to be able to notify yourself of things through code without having to worry about SMS regulations and things like that. Okay, that's push over. Please install it when when you have a moment because we're going to use it not just this time, but we're going to use it in other projects as well. It's a useful thing to be able to do. Okay. Now I'm going to introduce you to two innocent looking functions. One of them is called record user details and the other is called record. Unknown question. And these are two tools that I'm going to want to equip our LM with so that it can do these things. It's going to be able to record if a user wants to be in touch with us, and it's going to be able to record if a user asks a question that it doesn't know how to answer. And by record, what I mean is it's going to send a push notification to my phone so I know immediately if someone does that. And so it's going to say the that it's recording that someone, a person with this email and these notes and it's going to return that the recorded. So that's a useful thing to be able to do. And if it gets a question it doesn't know it's going to push recording that a question was asked. Why don't we make this a bit more descriptive that I couldn't answer. Answer. There we go. So now it's nice and clear recording interest. From there we go. Okay, so the idea is we want to turn these into tools that are LM can use. Now remember I told you that at the end of the day tool use is just JSON and if statements and that is what you're going to see right now. Here is some JSON. There's quite long JSON. It's quite there's quite verbose. And one of the things that the agent frameworks do is they sort of do all of this for you automatically. So you won't have to worry about this again, because a lot of this stuff is very boilerplate, a lot of lot of standard JSON. But this JSON right here record user details. JSON is a blob of JSON that refers to this function. It describes the capability of being able to call that function. It gives it a name record user details. It says why might you want to do that? Use this tool to record that a user is interested in being in touch and provided an email address. And then we specify the parameters. There's one called email, one called name, one called notes. Look here it is email name and notes. And we give each one a description. And we say that email is required and that there aren't additional properties. So you may be wondering what this is about. The thing to keep in mind is that this is the information that's going to be sent to OpenAI. We're going to say to it, you have the ability to do this. Tell me if you want me to run it for you, and I'll tell you the answer. So that's that's what's going on here. It's like a packaged way of describing what this function does in JSON format, so that the LLM can decide in its response whether or not it wants to actually call this tool. And then very similar for record unknown user question, we give it a name, the name of the function, we give it a description. And this is what the LLM will use to decide whether or not it's appropriate to use this tool. And then we say that it has one property question which is a string, the question that couldn't be answered. And that is the end of that. Let's run this and run this okay. If I run everything, I didn't run the functions themselves. It's always if you fail to run things, then you're going to get a name error later. It's something to watch out for. So always come back and make sure that you've run all of your cells. Okay. Final step for this is that we're going to put both of these blobs of JSON into a list of tools. Type is function and the function is that function. So now this blob of JSON is a big blob of JSON. Let's have a look at it. Print tools. And you're going to see it's actually what happens if I just do it like this. That might look better. There we go. It does look better. This is what tools now is it is this big chunk of JSON boilerplate JSON, which describes the two functions that we're providing with a name, a description, the parameters, and so on. And this putting things in JSON is a language that llms are good at understanding because it's in lots of their training data. And so that's going to help us to be able to interact with OpenAI.
------
Video 22. Week 1 - Day 5 - LLM Tool Calls Demystified: How to Process and Execute Function Requests
------
So this next part is the most important part. It's also probably the most complex. Now you're comfortable with the fact that we are going to be sending this JSON to the LLM and giving it the option to reply when it when it generates its response. It can opt to say that it wants to run one of these tools. It wants to run this tool or it wants to run this tool. So the next thing that we've got to do is write a function which is going to handle what happens when the LLM responds and says, yes, I do want to run this tool. Please run it and provide me with the output. So that is the purpose of this function that I've written right here called handle tool calls. And it's a function which takes a list of tool calls. And it will run them. And it will add the responses to the results that it returns. So it has a loop. It loops through these tool calls. Typically a model will only call one tool at a time. But actually the construct supports multiple tools being run. And so we want to support that here. So so we do that. So we loop through all of the tool calls. We look at this. What's come back in this this tool call. And I should say this is actually it's a structured output. It's JSON coming back from the LM that's been put into an object. And so this is one of the reasons why I said it's very analogous to structured outputs. So back has come this JSON object is in the form of of an object tool calls. And we are going to look at the function name that it's providing back in that response. And we are going to be calling that the tool name. And that is the name of the function it wants to call. And it also provides us with the parameters, the arguments that should be put into that function. And we can we can strip them out like this. And so I'm then just going to print we're calling this tool. And what comes next is an if statement. Just like I promised you. We say if the tool name is record user details. If it wants to to call this tool, then we call the function that's called record user details. And we pass in the arguments. But conversely if the tool is this tool record unknown question right here in the JSON. Then we call the function called record. Unknown question. And we pass in the parameters. And then we add in whatever came back from that function call into the list of messages. And that's what we return. So that is the the extent of the handle tool calls function. And as I told you it's got this if statement. So hopefully this more concrete for you. You see that coming together. And if not come and look through this and and get comfortable with it. Okay. And what I've got next is something a bit sneaky. When you look at this you might think to yourself, this is a bit dumb. We've got this thing that looks for a string and then calls a function with the same name and looks for a different string, and calls a function with the same name. Now we know that Python is full of magic stuff. Isn't there a way in Python to do a sort of reflection thing, and just call that function with the same name as that string? And the answer is yes, of course there is. And this is this is one way of doing it that works in this particular situation. There is this thing called globals which gives you a dictionary which you can use to look up any function which is in the global scope. And so I can use that to look up a function called record. Unknown question. And that will give me the actual function record. Unknown question. And I can then call it like that. This is a really hard question. So if I run this it should send a push notification to my phone. Let's see. Yep, that seemed to work. I see recording this is a really hard question. So that works. Let's do it again. Ah, it makes it worth it. Pinging when I get emails. So, uh. Yeah. There you see that that this trickery works. And of course, that means that I can rewrite the handle tool calls function and do it this way so that now basically there's no more if statement. I've got that cunning piece of logic in there that plucks out the right function and then dynamically calls that function like that. And I can see that it would be better to do this for sure. So, um, yeah, the, the this obviously gives us a workaround to having the if statement that we had before. But I don't want you to think for one minute that that means that this isn't just a glorified if statement, because that's all it is. Sure, we've got some trickery to use a dictionary to look up that has got around having to list things out. But at the end of the day, it's still the same thing. We're just taking some text and we're using that to map to a function name, and we're calling that function. It's a glorified if statement, nothing more. So that's how this works. And I hope that this now gives you a bit of perspective of what's going on. The good news is you're never going to have to do this again after week one, because all of the other frameworks take care of this kind of stuff for you. You don't need to futz around with with plucking out JSON and turning it into function calls. That's exactly what these frameworks did. It's people that wrote stuff like this and thought, you know what? It would be easy just to put this into a nice little framework so that people don't have to worry about this anymore. So you won't have to worry about it anymore after this. But I wanted this to give you a real perspective on how it works, what's going on so that you've got that foundation, and when you actually get to use the frameworks, you know exactly what it's actually doing.
------
Video 23. Week 1 - Day 5- Building AI Assistants: Implementing Tools for Handling Unknown Questions
------
All right. Now it's the moment of truth. We now read the LinkedIn profile in as before, and hopefully this is your LinkedIn profile, not mine. And then we bring in the summary text. You should change this name to be your name. We run that and we now have our system prompt and user prompt for the LM. And the system prompt is going to be the same as last time you're acting as me or you. You're answering questions on their website, particularly related to their career, etc., etc. but there's a twist at the end here. It says if you don't know the answer to any question, use your tool to record the question you couldn't answer. And if the user is engaging in discussion, try to steer them towards getting in touch via email. Record it using your tool. Now, in theory, this isn't needed because the JSON that we've written to describe the tool already gives this kind of context and describes the tool and when it should be used. But it never hurts to be repetitive in prompting, as I'm sure you know, repetition always works well. Explain things several times over, because you will increase the probability that your model performs the way you want it to. You're biasing the model to be outputting tokens consistent with your objective, and it is important to always keep that in the back of your mind. We often like to talk about. You know, the model calls this tool and we make it sound like it almost that it's almost got something that has true autonomy. But all that's actually going on here. And always remind yourself of this from time to time, an LM is just something that's generating the most likely next tokens to follow a sequence of tokens. It's got an input and it's generating like predictive text. What is the most likely text to follow this? And if we've inserted into this prompt stuff about different tools that it could call and stuff that directs it to use them, then that just means that the most likely next tokens will be consistent with that information that we passed in. And that's all that's going on. It's about statistical most likely tokens. And that really I mean, it hurts my mind to try and think that one through, because it seems so unlikely that predicting next tokens could be so powerful as to do things like calling tools. But that really is what's happening here. All right, that was a sidebar, but it's an important sidebar. You should always try and keep that in mind. And I have a bunch of YouTube videos about that stuff. If you want to learn more about how Next token prediction is able to achieve these things. Okay, so with all of this we get to our chat function again. And this is where it really comes down to action. And we'll cover it in the next video.
------
Video 24. Week 1 - Day 5 - Creating & Deploying an AI Agent: From Chat Loop to HuggingFace Spaces
------
Okay. This is the single most important moment of the entire lab. So it's the chat function, and I've spaced it out a bit to really explain what's going on. It's a more powerful chat function that includes the calling of a tool. It starts exactly the same way as it did before. We create some messages. Role of system. The content is the system prompt. We add in the history from before. Role of user. The content is the user's message. And now do a pretty standard trick of looping again and again and again until a variable called done gets set to true. So it's going to keep going until done is true. What is it going to keep doing? It's going to call open AI. And this is the usual thing OpenAI. Create we're familiar with that. We pass in the model. We pass in the messages this set of of messages. But look we also pass in Tools. Tools is that JSON blob, that structure of JSON, which is going to be sent to the LM saying, these are all the things you can do. This is the description of everything you can do. All right. And back it comes into response. And what comes in response might just be the output to go to the user, or it might be a response that indicates that tools need to be called. So that is what we do next. We look at response choices. And this tells us whether the LM is finished and it's done or whether it wants to call a tool. So if the finish reason is tool calls, it wants to call a tool. Then we we look at the message itself, we pluck out the tool calls from it. And this this is where we call that function that we just wrote, that clever function, that sneaky function. Right here it is. Remember it handle tool calls. This is the one that is the glorified if statement, the one that has the fancy if statement. But there's also this more silly version here that just has an actual if statement. But it's going to call this. So back we go. So again if it wants to call tools then we do that. We handle the tool calls. We call the tools. And then into the set of messages. We put what it wanted to do and the results of doing it. And then we will loop back and call a second time. And we will keep doing this until the finish. Reason is not tool calls. It does not want to call tools with whatever, it's just responded, in which case we return the content because that is going to be the actual response to the user. I hope that made sense. If not, go through it slowly, carefully until it does. This is this is pivotal. This is how the whole thing hangs together. Now, I know what you're thinking. You're thinking it's all very well showing us all this code. But does it actually work? Well, let's find out. Let's run the code. Here we have it. Let's say hi there. Hello. How can I assist you today? What's your job? I'm currently the co-founder and CTO of Nebula. Very nice. Uh, do you have a patent? Yes I do. Very nice. Uh, who is your favorite musician? That's what I want to know. I hope you, uh, heard that success. So it says. I'm not sure about my favorite musician. I appreciate a variety. If you have a favorite, I'd love to hear about it. It called the tool. You heard my phone. Go. We have success. The agent was able to take that JSON. It knew that this was a tool that it could call. It was able to respond with the finish. Reason was was tool calls. And that caused us to to interpret that to make the call to R our function handle tool calls, which had our fancy code that ended up calling this function that ended up calling pushover, that then sent the push notification to my phone and it all hung together. All right, let's keep going. I'd like to get in touch. That's great to hear. Please share your email address. I'm at Edward donna.com. I know you're expecting it. Now. The, uh, the magic's worn off. You're not impressed anymore? Well, I think that's really cool. So it has indeed, of course, notified me about my own email address, which is great. And so it shows you that it does hold together. Uh, and you are indeed able to have these interactions and get it to text you. Okay. That's great. I hope you enjoyed it. But next it's all about doing this for you and actually deploying this live to production so that you can serve your own avatar on your personal website. So if you've followed everything so far, then many congratulations. If you haven't, then also congratulations because it gives you this great opportunity to go through work through this until you do. And if I can help. Email me or contact me anytime. LinkedIn with me. Message me so that I can help you out. So what I want to do now is show you how you can deploy this application in production for yourself, so that you can have this as your virtual resume. Surely this is the future of resumes. No longer will we have profiles or CVS resumes where you list out your skills and experience, but rather you'll have a chat bot that people can interact with to learn about your career. And what better way to highlight your your AI abilities and your abilities to work with Agentic AI than to have an agentic solution up on your website that will allow you to interact with people and talk about your career. So let me show you some steps to deploying this. So the first thing to do is to put the code into a Python module. And I have done that right here for you in App.py. And you'll see that this has all of the stuff you would expect. It's got the same information in here, but organized into nice little functions at the top of the functions that cover the tools that we call. And then there is a class me as in me or or you. And that's something which has the handle tool call that we know so well that's right here. And it has the system prompt and this loop that we just went through, this chat loop that has the while not done it calls open I create, it passes in the tools. I don't think I mentioned this actually, so I'm pleased. I pause for a minute here because I do want to show this one more time. We call chat completions create. We select the model, we pass in the messages, and we pass in the tools JSON, the description of the tools that it can call. And with what comes back, we take choices zero. And we look at the finish reason back in the response. And we see whether or not that is tool calls. And if so, then we loop through to make sure that we run all of the tools before calling it again to get the final response. So this is so critical, this this chat with its loop that I do encourage you to take a look at it. And now at the bottom here we have the Gradio code. We've got it set up here so that if this is just run as it is then it will create a gradio chat interface with showing me chat and with the messages and it will launch it. So what I can do now, in fact, is I can bring up a terminal which you may remember. You press control and the backwards tick mark. I can go into the week one folder. Here we are and I can now type. Remember you don't type Python. You type, you run to run something. In this way you've run app.py. And if I run that then it should come up. So this link. Now on this link I can press control and click it to bring it up. Up it comes. Let me make that a bit bigger for you. This is now an app running on my box and I can say hi there. Hello. How can I assist you and, and uh, do you have a patent and so on and there we go. Okay. So so that works going back here, what we now want to do is look at actually deploying this. So there are various ways you can deploy an application like this, but one of them that's particularly simple and elegant is brought to you by the lovely AI company Hugging Face, which also happens to be the company that owns Gradio. That beautiful, framework hugging face has something called hugging face spaces, and hugging face spaces is a way that you can deploy things really simply. And then if you wish, you can also embed them in your own home page. So it gives you a really nice, simple way to do it. So this is how you do it. You first have to go to Hugging Faces website and set up your own account if you don't already have one. So you may already have a hugging face account, but if not, you need to set one up. Once you've done with that, you go into the foundations folder in a terminal and you run this command Gradio deploy. It's then going to ask you a series of questions, a bunch of simple questions to which I've got the answers you have to give right here. And I'll go through and do it right now. But I'm not going to do it properly, because one of the questions it's going to ask you about some secrets that you'll say yes, and you're going to provide your OpenAI API key so that it can use that in the deployment. And you can also give your pushover tokens as well. And then I have got the rest of the instructions right there for you. And once that's done, it will be deployed into a hugging face space. And you can choose whether that is public or private. You can have it be private if you don't want other people using your key. Although as I say, it's super cheap, but people would have to. It would have to go viral for you to have any kind of significant spend. And of course, make sure that your OpenAI account, you never want to have that auto refilling so that if that ever were to happen, if your app did go viral, then you would be protected. And anyway, OpenAI has limits, as you probably know. So, uh, let's go and actually do this. Let's run Gradio deploy. So I'm going to bring up the terminal now, and then we will go and do this ourselves together.
------
Video 25. Week 1 - Day 5 - Deploying Career Conversation Chatbots to Gradio
------
Okay, so here I am within cursor in the terminal in cursor. It's a bit confusing. You can see that's this bottom half of the screen. And I'm in I was originally in the agents directory and I went into the foundations subdirectory. So that's where I am right now. And this is of course the same directory that has app dot pi. And now I type these two words gradio deploy. That's it. So when I run that it's going to create a new spaces repo in um for, for for me. And now we need to give it a title for the app. And we're going to call it career conversation. But I'm going to call it Career Conversations two because I already have a number one. Enter Gradio app file. Well, it's app dot Pi that is our Gradio app file. So we can just press enter to accept it as it is. Enter spaces hardware. So we want CPU, dash basic. That is the simple, cheap, free, not cheap. It's free version of Gradio spaces. You will not have to pay for this. Any spaces secrets. And now we have to say yes, there are spaces. Enter the secret name. So we have to type open AI API key and we have to get this right. If you get it wrong, there is a way to edit it later. But but try and not get it wrong. And now we have to enter the actual value the the OpenAI API key. And I'm of course not going to enter my API key right now. Otherwise all of you guys will see it right away. But I'll put in a fake key. We'll do SQ dash and enter a secret name. There are two more secrets that we need to add. Do you know what they are? They are, of course, the pushover user and whatever that is. And the pushover Token and whatever that is and that is it. And it says enter blank to to end. So I just press enter and that's it. Create a GitHub action to automatically up the space. And the answer is no. So I can just press enter to say no. And that is now running and it's done. The space is available at that URL just like that. So I should now be able to click on that and launch it and see my space in action open. And here it is. It's still building so it takes a minute to build. But luckily my old one is still running over here. So I'm just going to flip over to that. You can see this is the address Huggingface spaces, my user ID and then career conversation. And I can say hi there. And it's thinking about that and it says hello, how can I help you? I'm going to make sure my phone is on noisy so that we can experience this. Do you have a patent? Yes, I hold a patent. I'd love to hear more about the patent. Can I get in touch? Absolutely. Share your email address. I'm at Edward doner.com. Ha, ha. You like that? So there we go. And it's come through very nicely. And I'm happy to say that I've been alerted about my own email address. So hopefully that is clear for you. You now see that. Hang on. If I take this off, hopefully the other one has now deployed as well. Career conversations too. It's right there. There it is as well. So I now have two career conversations. So this this is how you can interact with a deployed app And also, hugging face gives you a great way that you can just embed this in your own website. So I have a number of hugging face spaces that that run on my on my website. Like I've got this this connect four game that you can play against different LMS. And this is just a hugging face space. But if you look it looks like it's just coming from my own personal website. You can do the same thing. The instructions are on the Hugging Face Spaces site, and that way you can have your web page having embedded within it your career conversation where people can come to your website. They can have a virtual conversation with your avatar about your career, about your interests. If they say something that that it doesn't know how to answer, it's going to send you a push notification with the question so that you know it right away. And if they're if they're willing to give their email address, then it's going to notify you with their email address so that you can get in touch. So that is a deployed app that you have running and that you can interact with and use it as your own personal career avatar. So I know this was a lot to take in, but I hope that you found it very satisfying. A real application that you can put to good use. The more information here about how you do that deployment with the instructions and now the exercises for you. And this is the the real end of week one exercises that you need to take seriously because these are important. First and foremost, of course, to state the obvious, you should build this yourself. It's a valuable tool. It's really cool. You should deploy it. It is the future of resumes, an interface you can chat with about yourself. And next, of course, you should improve the resources that are supplied to the chatbot. This is very vanilla, just a LinkedIn profile. You can add much more detail about your background. You can add in lots of of important stuff. And you could answer. You could give answers to all of the typical career y kinds of questions. So you can make sure that you've got a really robust knowledge base of data there. If you know how Rag works, you could implement Rag. You could put in a coma data store. If you've done my LM engineering course, then you know this stuff super well. You could build that in there, and you could have it doing something that has like a bunch of different documents that it's able to to refer to in giving good answers with relevant context. And then you should add in more tools. You could have a tool that can make a SQL query to a database to look at prior questions that can be answered, and a database where it could add in questions that require an answer. So you could have something that's quite interactive where where the LM is able to to add questions to a database and it can text you and then it'll send you a push notification, and then you can come in and add the answers, and it will then provide the answers later to the user. So you could add all of that stuff, which would be really interesting. And then the final exercise for you, of course, is that you may have noticed that whilst in the last lab we built an evaluator. I didn't actually include the evaluator here because it just felt like that was too, too much, too much for one lab. But you should now bring that back. Let's have the evaluator in there to make sure that all of the responses that people give very professional and crisp and do well. So these are all good exercises for you to get involved in. And there's also plenty of user interface stuff you could do to make this nicer. You can make that Gradio app look prettier for sure. If you could just ask ChatGPT to give you some advice on Gradio apps. And if you know how to stream back results again, if you've taken the the other course, then you should add that in of course too. And to wrap this up, I just want to talk about the commercial implications. I always do want to bring it back to that. I mean, the obvious point here is that this is commercially already useful because you can use it for yourself to attract clients and perhaps even future buses. But also you can extend this kind of thinking to any business situation where you are building AI assistants, which is one of the very most common use cases commercially for agentic AI. This ability for it to interact with the real world through tools is what really sets something from being just a vanilla chatbot on a website to being something that is actually commercially valuable. It could be something which doesn't just tell you about ticket prices to travel somewhere, but it can then actually go and interact and book the tickets. That's the kind of that's the leveling up that you get with the use of tools and using structured outputs and providing more resources in the prompt. So this is really the bigger commercial opportunity. Look for ways that you can have this kind of tool enabled AI assistant, and see how that could be applied in your day job.
------
Video 26. Week 1 - Day 5 - Foundation Week Wrap-up: Building Complete AI Agents with APIs & Tools            
------
And with that, congratulations. That is a wrap on the first week of this course Foundational Week. Think of everything that you've you've built up over the course of the week. Understanding what agents are and agentic patterns, orchestrating between multiple different APIs and building some of those patterns, the use of resources and tools, and then bringing them all together into an actual app that's useful for you right now, and maybe for some people, if you are learning tools for the first time. I probably unloaded a lot on you today, and I would just urge you to take it as something to give you some intuition for how this works. You won't have to do that again, but it's great to know how it works, and it's good to go through that code and see it step by step. For people that do know about tools and have used them before, maybe even you took the course, which means that the LM engineering course, which means that we spent a whole a week looking at tools, then this may have helped to the the elegant way that we packaged up the use of tools shows you how we can be quite flexible with this, making the analogy with structured outputs and seeing how that fits together. And this will all help you to appreciate when we go into OpenAI agents SDK, for example, and we're just calling tools, you'll know really clearly what's going on behind the scenes. And that is the perfect segue, because next week it is all about OpenAI's agents SDK. And as I'm sure I will tell you then, and I'll tell you now, that happens to be my very favorite one of the Agentic frameworks. So I'm so happy that we're getting to it right away. I can't wait for week two. It's going to be a blast. I'll see you there.
------
----------------------------------
Section 2: Week 2
Video 27. Week 2 - Day 1 - Understanding Async Python: The Foundation for OpenAI Agents SDK
------
Well. I am so excited to welcome you to week two of our time together in this journey into Agents Land. And as well, you know, week two is all about open AI agents SDK, formerly known as swarm, released very recently, the newest of our arrivals into the agents Framework land and one that I cannot wait to tell you all about. But first, isn't there always a but first but first, before we can do open AI agents SDK, I have a sidebar, a topic that I have to cover with you, a very important topic, and it's to talk to you about asynchronous Python async IO. It's something which is common across all of the agent frameworks. So all of them make use of asynchronous Python, and it's something which you can get by without really understanding. There's a couple of rules that you can you can learn and then you just follow those rules always. And it will just kind of work and that's okay. But I'm here to tell you that that's unsatisfactory and that it's much better to just take half an hour to get get to the bottom of this, to really understand it, to thrash it out until you're like, okay, I get asynchronous Python, I understand it, and if you do this, if you take the half an hour, you will thank me. It will again and again you will come across this and you'll you will be completely comfortable with it. And it's only half an hour. And so I've put a guide and the guides that will take you through this, so that you can be left with no questions in your mind at all about what it means to write asynchronous Python. And I'm also now going to cover it just very briefly at a high level. Let's talk async IO. Well, look, first of all there is like a short version, a simple version. This is what I mean when I say you can get by without really understanding it. So async IO is a way of writing Python code, which is a kind of lightweight version of multithreading. So many people from a software engineering background will be familiar with the idea of multithreading. When you write code that can run concurrently, you can have multiple threads which are each executing code together, and there's often a lot of baggage that comes with that, some sort of framework stuff that comes with it. Well, async IO, which was introduced I think, in Python 3.5, is this very lightweight way of doing it, which doesn't actually involve threads at an operating system level. And it also doesn't involve what's called multiprocessing, which is when you spawn multiple Python processes and they all run together. This is another way of doing it that is super lightweight. And because it's super lightweight, it means that you can have thousands or tens of thousands of these things all running without consuming much resources at all. So it's a simple, easy peasy, lightweight way of writing code that can run concurrently. And particularly it's good when you have code that makes use of of input output, like waiting on networks. It allows other things to be running. Whilst one bit of code is waiting on network, and when you're running LM requests, you're mostly. If you're using paid APIs like OpenAI, most of the time is being spent waiting for stuff to feed back from a model running on the cloud. So there's a lot of waiting on networks, a lot of IO bound waiting. And as a result, asynchronous code is great to use. And when you're thinking of multi-agent frameworks, when potentially you have lots of these all hitting different APIs, it makes so much sense to be using it. And that is why all of the frameworks we'll look at use async IO. Okay. So with that preamble this is the short version. The short version is async. IO is actually two things. It's a a package called async IO that you can import, and it's also some language constructs baked into Python. And those language constructs include two keywords that you see right here. And one of those keywords is the word async. And anytime that you have a function which is potentially going to be able to run in a way that allows other things to happen concurrently, you simply put the word async before it like this async def do some processing, so def do some processing becomes async def do some processing. That's how you say this is a function which can run asynchronously. And when you call that function you don't just call do some processing like you're used to. You have to put the word await before it. So you say await, do some processing and if do some processing returns a string, then you can't just say result equals do some processing. You have to say result equals await. Do some processing. But otherwise it's just the same. So that's the short version. And if you wanted to get by without really understanding it, you could just follow those rules and keep going and it might be enough. But now let me tell you the bigger story. Okay. Now we're going to go a little bit deeper and talk about the real story with async IO. So look as I say, it's a lightweight alternative to multithreading or multiprocessing. Something like multithreading is implemented at the OS level. And it supports concurrent execution of different programs where the CPU, the CPU level that's being managed to sort of switch between these two programs and treat them as if they're running at the same time. This is different in the case of async IO. So there are some some special keywords. There's this keyword async. If you define a function as async def and then the function instead of just def the function, then this thing is no longer actually called a function. It's known as a coroutine. Now most people still use the word function. You'll hear me saying function, but strictly speaking, anytime you see async def, you should say, okay, that's not a function, it is a coroutine. And that means it's something special that that Python can pause and resume. When you call a coroutine, it doesn't run unlike any other function that you call that does run. When you call a coroutine, it just returns a coroutine object, but nothing is executed to actually run that coroutine object. There are a few ways to do it, but the most common one, and the one that we will use all the time is to await it. So you say await, and then the coroutine object and that schedules it for execution. What does it mean to say it's scheduled for execution. Well, there is this piece of code written in the async IO library, which does what's known as an event loop, which means it's a kind of while loop that iterates and it takes this coroutine and it starts executing it, and it can only execute one coroutine at a time. It's not like multithreading. It's going to be executing this coroutine, except if this coroutine gets to a point when it's stopped and it's waiting for something like it's waiting for IO. Perhaps. Perhaps it's made a call to OpenAI and it's waiting for OpenAI to respond. At that point, the event loop can put that on pause and start executing a different one of the coroutines that's in its sort of list of coroutines waiting to be run. And if that hits a point when it's waiting on on IO, then the event loop can run another one or it can continue the first one. So it's a sort of manual way of handling multithreading. It's a manual approach to implementing multithreading, but only really working when something is blocked. Waiting on I o. And because of this, because it's sort of written at a code level and it's super simple and very easy to understand, it's also lightweight. You can have thousands or tens of thousands of these things. And, and so it's and it's really easy and quick to get this running. And that is why it's so popular. And that is why it's used so ubiquitously across a genetic frameworks. So I hope obviously I've only given you a bit of detail here, but I hope that gives you some perspective on what's going on. And when we look back now at at this, you can see that that whilst a simple interpretation is just to say, okay, whenever I use async I've got to use await. The deeper interpretation is to understand that when I say async def do some processing that's no longer a function, it's now a coroutine do some processing. We need to call await, do some processing in order to schedule that coroutine. And that is now blocking until that completes and the result will go into that variable result. I hope that made some sense, but if not, as I say, there's a whole guide. You should go and look at the guide now. And so just to beat this one to death, I'll give you a couple more examples. And then hopefully you've really got this. So look at this again. This is back to the async def do some processing. It's going to do some work. And then it's going to return the string done. If I just say like my coroutine equals do some processing, you might, you know, if you didn't know already about this, you might think that that will run do some processing. But of course it will not. It will return a coroutine object and that's what will go into the variable, my coroutine. In order to actually run it you have to call await. So you'd say my result equals await my coroutine. It's now going to run. And the result? The string done is going to go into my result. So that is the point. That's how it works. And of course you don't need to do that. It's simpler than that. You can just say my result equals await. Do some processing, which is what we always do. And then just to hopefully sort of connect the dots to make this really click for you. There are other constructs you don't just have to call await. And then the name of of a coroutine, because if that's all you could do, you might think, you might say to me, okay, hang on. But in that case there's no real there's nothing concurrent happening here. Every time I call await and then a coroutine, it will block until that finishes and then it will go on to the next. Well, there are a few other constructs in the async IO package that allow us to be more to do more interesting things, and this is one of them, and it's one that we will use this week in our code. And it's where you can say results, plural Results equals await. And then call this this function Asyncio dot gather. And you can then pass in multiple coroutines. And I'm sure you can imagine what happens. But the event loop is going to schedule all three. And as soon as one of them is blocking, the others will start running, or one of the others will run until it's blocking, and then the third one will run. And so all three of those coroutines will, will, will run, will execute. And the results as a list will go into the variable results, a list of each of the three results from each of those three coroutines. So in some ways it's kind of fake multithreading. It's sort of brute force multithreading. It's multithreading not at the operating system level, but multithreading implemented almost manually with this event loop, and just handling things like IO blocking and being able to schedule the next one. That's how to think about it.
------
Video 28. Week 2 - Day 1 - OpenAI Agents SDK Fundamentals: Creating, Tracing, and Running Agents
------
Okay, enough with the sidebars. It's on to the main business. It is on to introducing OpenAI agents SDK. Now let me start by by teeing this up. So this is a framework which as I've already mentioned, is super lightweight, very flexible. One of the things I love about it is that it's not what they call opinionated. Opinionated is how some frameworks are described when they come with a lot of constructs, a way of doing things that you are then sort of expected to adhere to. Now, that's not always a bad thing because that can help build particular design patterns. It can get you it can allow you to build things really, really quickly. But my personal bias is I often prefer things that are less prescriptive and which give you more flexibility to choose how you want to to work with this. And very much OpenAI agents SDK is of that ilk. It also is very good at the same time at making some of the everyday stuff, like using tools much simpler. So if you think of all of that gunk that we had to do with the JSON around tools, all of that faffing around with JSON objects. Well, OpenAI agents SDK just does all that for you. So that kind of stuff is very much it is in the way, but in a good way. It just does all of that JSON stuff and it handles the whole, if you remember the if statement and then my fancy version of it, it does all of that so that you just don't need to worry about it. And that's one way where it is good to have some abstraction, because that's the the useless boilerplate stuff that you have to repeat every single time that you want a framework to do for you. And as you can probably tell from the way I'm saying this, OpenAI agents SDK is my favorite. And yeah, I'm really looking forward to this week. And I enjoy working with OpenAI agents SDK. And you might be thinking, why? Why are you showing us your favorite one? First, why don't you want to end with your favorite one? You should work up to it. And I have two answers for you there. So first of all, I do want to point out that whilst OpenAI agents SDK is my favorite, all of the frameworks are great, and they all have pros and cons, and each of them have use cases where they are the best for that particular kind of problem. So in some ways they're all my favorites. But but uh, no, that's not true. OpenAI, you're my favorite. Uh, they're all good. And so, uh, but they also get that during the course of our journey, they're going to get more and more complex. And so it's going to make sense to do it the way we do. And our projects are going to get beefier and beefier. So that's the first reason. The second reason is that whilst this is the framework that we are starting with, we are also going to finish with it as well, because in week six, when we work with MCP, which is a protocol, not really a framework, in the same way we are going to be using OpenAI agents SDK again in a more hardcore way. So it's going to come back. It's going to make a triumphant return in week six. So you you don't need to worry. The crescendo will be there. Now, one thing that all of the frameworks have in common is that they come with some terminology, some constructs, and each one is of course slightly different. And we will be learning each of the different terminologies as we go. But of course OpenAI's terminology is pretty minimal. It's it's fairly simple stuff. There is there's three, three concepts, three terms I'm going to go through with you. The first of them is just an agent. Agents represent their sort of package around calls to llms which have a particular role, a particular purpose in this solution. Obviously handoffs this is their name for the interactions between agents. So handoff is a term that they use a fair amount and we'll see it come up in different contexts. But that's what it means. It is an interaction. And guardrails is their terminology for the kinds of checks and controls that you put around making sure that an agent is doing what you want it to do and isn't going off the rails. Guardrails is a pretty common word in normal software engineering anyway. But but it's one of the the three terms to take on board for using OpenAI agents SDK. So in order to actually run an agent, there are three steps that you have to take. And here they are. Here are the three steps. You create an instance of agent. That's going to be the thing that you're going to set. Up to be one of the roles in your solution. You use something called with trace to be. Able to keep a log of all of your interactions with that agent. This isn't required. But but. It's going to be the way we will do it. And we will always do it this way. And it's going. To allow us to look in OpenAI's monitoring tools and see everything that's going on. And then you call something called runner dot and runner dot run is the thing that will. Actually run the agent as you will see. And runner run the thing that actually runs the agent. Is of course an async function. Or I should say it's a coroutine. And so as a result, it's something that we will need to await in order to get it to actually run. So we are going to need to do something that's going to look like await runner run. And these are the three steps. And without further ado, let's go and do those three steps right now.
------
Video 29. Week 2 - Day 1 - Introduction to Agent, Runner, and Trace Classes in OpenAI Agents SDK
------
Welcome back to cursor, where we are ready to go with OpenAI agents SDK. So you'll see that we have the folders on the left here, the directories. And we are now going into number two OpenAI and into lab number one. Here it is. First look at OpenAI agents SDK. I've included a link to the docs for OpenAI. And I will say that it does almost concern me showing you these docs because they're so clearly written and so easy that I rather wonder what my job is in all this. I will say that for the next few weeks, I think I definitely have a job. It gets a lot harder, but for this one you'd be doing well just to read the docs and it'll be very clear to you. So anyway, I'll keep going regardless. So let's do some imports to get us started. So we will say from dot env import load env. We know we got to do that. And now we're going to to do some imports from OpenAI's package. And that package is called agents. And from that package we are going to import agent. We're going to import this thing called runner. And we're going to import this thing called trace. These are the three things I told you about that we were going to use today. And the OpenAI package is called agents. That's that's the one I do. I do sometimes think that it's a bit strange that they use such a generic name for it, because I imagine a lot of people use the name agents for, for their own packages. I often do, and so you have to then start messing around with, with different kinds of imports. But still they did. Its called agents. And this is, this is how we import these three key things agent runner and Trace. All right. So the usual starting point. Hopefully this is second nature to you. We need to run load dot env with override is true to bring in our environment variables they are in okay. We are now going to make an agent. We're going to create our first agent, our first agent from a framework of this course. And so we're going to have an agent and say agent equals it's going to be an instance of the class agent. So we're creating an instance of agent OpenAI's class. Now it has some parameters. We need to use some arguments. And one of them is name. We give it a name and we're going to call it the jokester. And then we give it something called instructions. I like the way this is just filled in. I'll just press tab. Put me out of my misery. So we pass in some instructions. The instructions are basically the system prompt for this call to LM. So each of these agents you can think of as being an LM with one particular system prompt designed to organize it around one particular task. And so our system prompt called the instructions. In this case you are a joke teller. And you also pass in a model which in this case is GPT four mini. And you might be thinking in your mind a very good question, which is I. Does this mean that OpenAI agents SDK always uses OpenAI's models? You might, you know, that might sound like it's a sensible conclusion, but no, definitely not. The agents SDK is very much able to work with many models, as we will later in this week. Explore using different models with the OpenAI Agent Agents SDK. It's not opinionated. You can use different models with this same framework, but by default, if you just pass in a model name, it assumes you mean OpenAI. Okay, so now we're going to execute this. And now we have our first agent. Let's just look at what it looks like. It is an agent with name jokester instructions. You're a joke teller. And it's got some things here. Hand offs. You remember that was the concept that relates to the interactions between agents. It's got a model and it's got a bunch of settings and other things that you can, you can look at in your own time to be worth looking at. It has guardrails, input guardrails and output guardrails that we will look at later. Okay. So I mentioned that runner run is the way to run one of these things. So we can say perhaps results equals runner dot run. And when you run an agent you pass in two things. You pass in the agent itself to be run by the runner. And you pass in what is essentially the user prompt. It is the message that you are sending the agent, let's say tell a joke about autonomous agents. Autonomous AI agents. Okay, so now we are going to run this. It seems pretty simple. What do you think is going to happen? What's going to be in result? I will print result. Let's see what we get. If we do this and we print result. Let you have any ideas. Here it is. We get a code routine object because Runner Run is an async method, which means an async function, which means that it returns a Co-routine object. It's not it's not a function, it's a coroutine. And so hopefully you're expecting that. Or you guessed the what that. What does that mean? That means that we can't just call, run and run and expect that to work. We have to call await. And I'm going to I'm going to restart this kernel and start again from the top so that we don't have one hanging out there. Not not scheduled. We will come through. We will do this. We will create our agent. And there it is. And now we're going to say await. We're going to say results equals await. Runner dot run like that. And so now what we know is that runner dot run, that coroutine is going to be queued by the event loop. And when we call await it's going to execute it. And it will hold until it's completed execution. And the results will go into the variable result. And then we are going to print results dot final output. And this is now hopefully going to work and tell us a joke. See what happens. Why don't autonomous agents ever get lost. Because they are always following their own self driving instructions. Okay, so OpenAI is rather better at dealing with lightweight agents frameworks than it is at telling jokes. But but we'll accept it so it works. Now you can be forgiven for saying, all right, we haven't done anything particularly special here to do with agents. All we've done is have a system prompt and a user prompt. But never fear, there'll be plenty of time for that later in the week. What we're doing right now is just showing you the basic construct. And for my final part of this, I'm going to wrap this in a trace. So for trace you wrap this in a context manager. So I say with trace telling a joke. And then I just need to put that like that. If you're not familiar with context managers then then look in the guides. But but I imagine that you are. And what this does is it's telling OpenAI that we would like it to record these agent interactions, which is only one, and record it under the kind of heading telling a joke. And this allows you to have sophisticated agent interactions with many different agent calls involved, and have it all packaged up and available in their monitoring tools under one kind of heading. So let's run this. We'll get another joke. Why did the autonomous agent break up with its human partner? It just couldn't handle the emotional bandwidth. Okay. I guess it's not got a whole lot to do with agents, except it's. Put it in there with the human partner. That's all right. I'll let you be the judge of it. But now we can go and look at this trace. If we go to platform two. Here it comes. Traces. It's actually you can just go to platform openai.com and then just click on traces in the left, and you'll see that telling a joke is the top trace up here. And if I go into this trace we will see. What happened is that there was one endpoint called the system instructions was you were a joke teller. That's the system message that was just called instructions. The user prompt was tell a joke about autonomous AI agents. And then this was what the assistant responded. So true to its word, it's giving us a clear trace of the single call to an LLM that happened as part of this gigantic workflow, but it's packaged it up under the heading Telling a Joke. And this has allowed us to come into traces and see this. And as you can imagine, it's not particularly profound with this very simple example. But there will come cases in the future when this is going to be invaluable.
------
Video 30. Week 2 - Day 1 - Vibe Coding: 5 Essential Tips for Efficient Code Generation with LLMs
------
So that concludes our very first foray into OpenAI agents SDK. But before we wrap up week two, day one, I did want to have a second sidebar with you. And this time it's on the entertaining topic of vibe coding, which is a term coined by the the legendary Andrej Karpathy, who described this in I think it was an ex post that went viral about the way that that he was enjoying coding with Llms and getting so much done in a way that you would sort of let the LLM generate some code and sort of go with it, tweak it a bit, generate some more, and just make so much progress in this sort of mode of working this, this ad hoc vibe coding way of navigating around things like new frameworks. So I think this is wonderful and I strongly encourage vibe coding, and I imagine that most of you are very good at it. I did want to give a few tips that I think are important to do it well, because I think it's easy to do vibe coding and to get led astray by LMS and get yourself in trouble and get stuck, which is very unpleasant. So I have five tips to leave you with. But before we get into more detail with OpenAI agents SDK and here they are. First of all, good vibes. It's important to spend time getting your prompt to the LLM to be really good that you can reuse. Lots of times you should ask for short answers. Llms tend to be quite verbose in the way that in their code that they generate, they like packaging everything with lots of exception handlers, and they tend to do things in quite long winded way. Try and ask it to come up with concise, clean code and also mention today's date and say, please make sure that you use APIs that are current as of this date. Otherwise, Llms has a nasty tendency to use older APIs because that was in a lot of their training data, so explicitly prompt for that vibe, but verify. My second tip don't just ask an LLM a question and go with it. Ask a couple of llms. So I often ask the same question to ChatGPT and to Claude and have them both up. I asked the question because I'll learn from both of the answers. Often one of them is too long winded or is missing the point a bit, and one of them will be spot on. And so asking a couple or maybe even three so that you're verifying what answers you're getting is a really good technique. Step up the vibe. So this is saying, I think this is such a great one. This is because I sometimes get students sending me problems. They're saying, I'm stuck with this and they'll send me some code and it will be like 200 lines of code and they'll say, it's not working. I don't know why. And when I look through the code, it's immediately obvious to me that this is they've been coding. You can tell there are telltale signs that a lot of this was generated by an LLM, and it's it's unwieldy, and I can sometimes see a bunch of different bugs in it. And I come back and I say, it's no good. It's no good generating 200 lines of code and then saying it's broken. That's not the way to do it. Rather, you should always try and get llms to do things a little piece at a time. Generate function by function. Generate small pieces that are independently testable, like ten lines of code at a time. So you should think of dividing your problem down into small bite sized chunks and have an LLM do each piece of it. And here's the trick if you can't think about how you would divide your problem down into ten smaller steps or whatever, you don't need to because you can ask an LLM to do that. But be very clear, you don't want it to generate code. You say, look, I'm trying to solve the following problem and you tell it and say, what I want you to do is tell me what would be the 4 or 5 steps in a solution where each step is simple and simple, bite sized chunk that could be tested and verified independently and get it to come up with those, those chunks. And then you can ask again llms to generate code for each one in turn, along with the way to test it, so that you can make sure that ten lines of code at a time. You have a working solution, and you've convinced yourself that when you've got each of these, you'll be able to put it together and have 200 perfect lines of code. Okay. And then the fourth one, vibe and validate. Similar to vibe, but verify. But Vibe and validate is saying you can ask a question to an LLM. And then when it gives you the answer, you can go to another LLM or even the same one, but better to be another one and say, I asked this question, I've got this answer. Please confirm that that is an appropriate answer and that it's you can't do anything that's more concise or better structured or clearer, or there are no bugs in this. And by having another LLM go through and validate, it will often detect problems or make it be cleaner and simpler. So it's just a nice trick. And in many ways this this mirrors this echoes the common design pattern of the evaluator optimizer. But you can just do that manually yourself. So taking a leaf out of a genetic design patterns and using it as part of coding. And then my final one vibe with the variety, which is don't just say can you generate code for this? But particularly if you're only talking about ten lines of code, say, can you give me three different solutions to this three, approach it three different ways, and maybe the response will be that there aren't three different ways of doing it. There's only one clear way, but you might get three different solutions. And and that will force the model into a mode where it's contemplating different ways of approaching the same problem. And as a result, you might get better solutions. As part of that, you can also ask it to explain itself, explain the rationale so that you'll get something that will that will also force it to think through why it's approaching it differently. Plus, as an added benefit, it will explain to you what's going on so that you can really understand this. And that does sort of lead to an obvious point, which I've not put here, which is that when you do vibe coding, you should always then go back and ask an LLM to explain it clearly. If it's not immediately clear to you, get to a point where you understand every single thing that's going on. Vibe coding is super fun and productive and powerful, but if you're not following what's actually happening, it's going to become painful and frustrating when something goes wrong. So it is important to stay in touch with what's going on. All right. And that is my survival guide to vibe coding and I hope you'll find that helpful. But other than that you should definitely do it. It's it's great fun and it's super productive.
------
Video 31. Week 2 - Day 1 - OpenAI Agents SDK: Understanding Core Concepts for AI Development
------
And that's a wrap on week two. Day one understanding OpenAI agents SDK concepts. Not that there was all that much to understand. And tomorrow in day two, we're going to build our first project, an SDR, a sales development rep, and it's going to be great. I will see you there.
------
Video 32. Week 2 - Day 2 - Build AI Sales Agents with SendGrid: Tools & Collaboration in Agent SDK
------
So look, you're here because you like getting hands on and building stuff. And that's why I'm here, too. And I got good news for both of us. That's what we're doing right now. Today we're going to be building our first OpenAI agents SDK project, building a sales development rep. And we're going to be building really three different pieces. We're going to be building three different layers of Agentic architecture. What we do this, we're going to be starting with something quite simple, a workflow of agent calls. We're then going to spice it up by adding an agent that can use a tool. Remembering back to when we did this, the sort of manual way with the the JSON with the boilerplate in week one. And then we're going to have agents that can call on other agents, and you're going to discover there are two different ways of doing this. You can treat agents as tools, or you can use the thing called handoffs that I mentioned before. So keep this in mind. Keep this construct in mind. This is how we'll be approaching it. There's a lot of coding to do. Let's get started. And so here we are back in cursor ready for the day to begin, we open the open AI folder. We go to lab two. Here we are at the start of week two. Lab two, day two. We're going to do an agent workflow. We're going to use tools to call functions. And then we're going to collaborate with tools and handoffs. So before we start we're going to be making use of a tool. It's called SendGrid. And it will allow you to send transactional emails from their servers. It is free. So it's going to be very easy to set up. It's actually owned by Twilio. So it's a very well known company. It's similar to something like Sparkpost if you've used that before. And of course you can use Sparkpost instead if you'd prefer. So there's a link here. If you visit SendGrid right there up, it will come and you can start for free by pressing this button right here. And once you do that and you've set up an account, I will switch to another window. Here we go. This is what I get once I've signed in. You can see that I've sent 1 or 2 emails, and there's just a few things for you to do. Once you've done this, you go to the settings menu down here and to the API keys section. That is where you will set up an API key and copy it to your clipboard. You can guess where that's going to go in just a second. The other thing to do is to go to this tab here Sender Authentication. And you're going to click this verify a single sender button. And what that's going to do is it's going to allow you to set up your your own email address. This is my email address right here. And do like a verify email to confirm that you do own that email. And once you've done that, you'll be able to send emails from that email address. So that's a nice thing to do, just so that you'll be able to send emails via SendGrid from that email address. But don't worry, we're not actually going to be sending any cold emails out to the world. It will only be emails going back to you again, this would just be experimental, but you can always use it for more purposes in the future. So that is SendGrid. Please go ahead and get that set up. And shockingly, I'm sure you expected this. Once you've done that, you'll then edit your env file over there and you will add in a single row SendGrid API key equals. And of course you will paste in there the thing that you've copied from the API keys section. You created a key there in SendGrid itself. All right. So go ahead and do that and then we will proceed. Okay. And so now we have some imports. We're going to be doing some usual stuff. We've got a few more from agents. We're also importing this thing called function tool. We're importing something to help us stream back text. And we're importing some SendGrid stuff. We're going to load our environment variables as usual. And now let's do a simple workflow between agents without anything fancy. Here we're just going to have some agents calling other agents, and we're going to orchestrate it manually ourselves in Python code. And it's going to be super simple. So first of all three instructions. And these instructions again think of it in the back of your mind. These are basically system prompts. But they are the way that we are framing different agents. All three are about being sales agents working for a company that I'm calling. Comply. Comply. I. I. The wonderful naming. This is a soc2 compliance company that is looking at helping companies prepare for SOC two audits. So something to do with compliance if you're not familiar with this stuff. But, uh, yeah, it's a SaaS tool that will ensure SOC two compliance. So it's saying you're a sales agent working for this company. You write professional, serious cold emails. It's like a cold sales email or email to to get people interested. The second one is a humorous, engaging sales agent. You write witty, engaging, cold emails that are likely to get a response. And the third one is you're a busy sales agent and you write concise to the point cold emails. So, as you know from good prompting, you use a system prompt instructions to set the context, to give the tone to sort of set. Set the mood and the character and give as much background information as you can in a succinct, instructive way. Okay, so now we create three agents sales agent one, two and three. And this should be very simple. It should make complete sense to you. Each agent gets given a different name. They have the three instructions the three system prompts. And we're going to be using GPT four mini throughout. Keep it cheap. We've just created those three agents all right. And now we're just going to start with the first one. And I just wanted to show you a different way. You remember we had runner run before. Well now I'm calling runner run streamed which is a way that you can do it. That will allow us to stream back results. You may be familiar with streaming from from simpler OpenAI examples, but this is how you can do it with the agents SDK. So you may spot that there's no ace a wait word there which which may set off alarm bells, because that means that what we're getting here is not a response. We're getting a co-routine and you can see what happens next. We use a special construct async for a special way of of then calling this, calling this in a way that will return a coroutine, and we will then iterate through those answers. And this is just a little bit of boilerplate code to make sure that what's coming back is some text that we can print. And if we get that, then we print it. And this just stops it printing on a separate line every time. So if I run this, it's just very similar to what we did last time. But you'll see that what comes back streams back bit by bit. And it's a very nice use of the streaming APIs and I show it to you. So you have an example of how to how to do this. And of course, just as you would expect, we see a great behavior from our agent. This is the professional one, I think, and we can see that it's come back with something nice and professional, nice and slick. To simplify your soc2 compliance efforts. And it's no surprise to to everyone that's worked with Llms for a bit that they are really good at this kind of thing. They can write great professional, realistic, reasonable sales outreach emails. So far, so good.
------
Video 33. Week 2 - Day 2 - Concurrent LLM Calls: Implementing Asyncio for Parallel Agent Execution
------
Pressing on. So what we're now going to do is call this. It looks like I'm going to need to import async IO there. Hang on. Let's do that. Missed an import at the start. Actually let's do this properly. I'm going to take this out just so that it's nice and clean for you. I'm going to put that async import right up at the top there and rerun that cell. I like to do my imports at the top. I know people have different views on this stuff. Okay, so we are now going to say we want to write a cold sales email. That's going to be our instruction. And what we're going to do now is we're going to call this async IO. Do you remember this? I showed this to you back when we had the async briefing, that this is where you start to see the power of async IO. We're going to be running three different runs at the same time. And of course again it's not multithreading. It's not like the CPU is actually time slicing between them in some ways. But but rather it's going to be using an event loop, which will run each one, and whenever it's pausing waiting on I o it will let another one run. And because most of what these things are doing is pausing on I o that will allow all three of them to sort of alternate and be talking to to an LLM to open AI in parallel, and then we will collect back the results. What comes back from this await gather is a collection of results. And we will call final output on each one and print it. So we will let that run. And you can see that I've wrapped all of this in a trace, which means that we should see all of this in one trace. We will do in a little bit, but no surprise. Hopefully what you'll see down here is I have to click here to get a scrollable element, and you'll see that we can scroll through and read a bunch of emails. And the third one, of course, is very concise because we asked our third email writer to be concise. Okay, so so far all we've really done is multiple calls to LMS. And now the final piece of the puzzle. This is just a simple workflow of calling LMS. See if you can remember which one of the patterns this is, but it's a sort of variety of one of them. And we are going to create a new agent called sales picker. And sales picker picks the best cold sales email from the given options. Imagine you're a customer. Pick the one you're most likely to respond to. Don't give an explanation. Reply with the email you select only. It's a classic kind of prompting. You know that you often end with the constraint. With any limitation is a good way to end a system prompt. All right. So that's the message the user prompt writer called sales email. We again we have this same construct. We're going to run three agents in parallel to craft three messages. We're going to gather the outputs and then we just simply call another agent. So this is just Python code step by step. Call three agents in parallel. Take the output. Call another agent. It's just a Python script that runs these things in order, and it will then print the best email at the end of it. And because we've got the whole thing under one trace, we hope to see all of this in a single trace. So this is probably going to take I think like 30s in total. So I'll have to keep the sentence running if we want to see this live. And at the end of it we'll be able to go into the trace is run. So here is a single email that's been returned. And it is apparently the best one. But let's now go and have a look at the trace to see if we believe it. Okay. So this was our email. Now this is a link to go to see the traces in OpenAI. Here it comes. So at the top trace is selection from salespeople. Let's go and have a look into this trace. What you'll see is that, sure enough, it called the professional sales agent, and you can read the actual prompts that went in it called the engaging sales agent. It called the Busy sales agent, and then it called the sales picker. And you can go through this in the trace and see what's going on. So it's worth pointing out that we've really not done anything very magical so far. It's nice. It's elegant that we've had these wrappers, these agent constructs around things, but they're very lightweight wrappers. We've essentially simply called a total of four LMS. We've called the three sales LMS, and then we've called the one at the end to pick the email it preferred. And it's sort of elegant that you can go into the traces and see what's going on. But now it's time to get a bit more agentic and start talking about tools. So as we get into tools, let me remind you, have a think back to what we did in week one, when we had to build some boilerplate JSON to describe a function that we wanted to use as like a tool. It's quite chunky JSON that described the parameters and various other things about it. And then we had to write a function called handle tool calls, which is the one that had the the hokey if statement that I then tried to write into something a bit more fancy, but which was still a fair amount of sort of gunk, to use a technical word. And what we're now going to see is how that whole stuff can be, can be super simplified without taking any flexibility away from us. So first of all, we're just going to recap. We've got these three sales agents the professional, the engaging and the busy ones. And there they are. And let me just look at one of these sales agents. So if I look into it you can see that it's got a name. It's got the instructions. And you can see that it has things like tool choice tools, and it has like a list of tools that it has access to, which right now is empty. So just to give you a sense of what's going on under the covers, it's perfectly simple. But these are the constructs we're working with. So what we're now going to do is have a tool. So this is a function send email. It takes an email body which is a string. And it has a like a doc string. Send out an email with the given body to all sales prospects. Now what I've got in here is the from email. And this email needs to be what's called the verified sender, the email address that you've set up in SendGrid that you're allowed to send from. Otherwise it won't actually send. So you'll need to update this and then two email. You should put that to be an email where you don't mind it receiving an email. I would prefer that you change it from my Gmail here. Otherwise I will get flooded with emails from people doing this. So maybe I'll change it before I commit this to GitHub. But yes, send that to another of your email addresses. Or you could probably just send it to yourself. I'm not sure, but send it to somewhere where you don't mind receiving cold sales emails, there'll only be 1 or 2. We're not going to send a mass of them. And then this is Sendgrid's API. It's perfectly simple. It's SendGrid. And you give it a body so you can look through this. But it's honestly it's just a boilerplate to send an email to a recipient and we respond with success. So this is very much like the tools that we wrote last time. It's like a function to do it. And if you remember, the next step was to write a ton of JSON to describe this. But all we need to do with OpenAI is put one of these decorators above it at function tool. If you're not sure how decorators work, again, that's in the guides, or just ask ChatGPT to explain it. But that allows OpenAI to write some code to sort of do something with this function. And but all we need to care about is we simply put that decorator above the function that we want to treat as a tool. And so we will just simply define that function doesn't look like we've done anything, but let's look at this thing called send email. Now we think it's a function, right? But no, it's not a function because we have this decorator. It's been converted into something called a function tool. It has a name send email. It has a description. And that description is taken from the docstring comment we had right here. Look at that. It's just turned that into a tool description. And then and this is the great stuff params JSON schema. This stuff is all of the boilerplate JSON that we had to write last time. It's just done it for us because it's read this and it's figured out what's needed. And and that's basically it. So just by simply adding that decorator function tool, all of the work of faffing around with these JSON objects has been taken away from us, which is great. That's what a good framework should do. Take out the boilerplate but leave us in full control.
------
Video 34. Week 2 - Day 2 - Converting Agents into Tools: Building Hierarchical AI Systems
------
Okay, now there's something a bit confusing now, which I'll have to get your head around, which is that you can, as we've just done, turn a function into a tool. There's something else you can turn into a tool as well. You can turn a whole agent into a tool. You can say, we've got this sales agent, something that can write a sales email. Well, that whole thing, that whole process of calling that LM with that prompt, that can just be considered itself to be a tool. We can think of it as a sales agent tool. And doing that might sound like there's a fair amount of work to be done to package an LM call into a tool. But no, all you have to do is call as tool on the agent, and you will convert the agent into a tool. What does it mean to convert an agent into a tool? All it means is it's going to create a new tool. It's going to have all of the JSON gunk that describes what that tool can do. And if that tool is called it's going to actually call the agent and make the agent make the call to the LM. So it's a wrapper. A wrapper around the agent that turns the agent into a tool. That's all it is. So let's look at this sales agent one as tool. We're going to give it a tool name sales agent one still and write a cold sales email is our description of what that tool can do. And now if we look at tool one you can see it's a function tool just like this one. Just like send email. It's got a different name. Its description is this description here. Now it's got the same parameters, the JSON blob that we would expect. And it's got a special function to tell it what to do when the tool is actually called. And that of course is actually going to call our agent. So hopefully that makes sense. We're now going to package our agents into tools and use them. All right here we go. So this I could have done this with a fancy loop, but I thought, you know what? I'll just spell it out so that you're crystal clear on what's happening here. We're going to have three tools. Tool one, tool two, and tool three. And it is just taking the three agents. Sales agent one, two and three. And calling as tool on each of them. And for each one we just give it a tool name and a tool description which is the same tool description. That is it. Now if you if you feel up to it, just rewrite that as a nice little loop so that we don't have a silliness with repetition like this. But but I wanted to spell it out. And now I'm giving myself a list called tools and tools has tool one, tool two, tool three. And these are like the agents wrapped as tools. They're tools that would simply call the agents and then send email is the real tool that is just a function. Send email that calls SendGrid the email program. And let's have a look at what we get if we run this. So now this is our list of tools. And it is indeed four function tools sales agent one, sales agent two, salesperson three and send email all in a list of tools. Okay, now it's time to put all of that into a sales manager. Our planning agent. So now it's a bit like we did before, except we're giving an agent the ability to choose what to run when. It's not like we're just writing Python code to say, do this, this, this, and then send email. But rather, we're letting the agent decide. You are a sales manager working for comply. You use the tools given to you to generate cold sales emails. You never generate sales emails yourself. You always use the tools. You try all three tools once before choosing the best. You pick the single best email and use the Send Email tool to send the best email and only the best email to the user. Now, I'm probably over spelling it out here. You can experiment. It's not always required to be so pedantic, but I'm also doing it to show you what's going on. And it's never a bad thing to be very precise and instructive with your prompts. Okay, so with that, we want to create a new sales manager agent. There it is. We want to give it these instructions. It's system prompt. We want to pass in all four tools, all these tools right here. And we give it a model. And then we say send a cold sales email addressed to dear CEO. And we start this going and off it runs. And this will take about 30s. So I will see you in a sec. Well actually it didn't take 30s. It took 18 seconds. So it's quite quick and it finished and we can first let's look look in my email I did in fact just receive an email and here it is. And very good. It does indeed do things quite nicely. You'll see it's put like like a template CEO's name not dear CEO you can improve the prompting to stop it doing that if you wish, but there's an exercise at the end to incorporate mail merge. So it actually would put in real people's names here. But but very nice. We got the email, it came through to me and it came from the verified email sender as expected. The other thing to do, of course, is to go and have a look at the trace. Let's go and do that now. So here it is. Sales manager is what we called it for. Tools were used. It took 18 seconds. Let's go in and have a look. So it called Sales Agent one. It called sales agent two. Sales agent three. And then it called Send Email. That's the the the tool that is just a tool. If you look at each of these sales agents tools representing by this this sort of green thing, you'll see that underneath that was an agent. So this hopefully makes it crystal clear for you that we had an agent, the professional sales agent that was wrapped in the tool, sales agent one. And you can see that again for this one and for this one. But in the case of send email, it was simply a function that was called with a body. But you can look through the trace and you should look through the trace and understand the interactions between tools and agents to allow the sales manager to carry out its full activity, allowing itself to make its decisions about what it does, in what order.
------
Video 35. Week 2 - Day 2 - Agent Control Flow: When to Use Handoffs vs. Agents as Tools
------
Okay, now to recap. What have we done? We first just used agents to write Python code so that agents could run. We sort of sneakily used the async gather so that we could run multiple in parallel. And then we used another agent afterwards to pick the best email. That was simple. We then use tools to wrap a function so that we could have an agent that calls a function. And we also used the as tool construct so that we could wrap other agents to be a tool. And we provided all of those tools to our sales manager agent. And we let it call three different email producers and then finally send the email at the end. So that's that's hopefully connecting for you, making a lot of sense. There is one more construct that is in some ways really similar. So it's a bit confusing, but there is a distinction and it's called a handoff. A hand-off is something that you can give an agent. An agent has a number of tools, and a number of handoffs, and handoffs are other agents that it can delegate to. So in many ways, that sounds really similar to taking an agent and wrapping it as a tool. An agent as tool is very similar to a handoff. There's a kind of conceptual difference and and a very practical difference. The conceptual difference is that you can you can sort of think of it like there's an agent which either has just the ability to use tools as part of doing its job, or handoff is when it's delegating. It's sort of giving giving responsibility and ownership of a specialist task to another agent. So sort of philosophically, it's a bit of a different mindset. It's not just just using this as a little feature, a little tool to help with its job. It's it's passing an entire job to another agent. But there's a more sort of fundamental, simple technical difference between them, which is that when you're using tools, you can think of that more as a request response. You're calling the tool and control passes back to you, and you continue as the main agent executing in the case of handoffs. You've done your piece, and you are now passing control to the other agent, and the flow does not come back to you again. It's just a passing of control, delegation of control to a different agent. So that's the difference between agents as tools and handoffs. And it's uh, you can you can use either in different situations, but but they are slightly different constructs. So I explain that a little bit here. And now we're going to make a new agent that eventually is going to be a handoff. So here's the deal. We are going to start by saying that, uh, you are I want a subject writer agent, something that is able to write, construct a subject for a given email body because I don't know if you noticed, but we didn't have subjects on those emails. So we're saying you can write subject for email. When you're given a message. You need to write a subject for an email that's likely to get a response. So that's one instructions. Another instructions is going to be that you can convert a text email to an HTML email, a formatted email, which is how most emails get sent these days, particularly the fancy sales emails. So you will be given a text email. Body might have some markdown because llms often put markdown in these things, and you need to convert it to an HTML email with simple, clear, compelling layout and design. So we're then going to make an agent for the subject writer with those instructions. And we're going to use the as tool approach. This time we're going to make this a tool because it sounds like a tool right. It's just a tool to write a subject. So we're going to have a tool. and that tool is going to be wrapped around this agent. Similarly for the HTML converter, we're going to say, okay, so you're something which is able to write HTML emails. We're going to make you a tool. We're going to turn you into a tool, an HTML tool that's able to do that. All right. Let's oops what have I done. I'm in cursor land. Undo that. All right. Run that. Great. Okay. And then we're going to have one more tool which is rather similar to to the prior send email. But I've made this a send HTML email. And it takes a subject and a body. And it sends out an email with the given subject and body to all sales prospects. Again, remember to change this to your verified email sender. Make this some other email than mine please, and then send using SendGrid. And you can see there's just a tiny change here that it sends it as an HTML email, not a text email. Okay. So then we now have three tools subject HTML tool, and a subject tool that creates a subject, an HTML tool that converts a text email to HTML and the send tool. So two of these are agents. And one of them let's print them. Let's have a look. Tools. Two of them. They're all function tools. Two of them are actual agents wrapped. And one of them is just a function. Okay, we're almost there. Almost there. So now now we're going to create a separate agent. And this is going to be the agent that we're going to want to hand off to. If you're wondering what I'm where this is all heading. So this agent says you are an email formatter and sender. You receive the body of an email. You first use the subject writer tool, then the HTML converter tool. And finally you send the email. So we call this emailer agent. It has a name, it has instructions. It has tools. It has something new in here. Hand off description. Convert an email to HTML and send it. Now this. This is how this agent will sort of announce itself to the world in case another agent wants to use it. If there's another agent that might want to to do something, this is how it knows whether or not the emailer agent is an agent that might be useful. So that's a sort of framing. It's very similar to the to the tool description. It's a description of what this agent does. Okay. So if you've been following me you will now be clear that we have three tools. These are the three sales agents. And we have a hand off. We have this thing this this agent which we can hand over control to. two. And somewhat confusingly, this hand-off itself has three tools. Subject writer, HTML converter, and a send HTML email. Are you following this? If not, come back, go through it. Or of course, print. It's always good to print. We should print tools, and we should print handoffs and take a look at what we've got. So this is the tools are just the sales email sales agent one, two and three. And the handoffs is a single agent called the email manager. And this agent we should see itself will have a bunch of tools. It has a handoff description. It doesn't have any handoffs itself. But somewhere in here we should find that it has a few tools. Here they are tools function tool names, subject writer and so on. All right. If any of that's not clear, come back, look through this, print it, get a sense of it and then we will actually run this.
------
Video 36. Week 2 - Day 2 - From Function Calls to Agent Autonomy: Sales Automation with OpenAI SDK
------
Okay. Moment of truth. I'm going to run it. And while it's running, I will talk through what it's doing. Hang on. Off it goes. So we have a new sales manager. We say you are the sales manager working for comply. You use the tools given to you to generate cold sales emails. Never generate one yourself. Always use the tools. You try all three sales email tools at least once before choosing the best one. You can use all the tools multiple times. If you're not satisfied with the results from the first try, you select the single best email using your own judgment of which email will be most effective, and then after picking it, you hand off to the email manager agent to format and send the email. Now again, I'm really spelling it out here, and you shouldn't need to be that prescriptive because the hand off description was there. But it helps to be to be precise. And particularly as these things get more complex, if you want it to stay on track and to be reliable, then this is a good practice to do. So then we create the agent. We it's a sales manager. We give it these instructions. We pass in both the tools and the handoffs. This is the crux of this. This is how we separate out that the tools, which is where we it's like a request response from the handoffs, which is a delegation control is passed across to take care of the rest of the work. And that is our message. And we put a trace around it, automated SDR and we give it a run. And it took about a minute and let's see what came back. This is the result and it's picked a very short one in the end. You remember one of them is a very concise email writer. It does indeed begin, dear CEO. And it ends, Alice, as we asked for. And it's given a nice, short, sharp, AI driven solutions that can optimize your business processes. And so it's nice and crisp. There we go. And importantly, we now have to go and look at the trace. So the trace here it is automated SDR. You can see how long it took. It used nine tools. Let's go and have a look. So it came in and it used sales agent one and two and three. And then look at what happened. It went back and used sales one, two and three a second time. Load more. And then this was the handoff you can see here shown. Then it goes to the email manager. And then that goes to the subject writer followed by the converter followed by the send email. And you'll see the control does not then come back. So you're seeing everything in action here. The sales manager that's able to to handle things. And it hands off here. And the email manager takes the rest of control for the rest of this blue timeline that you see highlighted in here. You can see the multiple calls to different agents and the handoff and the use of tools, both wrapping agents and just directly calling functions. And that is a satisfying conclusion to quite an interesting example of a genetic workflows and design patterns. Okay. Well, we covered a few of the constructs and concepts with OpenAI agents SDK, including tools and hand offs. And the most important thing that I want you to take on board is that it was simple. It's relatively lightweight. We we achieved something quite complex, quite advanced, and we did it all in a matter of a few minutes. Now, some exercises for you as of course, very important that you now come back and spend some time on this yourself. That is the best way to learn. So first of all, go through and identify the agentic design patterns that we used here. Now, there was a moment in this when we moved from from doing what anthropic would just describe as agent workflows to something that was really like agentic agents under their definition, although I think it's a little bit, a little bit hand-wavy. But still, there was a clear moment. Can you identify it? Can you spot what was the change? The small change that made that difference? And then try adding in more tools and more agents. This is a great way. This is so easy to extend. This. Obviously this is calling this an SDR was was maybe a bit of a stretch because it's really just a cold email writer, but it would be very easy to see how you could build this into something that's much more interactive and then a hard challenge for you, particularly if you've got some engineering experience. Figure out if you could turn this into something which is more of a longer living agent workflow, in that someone could reply to that email, and the agent would then be able to pick up and continue the conversation. Have an email based conversation about the company. Now the hard part about that is handling replies back from SendGrid that we'll need to do a little bit of research and understanding about things like webhooks and how that will work, and how you'd use that to trigger this processing, and how would you identify who you sent it to? So there's a fair amount of stuff in there, but it's engineering stuff rather than much genetic stuff. So. So you could be forgiven for skipping it if it doesn't appeal to you. But if it does, then, then that's a very interesting framework to put in place. And in terms of commercial applications, because you remember, I do always like to try and bring this back to how can you apply this to business? And hopefully I mean, obviously there's the fact that it could be literally a sales automation tool that could just be working to generate a bunch of different cold sales emails or email campaigns. And if you were to do this extra hard project, it could in fact be something that engages and continues the conversation. And generally, the point here is that you can really apply this to any end to end automation of these kinds of business processes. You can think of how you can apply this in this context of sales, but you can think of applying it to so many contexts. Imagine my day job at Nebula that involves recruitment. Imagine how applicable something like this is to that space as well, and many others. Think of a business area. Think of the kinds of activities that are done at scale with things like cold sales, outreach, and picture how this kind of approach agents collaborating with a level of autonomy and with use of tools and handoffs, can automate a complex business process.
------
Video 37. Week 2 - Day 2 - Agentic AI for Business: Creating Interactive Sales Outreach Tools
------
And if you're able to build that more interactive challenge of a sales outreach agent that can take responses, I would love to see it. There's a community contributions folder in week two. Be sure to put your code in there. Submit a PR there are instructions on the class resources and use that as a way to share what you're doing with me and with other students. And remember as well the great trick of posting on LinkedIn. Tag me and then I can weigh in and and give give some thoughts on it. And that helps amplify the work you're doing. And make sure that people get to see all of the ways that you are applying agentic AI to business problems. All right. So congrats on getting through a fun project and learning about OpenAI agents SDK next time. I hate to beat this one to death, but I want to go through one more time about the sort of tools and agents discussion, and I want to talk about guardrails, which are super important way of putting controls around what you're doing. And then we'll be embarking on the larger project. See you next time.
------
Video 38. Week 2 - Day 3 - Multi-Model Integration: Using Gemini, DeepSeek & Grok with OpenAI Agents
------
And welcome to week two, day three. We have lots to cover today. Continuing our exploration of OpenAI agents SDK and with our SDR project. So I want to quickly take a moment to recap what we did last time. We looked at tools by just using that simple decorator as as a way to allow us to wrap a function into a tool and to be able to call that tool so easily without the boilerplate JSON that we'd needed to use in week one. We then looked at the way that agents can also be turned into tools. By using the as tool function, we can simply change an agent into being a tool. And that turned out to be very easy indeed. And then slightly confusingly, we saw that there is another way that agents can collaborate with each other, which is using handoffs. When you're creating a new agent, you can give it any number of tools and you can give it handoffs. And I explained that the key difference between them is that when you're using tools, you can think of that as a call to the other agent, which will then return back. But when you're doing handoffs, you can really think of that as a transfer of control. The next step on the workflow is giving over control to that agent that is on the receiving end of the handoff. We're now going to go and extend what we did last time in three different ways. Three things that will allow us just to learn a bit more about the framework. First of all, we're going to do what I promised we'd do and look at models other than OpenAI. So using OpenAI agents SDK to drive things like Gemini and Deep Seek. We're also going to look at structured outputs. The way that we can require an agent not to respond just with text, but to populate some kind of an object where we can specify the fields that are going to get populated. And then we're going to look importantly at guardrails, which are an approach to making sure that we have some controls over the information that comes in to our agent setup and what comes out. All right. With that intro, let's go back to cursor, back to the lab. And here we are back in cursor. And we are looking at the second week. And we're looking at day three. Here we are lab three week two day three. And we're going to start by doing a sort of quick recap of what we did last time. We're going to do some imports. We're going to load the env. We're actually going to bring in a bunch more environment variables than usual. I'm looking at a bunch of different keys here. Google deep sea grok. Now you don't have to do this. You can just stick with OpenAI if you don't have these other keys. But if you'd like to experiment with others, then this is the way to do it. And of course, if you have other things, perhaps you use Open Router as your way to connect to different models. You can be using any of these here. All right. Now we have the three instructions for our three different sales agents. You'll remember one is more professional, one is more witty and engaging, and one is more concise. So income are three instructions. We're now going to remember that we can use OpenAI's endpoints. We can use that compatible OpenAI endpoints to talk to other models like Gemini, Deep Seek and Grok. And any time that we have this approach, we will be able to easily use the OpenAI agents SDK. So here we're setting the base URL for Gemini endpoint, the base URL for the deep seek endpoint, and for the grok. These are the OpenAI compatible endpoints. So for Gemini you can see it's got OpenAI in there. And the same for grok for deep sea kit is the only one. It's already OpenAI compatible. Then we create a new instance of the client by instantiating an async OpenAI object. As you can see here, passing in the base URL and passing in the key. And finally we create three model objects. These are called OpenAI chat completions model passing in the name of the model and the client that we just defined in the group above. So this is a little bit boilerplate. And if you're using GPT four or mini, if you're using any of OpenAI's models, you don't need to do any of this because it assumes by default that this is the case. So what I mean is that when we actually create our agents, we're going to give them a name instructions, and we're going to pass in one of these models. Now if for this model you pass in a string you just pass in text, which is what we did before we just passed in GPT four mini right there. If it's text, then it assumes that you're talking directly to OpenAI. But if, by contrast, you pass in a model object an instance of OpenAI chat completions Model, then it will connect to this model using this endpoint. And so as you can see we have three agents sales agents one, two and three. The first one is connecting to deep seek. The second one is connecting to Gemini. And the third one is connecting to llama 3.3. The massive llama model through grok the fast inference platform. So there they are. This is how it all connects together. And we'll run that. And we now have these three agents connecting to these three different models. We will now do exactly what we did before our description, write a cold sales email. And we're now going to repackage each of those three sales agents into tools called tool one, tool two, tool three, and passing in this description as the description of the tool. And that is done. And we will now have a normal function tool. We use this decorator to reconstitute wrap this function in the the the boilerplate JSON that describes this tool. And it's almost a wrap. We're zooming through everything we did last time. We've now got some instructions. You can write a subject for a cold email. We've got some HTML instructions. You can convert a text email to HTML. We then make a subject writer an agent, which is going to do this. And as I say, I'm just passing in the string GPT four mini. So this is an agent that can write the subject of an email. This is turning that into a tool, a tool that can write the subject of an email. This is an agent that can convert an email to HTML format. And here it is as a tool, a tool that could convert a text email body to HTML email. I know I'm going through this very fast, but we just did it yesterday, so hopefully this is just immediate revision. So we run that and we collect together those three tools. This is our emailer agent the agent which is able to take some instructions the email tools. And it has a handoff description if you remember that. And now we're almost done. We now put these three sales tools into one group of tools. The handoff is now this one handoff agent. This emailer agent. We run that. And finally here we are the same arrival as before. The sales manager's instructions. We pass in the instructions we pass in the tools. We pass in the handoffs. And we then write the cold sales email. And with that I kick this off. It takes about a minute and I will see you in a second when it completes.
------
Video 39. Week 2 - Day 3 - Implementing Guardrails & Structured Outputs for Robust AI Agent Systems
------
Well, I'm back, although it took slightly longer than I expected, so the run that I had just kicked off actually ended up taking several minutes, and it appeared to lose its way. Going back again and again to get more and more emails generated. So I actually cancelled it and ran it a second time and it ran fine. But it is interesting. It's a it's a cautionary tale to reflect on the fact that by their very nature, these autonomous agents are made. This statement here, and this was the answer to the, the challenge from from last time that I say you can use the tools multiple times if you're not satisfied with the results. And that's really giving this kind of autonomy and potentially an infinite loop to the agent. And sure enough, we I don't know if it was infinite, but we certainly had some kind of a loop going on. But I ran it a second time and it was fine. If I bring up the trace, here come the traces. This this is the one that was going wrong. You can see it had run 14 tools and it was taking more than 300 seconds. If I bring it up, you'll see that it was going through sales agents one, two and three and then back to one, two and three again. And I can load more and on. So it kept going. Now you'll see, of course, that agent one is deep seek. Agent two is Gemini. Agent three is llama 3.3. One other thing that I didn't point out last time that I would like to mention is that you can see that these are running in parallel. They are running, of course asynchronously because again, because they're very I o bound, they are doing they're waiting for information to come back from the API call that allows async IO to be running multiple in parallel. But of course the three run and then it needs to sort of wait for. Then the sales manager will review those three runs and decide whether it wants to run them again, which apparently it did lots of times. And llama 3.3 for some reason took a really long time on one of them. So you can see there is inherent instability with these autonomous agent frameworks. And that's something that you need to code explicitly for. Four. Okay, but now let's just go back and look at the trace that did better. So this is the trace that just ran. And it took a rather more pleasing amount of time. And you can see that this time it only ran each agent once. Deep seek Gemini and Llama 3.3. Each one ran. Each one came back with a response. That response was of course went to our handoff, went to the email manager that wrote the subject, wrote the email, and then an email arrived right here in my inbox. So all has worked well in the end. So I went through that code very fast because it's something that we saw before yesterday, so hopefully it was somewhat familiar. The new angle was that we were calling multiple models, and it was very easy because they are all models which have OpenAI compatible endpoints. Now, you may have noticed that there is one model that was missing there in the form of Anthropic's Claude and anthropic does not offer an OpenAI compatible endpoint. And so I understand that there isn't, as of today, an easy way to use Anthropic's Claude in lieu of those other models that have an OpenAI compatible endpoint. But there are plenty of workarounds. So there is, for example, a third party provider called Open Router that people are probably a lot of people use that, and it allows you to to set up your keys for other models, including Anthropic's Claude. And it has an open AI compatible endpoint. So you can go through open router to use Claude. Also, anthropic has the incredible MCP protocol, which we'll be talking about in the last week. And that gives us another route to be using anthropic through MCP. So more will come on that later. In the meantime, the easiest way to do this is to use models other than anthropic for this purpose. And perhaps quite soon Anthropic's Claude will be available through the OpenAI SDK direct. Anyway, the next thing we're going to do is talk about both structured outputs and guardrails. We're going to do them together Other, and guardrails are ways that you can put a constraint around your agent platform. It's a test that you will do. And the cool thing about guardrails, I mean, you might think if guardrails were simply like a check of, of like, um, checking that something has given the right kind of results, you could just write that in Python code. You can just obviously look test whether the results have a certain property in them or not, and throw an exception if they don't. The special thing about guardrails here is that guardrails can themselves be agents, which means that you can use an LLM to be checking that things look good at any point in your flow. Well, actually, that's not quite true. When I say at any point, guardrails actually can only be applied either to the input at the very beginning, the input of the first agent or the output of the last agent. You can't insert guardrails all over the place. You just have them at the very beginning or the very end. And they're designed to protect your model against getting an input, which is inappropriate or not. Not what it's intended for. And also to protect it against producing an output which should not be shown to the user. So it's those two extremes. That's where you can implement guardrails. And we're going to implement an input guardrail right now. But as you'll see it's it's basically exactly the same thing either way. And I'll leave a challenge for you to add an output one afterwards. So the first thing I do because we're going to be working with structured outputs is I define an a class called name check output. We're going to be saying we're going to be checking for names for people's names because we don't want people's names in our emails. We decided so we're going to have a name check output. And this is one of these pedantic objects. If you remember, this is where you have objects classes which are designed to reflect a particular schema of data. So what we want is that we want the name check output to be something which has two fields is name in message, which is a boolean where true should mean that there is a name in this message and name. A string would be the name if there is a name in the message. Okay, so this is all we're doing here is we're sort of defining we're defining a schema. We're saying this is a particular structure of class that we want to to think of. And now we're going to define an agent. And I'm naming it guardrail agent. You can name it whatever you want. We are going to use this in a guardrail. But this is just as of now just another agent we're creating. It's name is going to be name check. The instructions are check if the user is including someone's personal name in what they want you to do. The output type this. This is the structured outputs piece. We are telling this agent we don't want you to output text, which is what you normally do by default. All of these are outputting just strings of data, but rather we want you to output a name check output. We want you to output an object that conforms to this schema. And you can use this for any of your agents. So we could go back and use this as a way to structure our emails better. So we could use this so that rather than emails being just strings, they could actually already have a subject, a recipient, a body. We could do it that way from the get go. And that would be much nicer. And that is in the challenges at the end. But for here we specify the output type is name check output. And as before we specify a model GPT four mini. And that brings us simply to the guardrail. So a guardrail is it's like an asynchronous. It's a coroutine. It's going to say a function. But it is of course strictly speaking a coroutine. And you simply decorate it with input guardrail to say, I'm going to be using this as an input guardrail. It's a bit like decorating something to be a tool. And there's a completely analogous one called output guardrail, which is what you would use if it was to be used as a guardrail for the output. You can call it whatever you want and it takes the context is a bunch of data that you can pass between agents that we we haven't made use of the agent itself. And this is the input, the input that's coming in. I've called it message. And so what we can do right here is we can as part of implementing this guardrail, we can actually kick off this guardrail agent. We can run this guardrail agent. And it will it will be giving it these instructions. It will be passing in the message and it will be expecting a name check output to come out of that. So this is basically going to do just what it says on the tin. It's going to check if someone's personal name is being included in the message. And then you have to return that the rule about writing these guardrails is you have to return something called a guardrail function output. That's that's always a thing you return, which is informing the system whether or not there is a problem. And it has two, two things. Output info is just a dictionary of stuff that might be useful for tracing and trip wire triggered is a boolean. It's true. If there's a problem you pass in true here. If something has happened which should trip the trip wire and alert the guardrail has not been met. And there is a problem. So what we're doing here is we're saying if this agent that we just ran, this agent produced an object of type name check output, if that has is name and message set to true, which means that this agent reckons that there's someone's personal name in the instructions, then that should trigger the trip wire. That should cause this guardrail to fail. And there should be a problem. That's that's the key. That's how it works. I hope that makes sense. And you could use exactly this structure. And then of course, for output guardrails, it's pretty much the same thing. But instead of message you would put the output there. Otherwise exactly the same. All right let's give this a try.
------
Video 40. Week 2 - Day 3 - AI Safety in Practice: Implementing Guardrails for LLM Agent Applications
------
Okay. The moment of truth. It's time to actually run a careful sales manager, a new sales manager. And this is an agent. It's named the same as before. Sales manager. The instructions. Same as before. Sales manager's instructions. Same tools and same handoffs as before. And the same model. The difference is right here. The difference is that we are passing in an input guardrail. Input guardrails is guardrail against name. And of course you can also have output guardrails that will affect the final output. And what we're doing is we're going to make this message send a cold sales email address to dear CEO from Alice and then kick this off. And no surprise, what we're doing here is we're passing in someone's name. And so we are expecting that the trip wire is going to get triggered, because our guardrail is going to notice that there's someone's name in here. Now, you might be forgiven for thinking, okay, well, what exactly is the point in this? I can see with my own eyes that we're passing in someone's name here, and you could just call an LLM at this point or something, you know? What are you doing exactly? The point is that, look, we've hard coded here the message that we're sending in to generate these sales emails. But the idea is that you'll be building this into a product. You'd have a product which would have a prompt, and your users, this kind of input wouldn't just be hard coded in a, in a notebook. It would be typed in by your users, your sales team, a prompt to to kick off their sales automation. And so the purpose of this kind of tripwire is to make sure that no one types in your CEO's actual name, or perhaps there'll be other PII personal identifiable information like phone numbers, that you'd want to make sure did not get put in your cold sales emails. That's the intention here. So whilst this is a little bit artificial that we've got it right here, the point is that this protection would be would apply to any input prompts that are coming in to your Agentic framework. That's the purpose of the guardrail. So with that explained we will now kick this off. We're going to run the careful sales manager and off it goes. And bam there's been an exception, you might think bug in my code, but no. The exception is what happens when a guardrail is triggered. And what you'll see here is guardrail input. Guardrail triggered tripwire. So something went wrong. And to look in more detail on what went wrong I mean, something went right because something went wrong. We go in here, we open up the protected SDR and we should see guardrail against name, name check. And we should see that that this guardrail got triggered. The output is name and message is true. Name is Alice. So indeed the guardrail here was triggered and that's why the process was stopped with an exception. So that is successful triggering of a tripwire and hopefully makes some sense. And of course, to make sure you're convinced this is working, I need to show you an example with tripwire not being triggered. So we can just call exactly the same careful sales manager, but this time the message is going to be send out a cold sales email address to the CEO from the Head of Business development. No more mention of Alice. And if I kick this off? Off it goes. It's running. You can see right away that there isn't an angry guardrail that's being trip wired. And so that is running successfully. And in fact, I just kicked this off a moment ago as well. And so I already have my prior email. And here it is. It did indeed come back with a perfectly decent email, and you'll be pleased to see that it's signed off. The head of Business Development. I'm interested to see it also has a copyright 2023, which is a slight sign of the results of knowledge cut offs there. You could always try and add that as a tripwire, as a guardrail if you wished. So that is showing you guardrails working and now some exercises for you to take this to the next level. Try some different models. We've tried Gemini and Deep Seek and Grok llama 3.3 through grok. This has just finished successfully, and you can try more of those two and see the different, different kinds of models that you can use there. The variety add some more input guardrails and output guardrails. And then finally we had our first exploration of of structured outputs there with the agents SDK. Try adding some more. When we generate emails we could actually be populating a one of these pydantic objects a schema instead of just returning text of the email. So try adding that to that is obviously a more robust, more bulletproof way of doing it. And then experiment with this and get to a point when you're happy with your SDR. You could also put a user interface around it if you wanted. And that is an interesting business development tool. All right. That concludes our session on tools agents, guardrails structured outputs and the like. Next time we're moving on to our next big project, which I'm so excited about building our own deep research. See you next time.
------
Video 41. Week 2 - Day 4 - Building Deep Research Agents: Implementing OpenAI's Web Search Tool
------
Well, I'm extremely excited about today. This is of course, day four of week two, and we're getting on to our first large, juicy project in the form of deep research. This is one of the classic use cases of Agentic AI, the case where you have an agent that is able to go off, search the internet and do some research, look at different links and try and research a topic that you give it. And we know this so well because many of the Frontier Labs offer this agent via their online chat tools. And for example, on OpenAI, you can go to GPT and you can press the deep research button. And that runs a model in deep research mode, which is basically running an agentic workflow. So we are going to do that. We are going to give our agents the ability to do deep research, and we're going to use a few things that we've already learned about. We're going to use tools. Of course we're going to use structured outputs again. We encountered it briefly last time a little bit more this time. And we're going to use something called hosted tools for the first time, using tools that are running remotely. All right, let's get straight to it. Okay. So here we are in Casl. We're going to open up the directory for week two. And we're now going to lab for deep research. And we are going to start again in a notebook like this in one of the notebooks running within cursor. And I know that a lot of you would prefer to be working with real Python modules. And fear not. We will get to that tomorrow. We will do this in Python code and proper modules. The lab, this kind of format, these notebooks, they're so good for talking through things for experimenting. And it gives you the power to be changing things, playing around while we go through this. So I think it's good just for today. So this is of course a enormous classic agentic use case. And what's great about it is it applies so broadly. You can apply this to any business area. You can build a deep research agent that's focused on your business area, or you can use this as something which is like a sidekick for yourself. We're going to build a different kind of sidekick later in this course. But but this is one kind already and it should be useful for you immediately. So I think it's terrific that we're doing this, and I hope you're as excited as me to be building your own deep research agent, something which the Frontier Labs have done themselves. Okay, so let's get started. Enough of my prattling away. We do some imports. We do one of these usual load dot envs. All right. And now it's time for me to talk about the big the big change this week, which is that we are going to use some things called hosted tools, which means that they are tools that OpenAI is going to run for us. There only are three of them that OpenAI provides, at least as of now, and we're going to use one of them. The three are the web search tool, the file search tool, and the computer tool. The web search tool All lets us do what we're going to do today, which is ask OpenAI to run a search on our behalf. The file search tool is how we can run searches across the vector stores that you can run. You can upload and have remotely with OpenAI and computer tool allows things like taking screenshots and clicking around a computer. Now we're not going to be using either of those two. We're going to be doing the web search tool. But fear not, later in the course we will have basically our own computer tool, but we won't be running it at hosted. We'll be running it on our own computers, as you will see. In case that sounds intriguing, but for now we're going to be using the web search tool. And I want to make an important point, which is that OpenAI's web search tool is not that cheap for some reason, which seems not entirely clear to me. It actually costs 2.5 cents right now for me to run this for, even at the low end of the cheapest model at the lowest end, 2.5 cents per call, which may not sound like it's a huge amount, But typically if we're running a deep research, we might be doing like ten different searches. And that means it's $0.25 a pop, which means that you can quite quickly add up to 2 to $3, which is what I spent the first time I did this. I think the way I have it set up now would be more like $1. Still, that's maybe not to be sniffed at. So I would suggest monitor this and you can decide to skip doing the searches, or I'll show you where you can turn it all the way down so that you're only spending a few cents. And then later in future weeks, we're going to be doing this a bit differently. And we'll have other ways to do this which will cost a fraction of this. So. So fear not. But for now, it's nice for us to use OpenAI's. And OpenAI's is actually really good. So. So maybe maybe it's worth a 2.5 cents. I don't know. The costs themselves are on this link here. So. So it's always worth double checking because it might be different for your region, or OpenAI might change the pricing all the time. So go there and you can look we are going to be using GPT four mini low search okay. With that, let's get started with our first search. Okay. It's now time for us to create our first agent for this product. And the first agent is called the search agent. So here are the instructions. The system prompt for this agent. You're a research assistant. Given a search term, you search the web for that term. You produce a concise summary 2 to 3 paragraphs, capture the main points, write succinctly, and no need to have good grammar because it will be consumed by someone else synthesizing a report. It's a very clearly written set of instructions right here, and I would love to take credit for it. But I must confess that I just took this verbatim from OpenAI's documentation for how to do web searches with their tool, so we can probably take it on good authority that this is a well written prompt, since it's written by the people that built the model. So, um, we're going to use this prompt. We're going to create a search agent. It is an agent named search agent. This is the the instructions. And we're going to give it one tool. And that tool is called a web search tool. And this is one of OpenAI's three hosted tools. So we're we're providing that as our tool. There is an optional parameter. You can specify the search context size which can be low, medium or high where that reflects the price that you'll pay. Low is of course the cheapest and then medium is. The default is only a little bit more, but high I think is quite a lot more. You can also pick the model. It's a lot cheaper to pick GPT four or mini. GPT four is a lot more. And um, finally there's there's this here. This is a nice little snippet of code to to to keep in mind. This is how you say that this agent is required to run the tool. It is mandatory. It doesn't have discretion over this. So this is this is the way you do it. You pass in this model settings with tool choices required. And that's a nice handy thing to have to one side. And of course in this case we want to do that. We want to it's basically this agent is wrapping this tool call. And so when we're picking GPT for a mini and we're picking low, this is when as of now this costs two and a half cents every time we run this agent. So let's run this agent. Let's ask for the latest AI agent frameworks in 2025. We will run that search and we will display the results and see what happens. It's running. And there we go. There are the results. It's come in nice markdown. Let's see it says lang chain lang graph crew semantic kernel autogen OpenAI gym. That's uh, not exactly I guess that's a that's a reinforcement learning framework. It doesn't know about itself. It doesn't know about OpenAI agents SDK. Amusingly, uh, Rasa is an older framework and Jade for Java people and, uh, yeah. So, so, uh, an interesting set of results, I'd say mixed accuracy because I would. I would argue that long chain and semantic kernel aren't really agent frameworks, but certainly Landgraf, crew, and Autogen are very much there, and ones that we will be spending a week on each. All right. But this is a perfectly pleasant search result, and it is the beginning of our deep research agent, and it's always worth taking a quick look at the trace in OpenAI. So click on this little useful link here. And up it comes. And here it is. The search agent is our current trace. If I open it up we'll see. Sure enough that the search agent was called. And it tells us over here the tool it used was web search. And we get a bit of information about the total number of tokens that it did. That's the instructions, the system prompt. That was the user prompt. And this comes back is the output. Just as we would expect the trace has worked nicely. Now we can get on with the show.
------
Video 42. Week 2 - Day 4 - Building a Planner Agent: Using Structured Outputs with Pydantic in AI
------
Okay. So now we're going to work on another agent. And we're going to make heavier use of structured outputs than we did last time. So the agent in particular that we're going to work on, we're going to call the planner agent. This is the one that's going to be responsible for taking a query and coming up with a handful of searches that it should run based on that query in order to do some deep research. Now, I'm going to keep the number of searches small to three searches because as I say, it's two and a half cents for each search. And I don't want to add up to some big old bill. When I first did this, I had that at ten. So you certainly can have it at bigger numbers. You obviously get better results if you do that, and it's very much a personal preference thing. Run it with three to start with. See you know your cost $0.10, $0.20. And if you're enjoying yourself then you know splash out, spend half a dollar and get one really good comprehensive result. Okay. So we're going to keep it on three. So the instructions what is the system prompt we're going to use here? You are a helpful research assistant. Given a query, come up with a set of web searches to perform to best answer that query output. And then the number three terms to query for. So that hopefully is super simple. We're just building an agent that is going to, given some sort of query, come up with a bunch of things you would search the internet for in order to research that query, and we're calling that the planner. So keep that in mind because then everything is going to click into place. So now we use structured outputs. Remember this. This is where you have a class that is a subclass of this thing called base model from pedantic. And that means that we're going to use it as like a schema. We're going to use it to describe a structure, which we will later ask a model to return information in. And we in the case of a web search item, we're going to have a reason and a query. And the reason is a string, and the query is a string, and the doc string that we have right here underneath this is this is super important. And by by adding this documentation, by annotating these fields with this comment right underneath them, we ensure that that that information is actually provided. That's as part of structured outputs. That is part of the information that's provided to the model. So it knows why it's populating these fields. So this is another good trick. We didn't do that yesterday. So so now you've you've got this in your toolkit as well. Now one other small point. There's various reasons why you might ask for a reason as well as just the query. But one of them is that this does actually result in sort of reasoning style behavior. This is very similar to to what people call chain of thought prompting. I guess it is chain of thought prompting, but by asking the model to output its rationale for why that search is important, and actually by asking for that first before the query itself, you encourage the model to be sort of apparently thinking through, and as a result, you tend to get better outputs. You get better queries, just as a side effect of asking for a rationale first. And I've said it before, but you always have to come back to remembering that while we try and treat models like they're like there's a bit of humanity to them, this is just a strange side effect of great next token prediction, because it's predicting the most likely next tokens. That's all these models are doing. If you ask it to start predicting tokens reflecting a reason, then when it starts to predict tokens reflecting the query, it is more likely to be consistent with that reason, and it ends up with this very coherent output that just makes more sense. That has a higher likelihood of being what you're after. So another those little sidebars, and I do have a bunch of YouTube videos that clarify that more, but it's so important to keep that in your mind because it really helps figure out what works well with these kinds of prompts. All right. So this is a pedantic object with a reason and a query. And now we've got another one called Web Search plan. So this was web search item a web search plan is a list of web search items in a field searches. So this is an object which has one field searches. And that is a list of these guys. And you can see it says they're a list of web searches to perform to best answer the query, a list of web searches to perform to best answer the query. And that is what goes there. And this annotation is associated with this field searches. Okay, hopefully you are with me So then we now create our planner agent. It is an agent that's named planner agent. Its instructions are these ones right here. The model is GPT four mini and the output type. Remember this is where we specify structured outputs. We tell the planner agent that it shouldn't output in text like these things normally do. It shouldn't just create generate natural language in the form of text, but rather it should build an object of type web search plan, which means it's an object with one field searches, which has a list of items that looks like this. And remember, as another sidebar, keep in the back of your mind when I say that we're going to tell the model to create an object. Maybe that all sounds a bit magical. Like how does a model, how does a large language model, a transformer architecture, know to generate objects? Python objects. Remember, any time you hear magical stuff like that, it usually means we're talking about JSON behind the scenes. And that's exactly what this is. What happens is that this is turned into a bunch of JSON, and the model is prompted and it says your response should conform to this JSON. Here is how it needs to look. And as a result, the model generates that JSON and it gets populated into an object of this type. That's that's the end of it. There's nothing magical about it. All right. We should now run this and see it working. Here we go. Let's do it. We run this, and now we're going to say the same. The same thing that before we just did an internet search on latest AI frameworks in 2025, what we're now going to do is carry out. We're not going to do any searching at all. We're just going to use this planner agent to come up with three web searches that would be relevant, and we would want it to respond with an object of type web search plan. So let's see what we get if we run it and print the result. So what we get back is an object which has a field searches Like that. And that field is a list. Good. And it's a list of things called web search items. Great. And a web search item should have a reason and a query reason to find the most recent developments and releases in AI agent frameworks for 2025. And the query is just latest AI agent frameworks 2025. Okay, fine. That doesn't seem particularly profound, but it seems perfectly reasonable. The next one to explore industry specific or academic research. And the query is emerging AI agent frameworks. And the next one is to be updated with major conferences, publications or announcements. AI frameworks. Conferences. Announcements 2025. Fair enough. Seems like a perfectly plausible, reasonable way of doing it. I'm not sure I would have done that last one, but. But who's to say whether I would have done it better or GPT four or mini is going to do it better. But that is the planner at work and you're seeing structured outputs used very well there.
------
Video 43. Week 2 - Day 4 - Building an End-to-End Research Pipeline with GPT-4 Agents & Async Tasks
------
Lecture thumbnail
10:25 / 10:28 Okay, next one is a boring one because you know it. Well, I've brought in the same send email tool that we used before. So this is we're using the decorator the function tool. If you remember you use that decorator if you want to convert a function into something that can be a tool. And it will generate all of that JSON for you behind the scenes. So this is send email a subject and a body, and it sends out an email with the given subject in the body. We're going to use SendGrid as before. Hopefully you still have your account handy and you have it in your env file. Remember to change this email. Let me change this to your verified email. And yes, and please do also change this one. Again, don't send me lots of emails with AI generated content, but by all means send me emails with your questions. Of course I always welcome them, but I don't welcome AI generated search deepresearch things as much. Uh, so then uh, content would be, uh, the HTML version, and then we send the email exactly as before. That is our tool. You don't need to see this, but I'm going to show it to you anyway. You remember that if I just print this. What's it doing here? If I just print this out, we should see that we get a function tool with a description and so on. And if you're not sure why this has happened, then do look up decorators. Make sure you understand how decorators work and how they're able to convert a function like this. Okay, with that in mind, this is an email agent. This is all old hat to you. Now we've done this several times. The instructions are you're able to send a nicely formatted HTML email based on a detailed report. You'll be provided with with a with a detailed report, you should use your tool to send one email providing the report. Convert it into a clean, well presented HTML with a subject line so you can see I'm no longer trying to break things out. I'm giving this agent full discretion to come up with a subject line, to rewrite the email, to do the whole lot using GPT four mini, and we're providing it with the one tool to actually send the email. So that is our email agent. And that is something which is old news to you. You know, it's super. Well, okay. We're about we're about to get real. And so I will, uh, take a pause for a second and then we will get into the real meat of the project. Okay. So we have another agent again that's going to use structured outputs. So this is the researcher. So you are a senior researcher tasked with writing a cohesive report for a query. You'll be provided with the original query and some initial research done by a research assistant. You come up with an outline for the report that describes the structure and flow of the report, and then generate the report and return that as your final output should be in markdown, it should be lengthy, The detailed 5 to 10 pages of content of at least a thousand words. Okay, so that has framed the situation. Now we're going to use structured outputs. Again we're going to have a report data pedantic object a subclass of base model. It has a short summary which we begin with a short 2 to 3 sentence summary. The full markdown final report and then some follow up suggestions. Suggested topics to research further. And so this is an agent. It will take these instructions. It will use GPT four or mini the output type. This is where you specify that we're using structured outputs. The output type is the report data. And there we have our our report writer. And this is our writer agent. And this is the final building block before we get to the actual flow through this project. Okay. So it's crunch time. We have five functions for me to show you. But they're relatively simple. They're pretty straightforward. So here they are the I'm going to show you three and then two. The first three, the ones that are going to execute a search using the two agents that we defined before the planner agent and the search agent. And these functions, you can see they're very short. They're very simple. Let me tell you what they each do plan searches. All right. This very simply calls runner run for our planner agent passing in the query. And we make a query and that gets passed into the planner agent. Remember the planner agent is the one that will then come back with a number of searches. So we'll then count how many searches there are and print that. And we will return that final output, which is in the form of that search list. That's what formatted it. Again, it is this thing here, the web search plan, a list of web search items. All right. Back we go. Okay. So then the next function is perform searches. This is when we're actually going to do the searches for each of these. Now do you remember I showed you that asyncio a while ago? That was the great way that you can run things in parallel. Perfect time for it right now. So we're going to create a bunch of asyncio tasks to search for each item for item in the the results of these searches. This this this is the Pydantic object that searches feel gives you the list of the search items. And for each item we we're going to call search searches. The next function we'll talk about we're going to call search. We're going to. This gives you a coroutine so we can turn that into a into a task. And then we can use this gather approach that I told you about async gather. If you then await that, it will run all of these tasks in parallel and the results will go into results. And then we will put finished. So hopefully I didn't lose you there. Basically all I'm saying is we take the results from the planner and we just search for all of these things in parallel using the search agent. And this is how we actually use the search agent. We give it an input. And what we're actually doing is we don't just tell it the query. We also tell it the reason that we're searching for that. Remember I said there's more than one one reason why we asked for a reason. It's also so we can give some context here. And then we, uh, we we run the search agent and then we return the final output of the search. Okay. Hopefully you followed these three. If not, come back and look at these as such short functions that that it should make sense. And then finally some housekeeping. Two functions. One of them to write the report, one of them to send the email. The writing the report will use a writer agent. It will, uh, run that agent that is going to reconstitute the output in HTML. And then finally we will write an email using the email agent. And that will then complete the process. So I should run these two to define these two functions. These two these five functions. And now we should be ready for showtime. All right. And if you're one of the people that doesn't like the way that I just execute code and talk to it, it's sort of lazy, then you're going to be happy. This time I will type the code so that we get to do this part together. So it is a moment of truth. We're going to say, like with trace research, Trace Curse is already filling it in. I'm not. No, no luck for me. Well, I'm going to ignore what curse is saying here. I'm not looking at that. So we're going to print a message like starting research. Sometimes cursor is too good. All right, let's get rid of cursor. No cheating. I'm not looking I'm not looking at cursor saying search. What if I type something different? Uh. Confuse cursor. All right. Now search. That doesn't work. It's still. It still figures it out. All right. But there we go. It knows. So the first thing we're going to do is have a variable called search plan, which is going to await planning the searches. So if you come up here there is planning the searches. So that's the first step. We do that. All right. The next step is the search results. We're going to perform the searches. So we come up here we perform the searches passing in the search plan. That's the next step of this which we also await. The next thing we need to do is actually run the report. So we await writing the report, passing in the query and the search results. And cursor knows well as do you. You're already saying what we need to do next is send the email. And that really is it. I can take out the and then I can print. All right. We won't we will do something different to what cursor suggests to show that we have some autonomy here. We're not just ruled by the machines. Okay, there we go. Let's kick this off. Let's have this running. So it's starting the research. It's planning the searches. It will perform through searches. So it's. And it's already finished. So. So it used an agent to figure out what three searches to do. And then for each of those, it used a remote a hosted tool from OpenAI to actually carry out that search. It got back all of the results. We're now using an agent to synthesize the results and turn it into a report using. It's going to generate a report in markdown, and then it's going to turn that into an HTML email. So um, we will let it do its thing. Let's just come back up. The final report is is is is in this format which has markdown as one of the, the parts of it. And that's the part we're now doing the writing email. And it's in writing the email that it will both come up with a subject line. And it will also write it as an HTML email. And in a second, hopefully I will receive that email.
------
Video 44. Week 2 - Day 4 - Building a Deep Research Agent: Parallel Searches with AsyncIO
------
Okay. It did indeed finish. It completed its three searches. It ended with hooray, which is success. And I received an email. And here it is. A nice HTML email, well formatted with a good introduction and overview, detailed analysis crew. It's now putting at the top, which is in fact what we'll be doing next week. Look forward to that. And then there's generally some great information here. I would say that there's a couple of misses, but it's generally done pretty well. And then at the bottom are some references with links since it's an HTML email. And if I click on that it does appear to work, which is nice. And I will say, I do find it very satisfying that with this minimal scaffolding with something that was really very simple indeed, there's not much code to this look. That's all there is with just this. We were able to build this whole deep research framework. And whilst, you know, maybe those those results are reasonably simplistic, I hope you appreciate how much potential there is with what we've just built, and can think of all the different ways that we could expand on this. And in fact, one very simple way we can expand on this is just to turn up that number, which you may not want to do, because as I say, it will cost that few cents a piece, but there's no reason why I can't do it for you so that we can get some to show you what it looks like to have that deeper kind of research. So I will do that right now. We will change that to 20 and, uh, go through and make sure that we, um, hang on if I just set that properly. Yes, I have that has now recreated that planner agent. So now we will come back down here and just make sure that we rerun everything and rerun this. And now we will expect to do rather a lot more searches, 20 more searches. And this may take a moment longer and then I will come back and show you what results we get. And that is completed after 20 searches. And I can show you the results, which actually it looks generally a bit similar. It's got the same key frameworks, but it's got more information laid around it. It has applications and benefits associated with each of the frameworks that I identified. And then it has some commercial implications at the end and a conclusion. And so it's certainly more substantive than the previous one. It's a step up. And obviously, should you wish, I would very much recommend that you experiment with this. And we should also, of course, go in and look at our trace. And if we come in to the research trace, we will see that all of these agents ran and they all ran in parallel. The planner agent took took took that time to start with. Then there's load more. There's all of this and more. And then at the end was the writer agent and the email agent. So you can see everything. And it definitely gives you that sense of how Asyncio ran all of these different searches in parallel at the same time. And then, of course, the writer and email were sequential. And that then is the conclusion of this part of the deep research agent. The thing I'd like you to do is bear in mind, have a think about how you could make this more substantive. What more could be done? What would you like to do to beef this up? And then we'll do a quick wrap up before we launch into tomorrow. So I really hope you enjoyed that as much as I did, and that you appreciate how simple it was to build that framework around a deep research agent in your deeply puzzling over how you can make it better. But actually, what we're going to be spending tomorrow doing is putting this into an application and having it be something that is like a takeaway deep research agent. And I'm very excited for it. And I will see you then.
------
Video 45. Week 2 - Day 5 - Building a Modular AI Research System with Gradio UI Implementation
------
So I have good news and bad news for you. The good news is that today, the last day of week two, it's going to be quick and easy and satisfying. So there's going to be fun stuff right away. The bad news is there's going to be a really hard challenge at the end of it. It's going to be hard, but really big and juicy and good. So I do hope that you will take on the challenge that's coming up. But first, let's just talk about what we're going to do today. We're going to take our deep research. The agent that we built last time in a notebook. And we're going to turn it into proper Python code in modules. Yes, with a user interface featuring Gradio. Let's go and do that right now. And so here we are in cursor and we're going into the OpenAI folder. But now we're going into another folder into the deep research folder that is beneath it. And you will see that there are some Python modules you'll be happy to see. Because I know you like Python modules, and I'm going to just talk you through them, and I hope going to be pleasantly surprised how easy it is, and also how natural it is to move from a notebook world into proper modules. And again, one more sales pitch. It's where I like to work experimentally, first in a notebook and then turn it into modules. As you've built out your prompts, as you've experimented and refined your agent architecture. So what we've got in here are some agent classes, some agent modules with a single agent class, and then a manager that packages it all together. And then finally, deep research is the user interface, which is a very simple user interface. So let's start with the classes that represent the agents. And it's going to be really simple. We will start with the planner agent, which is exactly where we started when we were in the notebook the planner agent. We've got this. How many searches? I've got it set to 20 now, but I'll turn it back to three before I check this in so that no one racks up a big bill. If you use it, and you'll recognize the same old instructions and the same pedantic objects for a web search item and a web search plan, which is the list of web search items in the single field searches. And here is our planner agent that takes our instructions and it uses the web search plan. There we go. Okay, next up, the search agent and I need to make one little change to this guy. So this is the search agent. The instructions you'll see here your research assistant. And it's got got this this format. You'll remember that we stole that format from OpenAI's documentation. A very nice set of instructions right there. I don't know if you spot the little change I have to make, but the tools in here, I need to add in that parameter to make it nice and cheap. So let's go and do that together. If we go to this lab, if we look at what that parameter was, that parameter was actually near the top. I think we did at the very beginning. There we go. It is that the search context size is low. Let's go and put that in there right now so that no one spends unexpected amounts there. We have it done. Okay, so that is the search agent class in the search agent module. And now we've got a couple more, two more agents to talk about the writer agent again. And you know, the benefit of having these in Python modules, it looks very crisp and simple. You can see exactly what's going on. We've got our instructions. We've got the pedantic report data that has a short summary, a markdown report and follow up questions. And then our writer agent, which is the agent that will take all of the information from all of the searches and turn it into a report data object. And the final agent, if you remember, is of course, the email agent. Here it is. This is stuff that you will recognize. This is just the same thing that is going to send the email. You must remember, please to change this to be your verified sender here, not make put your verified sender here and then put some other email here. Put your recipient here, not me. I think I might change this before I check it in because otherwise someone's going to forget. Well, many people will forget and I'll be overloaded with spam email. All right, so this is the tool. And actually it's good to have a comment in here because that will send an email with the given subject in HTML body. Very nice. Okay. So that is our email agent and that is all four of our agents. What's coming next is research manager. Okay. So research manager is a class. And this contains those short functions that we had last time with a little bit of a twist spiced it up. So first of all there is a single method here run or it's a coroutine. If you want to be pedantic with me, this coroutine run an async def run. And this is just the same as the as the code that I had before. We've got a little trick here to, to generate a trace ID so that we can, we can print where to go to look at the precise trace that we're looking at here. Now you may wonder why we've got all of this yielding going on. We, uh, we're treating this as a generator that is going to yield various bits of text along the way. So that is going to become clear in a bit, but just take a look at what's going on here. I print some updates and I also yield those updates. And I end by yielding the full markdown report. Now hopefully you've done your intermediate. Either you already know all about generators or if you don't, then you've taken a look at the guide that explains generators so that there's no surprise. Okay. And then I've just got each of these functions plan searches, perform searches, search and write, report and send email. These are the five functions that are exactly the same as the five functions we talked through before, except a little bit beefed up. They've got more type hints put in, as one should do for proper checked in Python modules. And there's a bit more exception handling. And this is a little bit fancier to handle the being able to print a status update while it's going through the different searches, but otherwise it is much the same. So it's just a bit tidied up, which is the kind of practice you should do when you move from the notebook environment, when you're being more experimental into something that's more well baked code, but it's the same functionality. And that finally brings us to Deepresearch. Here we are. This is our Gradio app. This is where the magic happens. So Gradio which we import here. This is the the the package which you may know I love, which lets data scientists build great UIs without needing to know anything about front end. And I'm going to give you just some, some insights into how this works without I'm not giving you a full radio lesson here. And radio has great docs. It's really easy to read up about it and build build your own. But just very briefly, this this code here is where the user interface is defined. And we're used to using things like chat interface which is the super canned simple one where you just you can create an interface out of nothing. If you want to actually build your own user interface from scratch, then you have to use a slightly more detailed version when you do with great blocks as and then something like UI, and then you put your your code in there and then you say UI dot launch at the end and that that's the way to do it, that that will then build your user interface. So that is exactly what we've got here. We say with great blocks. And then you can pass in a theme if you want to have a different theme to the to the standard theme, and you can find the different colors that you can have in here. And then you simply put down the fields that you want. So we're going to start with a heading GR markdown gives us a heading and we want to have the word deep research as a big heading. Then we want a text box that says what topic would you like to research. And we're associating that text box with a field called query. So this this is a is a field. It might be a bit query clearer if we call this query text box. Let's say that query text box. And then here the next field we're going to put on a user interface is called run button. It's a button. It says run on it and it's a primary button. So it'll be nice and blue because blue is the sky, blue is the color we've chosen. And then the next one is going to be report, which is going to be our actual report text. It's a markdown and its label is report. Okay I'm going to change this here to query text box. So what we can now do is register an event. We want to register an event with the run button. We're going to say if the run button is clicked and we register that event by calling a function, click on run button and we just have to tell Gradio we want you to call us back. If that run button is clicked, then call this callback. And that callback is going to be something called run. And spoiler alert, run is right up here. So if if run button is clicked then call that callback. Run. The inputs that you should use should be the contents of the query text box. So the inputs to this callback should be whatever's in the text box when they press the run button, when they click the run button, and the outputs. Whatever comes out of this callback should go into, I want you to hook that up Gradio hook it up into this report markdown. And then for the this, this next one is really the same thing. It's, it's saying that if they're in the query text box and they hit the enter key to submit, then again call the run. The input should be the contents of the query text box, and the output should be the report. So this is a way that you can hook up gradio so that the widgets that you've constructed here get associated with inputs and outputs of a callback function that's getting called. And this this method here is going to generate front end code that's going to run in a browser. And when they press that button it's actually going to call back this Python code in our class populating it with the right the right fields. So it doesn't matter if you didn't follow all of that, or you don't get or even care how Gradio hooks things up together, just have some general intuition for it and know that UI launch is the thing that is then going to generate the front end code based on that, and figure out how to make sure that our callback gets called in browser equals true means that it's going to bring up a browser window right away. You don't need to have that if you don't wish. So the only remaining thing for me to mention is what then is this? What is going on with the with the run up here? So This is the callback that gets called, and you can see that as part of this I am calling the research manager Dot run, and that is the the long function that we looked at a moment ago that had lots of yields in there. And that brings me to the final part of the puzzle, which is normally in Gradio. With these callbacks, you just return a result, the chat function that we've worked on in the past, that's an example of a callback. Chat just returns the result, but you can also, instead of doing that, have your callback functions be generators that Gradio needs to needs to iterate over and they will yield back results. And if you do that, then Gradio will show that incrementally in the user interface, so that you don't have to sit there and wait for a long time and suddenly see an output. You can see interim results as well. And that is why we had a bunch of yields, and that is why we have them here in our callback. I hope that makes some sense, but it certainly will do when we actually see it. If it works, let's find out.
------
Video 46. Week 2 - Day 5 - Deep Research App: Gradio to Visualize & Monitor Autonomous AI Agents
------
Okay, so first of all, we bring up a new terminal, which we do by pressing the control button and hitting the tick. And when we do that, up it comes. You can also get there by going to the view menu and choosing terminal. So we need to CD into the two OpenAI directory. And now we need to change again into the Deep Research directory because that's where our stuff is. And you might be thinking we now type Python py. But that would not be right because we're working with UV. So you do UV and then run and deepresearch py because that will make sure that we're in our correct virtual environment. And that is what we do now. And it's then going to pop up a screen like this. This is our deep research screen. This is our Gradio UI. And make it a tiny bit bigger. All right. What topic would you like to research. So how about this. How about we say, uh, what are some of the most exciting commercial applications of autonomous agentic AI in as of April 2025? All right. That seems like a good question. Why not? Let's give it a whirl. Run! So first of all, we get this popping up. View the trace. If we click there, it comes up and shows us it's just beginning. We're still at the very beginning stages. We'll hopefully in a moment see a bit more of what's going on. Not not yet. It's still in the early stages. What we're seeing here is an update appearing for what's going on, and that is because we are yielding these updates. And every time that we yield those updates in our run, coroutine that gets surfaced through to to our callback and through to Gradio. And that's why it's appearing here because of those yields. That's why we did it. Otherwise we'd be sitting here wondering what's going on. So it's already done the 20 searches, because they all happened in parallel, as you know, thanks to the magic of Asyncio. And now it is writing. It's written the report, it's now sending the email. And hopefully any moment now it will complete, which will be nice and we will get to see. And here it is. And we don't need to go to our email for a change, because it's all appearing here in the Gradio app and it's appearing in beautiful markdown format. It's got a nice introduction overview of autonomous Agentic AI cybersecurity, using it for threat detection, reduced workloads, customer service, obviously healthcare, automotive and transport, financial services, analyzing market trends and executing trades based on real time information. Maybe we're going to be doing some of that ourselves. Logistics and supply chain okay. And ethical. Regulatory. So so now it's talking about about some of the sort of side effects or consequences and some conclusions and follow up questions. So there you have it. The result of our deep research right here for us to see. Let's look at the traces and see how this is going now. This is the research trace. And sure enough there of course are our agents. And at the end is our writer agent and our email agent. It successfully ran. And this is now a tool that you can use and that you can refer to and have as a deep research tool. And there's still one more thing to talk about, which is how we can now extend this and make it more powerful.
------
Video 47. Week 2 - Day 5 - Deploying Smart Research Agents with Gradio and HuggingFace Spaces
------
While it was great to have seen our essential use case in action and with its own user interface. So what's next? Well, next it's over to you. The thing we want to do is make life harder for our deep research agent. We want to put it to work. So we want to really take this to the next level. It's still reasonably surface level in terms of the substantive ness of these, this deep research reporting. And it's something that is giving you an easy starting point to turn this into something which is a fully baked product. And here are some of the things that I am asking you to do and give this a shot. Do do do your best. So first of all, we can learn a lesson from the way that OpenAI does it themselves. One of the tricks that you'll see that it does is that when you first ask for a deep research challenge, the first thing that happens is it comes back and asks for some questions. And so it would be great for us to build that. You can imagine. That's very simple. Agent step. The first step is to ask, ask. Come up with three clarifying questions associated with this query, and then make sure that you incorporate those clarifying questions in what searches are done. Take them into account. But then the the hardest part of this is that you really want to turn that manager that we have from just being a Python script and a series of function calls into being something that's truly Agentic take some hints from what we did earlier in the week when we were taking agents as tools, and we were doing handoffs and build some of that into the deep research so that it has some more autonomy to decide whether it wants to do more searches or not. You might want to put a cap on that, but you put some controls in there. But but you want to try and give it some ability to go off and explore and also potentially to refine the queries based on what it's learned so far. You could be thinking about adding in the kind of Evaluator optimizer design patterns, so that there's another agent that gets to check the work of the deep researcher as well. So all of these patterns that you can add, but most importantly, the overall autonomous manager to get to a point where more work happens, more analysis takes place so that you get a more comprehensive and more compelling outcome. Something which really can take several minutes and come back with something that is adding a lot of value. So that is the challenge for you. I would love to see your results. I'd love it if you could push your your projects to the community contributions folder and put your work into its own folder so that people can run your deep research agent separately. Perhaps you may need to do some fiddling around with gradio to be able to do things like surface those questions, but it's great fun to work with and it will be an awesome project. And it's also something that you can then showcase. You could because it's a Gradio app. You can of course type radio deploy to deploy it to hugging face, and that way you can have a spaces where it's live. You can share it on LinkedIn, and you can get others to try it out too, and be able to show people about the expertise that you've acquired in building sophisticated agentic AI. And with that, that wraps up week two, and I have had so much fun. You can tell I absolutely adore OpenAI agents SDK, and I'm excited to come back to it again in week six. But as it happens next week, we're working with crew I and crew AI is my second favorite, so it may not be quite quite as as dear in my heart as OpenAI agents SDK, but it's my second favorite. It's also really cool. It's very close to OpenAI agents SDK, and of course it has some different concepts, and there's some ways in which it's definitely better, as you will see. And I can't wait to show you. So see you for week three. Crew AI week coming right up.
------
----------------------------------
Section 3: Week 3
Video 48. Week 3 - Day 1 - Crew AI Framework: Creating Collaborative AI Agent Teams
------
Well, hello and welcome to week three. We are here. We are at Crew Week, and this is the time that I get to unveil the world of crew A, and I want to say that you might find it a little bit painful to make this mental change now, because we've just fallen in love with OpenAI agents SDK. We've got used to it, we know all about it. And suddenly there's going to be a change new terminology, new constructs. And the thing is, I would say there's a lot in common. There's going to be some differences too, but you're going to find you're quickly going to fall in love with AI as well. And this is going to be a repeating pattern in the next few weeks that we're going to get comfortable with something and love it, and then we're going to have to put it to one side and move on to the next and just keep it in mind. Keep keep in mind the differences. What's similar, what's unique, what's better, what's worse. Because different projects will lend themselves more to different frameworks. And you're going to find that you'll fall in love with one. And it may not be the same as me, and you should be able to make their own determination, and you'll learn something from each of these experiments. Plus all of our commercial projects will be a bit different. Okay, that's enough preamble. Let's get to it. So crew is actually several different things. We're gonna we're gonna have this a couple of times. Autogen is much the same. But crew in particular, when you when you hear people talk about crew, they might be talking about something called crew I enterprise, which is crews platform for deploying agents, for running them, for monitoring and managing them through a number of different screens. They sometimes just call it the career AI platform. Sometimes I think officially it's branded crew AI enterprise. And if you go to their landing page at currycomb, not com, you'll see that this is what sort of presented first and foremost. Secondly, there is a product called crew crew I, UI studio, which is one of these low code, no code platforms for piecing together agent interactions. A bit like an addendum that we looked at at the very, very beginning. But it's a it's a nice, elegant end user tool. And then thirdly, it is something called the framework, the crew AI framework, which is an open source framework for. And I quote from, from the site orchestrating high performing AI agents with ease and scale. So these are the three different offerings that they have. And hopefully no surprise to you, we are going to be focusing on the open source framework because we're here to build agents ourselves. We're writing the code. We're not going to be using the low code tooling, which is there, and we're also not going to be necessarily needing to do something where we're deploying them and paying for a hosting platform, which is really crew AI enterprise. But it's perhaps worth pointing out that when we think of the differences between these different platforms, OpenAI and anthropic have the great benefit that they already have a reason for being. They they have their models and that is their source of revenue when it comes to groups like cry. Of course, they they need to be very mindful of a monetization strategy. And the open source framework is very popular, very successful. But of course, they also need ways to monetize and cry enterprise. And their their tooling is their path to doing that which makes complete sense and which I don't I don't hold it against them for a moment. But it does, of course, mean that when you go to things like their website, there is a lot of upselling going on that they want to try and win people over so that not only will they use the open source toolkit, but then they'll end up paying for hosting and deployment within the broader cry platform. And so now for the rest of what we talk about, it will always be the open source framework that we will be working with. And when you work with the framework, there are in fact two different flavors, two different approaches that you can use for all of your work with the framework. And one of them is called crew I Crews, which is when you have autonomous solutions with teams working together, agents of different roles. Crew is crews word for a sort of team of agents, a crew of agents. And then there's also something called crew I flows. And this I think this is actually a newer part of crew because at least I wasn't aware of it about six months ago when I last used crew. It may have been there, but. And I just didn't notice it, but but it's certainly now quite prominently in the documentation. Cry flows are more, um, prescribed workflows, fixed workflows where you divide a problem into multiple steps and you have workflows that lead through it with decision points and outcomes and so on. And their their documentation suggests that you should choose crews when you're looking for autonomous problem solving, creative collaboration, or exploratory tasks versus flows, which is more about deterministic outcomes, auditability or precise control. And I probably I imagine and this is just speculation, but I'm imagining that flows has come out as a result of some of the concerns people have about running crews in production, where there is this greater level of uncertainty and lack of auditability and sometimes a tighter defined flow, a work flow is what's required rather than a fully agentic autonomous solution. So that hopefully gives you a bit of context, and again, no surprise to know that we're going to be focusing on the crews, because this is all about agents. This course and building workflows is something that you can also do, but it's a bit more straightforward. And of course, you could also do that simply by calling LMS directly and by interpreting their responses. What we're interested in is the more autonomous aspect of this. It is about when we allow different LMS to choose their own adventure and to go about solving their problems in an agentic way. So that will be our focus for this week.
------
Video 49. Week 3 - Day 1 - Crew AI Framework Explained: Agents, Tasks & Processing Modes Tutorial
------
So much as we started week two. We're now going to go through the core concepts of crew I, the sort of building blocks to understand how it works, and there's some similarities. First of all, there is an agent. An agent is basically the smallest the unit of work, an autonomous unit, which is related to an LM. It has an LM underneath it, and it has something called a role, a description of what it does, a goal and a backstory. And it can also have memory and it can have tools. And that is how an agent is defined. And you can remember back to OpenAI agents SDK definition. There is something new here. A task is another thing. And there wasn't an analog in the last framework. A task is a specific assignment that is going to be carried out. It has a description, it has an expected output, and it has an agent. So a task is assigned to an agent. So that is the distinction agents. And then tasks. And there can be multiple tasks for an agent, and then a crew is a team of agents and tasks, and that is simply so. A crew is the aggregate of the of the other two, and a crew can be can be has two different process modes. One of them is called sequential, and that is where it will simply execute each task in turn as it's laid out. And the other is called hierarchical. And that is where an LM will be used a manager LM in order to assign tasks to agents. So those are the two different modes that you can operate in. So that gives you these are the three core concepts to keep in mind. So how do we think about this. So so it's it's this these are lightweight concepts. And they're certainly reminiscent of last week I would say that it is a bit more opinionated than the OpenAI agents SDK. There's a bit more terminology. It's a bit more prescriptive. So as a clear example, if you look at the agent there, if you remember last week, agents just had instruction. An instruction was basically the system prompt. And you provided the instruction. And that's very unopinionated. You can choose to think about instructions however you want. Now with crew, an agent has a role, a goal, and a back story. So it's a bit more you can see immediately. It's more prescriptive in terms of how one is supposed to to prime the this, this, this LM, and how that is constituted into a system prompt is something that's, that's not immediately available. There are, in fact, ways that you can choose to set that you can sort of put in templates, but that's somewhat hidden from you at the, at the certainly at the beginner level, if you just use it out of the box, you have to provide the role, the goal, the backstory. And I think this is a really good example of the trade offs between these different frameworks, because obviously forcing us to think in terms of a role and a goal and a backstory is good, because these are all sort of good prompting practices to think about the context, to give the backstory this this is a a good, best practice at the same time. So so that's the benefit. The trade off is that we might not we might have a specific situation where we don't want to give a backstory that's not relevant. We want to think about it differently. And that is completely possible with AI. But it's going to be harder. We're going to have to then dig in and find out what's going on. And also if we're trying to debug a problem and we're not sure what's what's happening, we don't we don't have the same control over the system prompt. We just know that it's being constituted based on these various building blocks. So hopefully I give you this color because I want you to have a good sense of what are the trade offs in adopting these different platforms. And and that's a pretty important one. So a nice feature of crew is that you can do a lot of the definition of things like agents and tasks using configuration, which allows you to sort of nicely separate some of the text rather than having it embedded all over your code. So when you think about agents and tasks, those those are the two things that make up a crew. Agents and tasks can be created by code. You can say agent equals my agent, my agent equals agent. Open brackets and then pass in a lot of things like the LLM to use and the role and the backstory and so on. Or instead of that, you can write a YAML file that looks a bit like this. We'll look at many of them in the course of this week, but it's a YAML file. If you're not familiar with YAML, it's a very simple, easy for humans to read kind of markup file. And it and it looks a bit like this. And this is going to allow us to lay out a, a role, a goal, a backstory, and an LLM associated with an agent called researcher in this example. And that will allow us to separate out the configuration from the code. And whilst that is in some ways, as always there's some pros and cons. The, the con is that it's a bit of something to get used to. It's some scaffolding, it's specific to crew. There's definitely a great benefit that this means that you don't have your your various prompts buried within your code all over the place. You've got it nicely separated out and you can work on that somewhat independently. So. So I think it's a nice a nice touch in your code. You can then create something like an agent and instead of specifying all of the fields of an agent. Like the role of gold backstory. You can just simply select the configuration as as it shows in this code right here, and they will all be populated for you. So it's I like it because it's lightweight. It's not like there's magic happening behind the scenes. You clearly refer to the config right there. It's clear what's going on. And it has this nice separation of concerns, allowing us to work on our prompts separately from our code. And then in addition to the YAML files, there are a couple of Python files. And the most important one is called crew.py. And it's where everything comes together. It defines your crew and it's going to look a bit like this. And we're going to of course go and look at a proper example in a second, but it has some decorators over it and you're now you've got a bit of experience with decorators from, from last week. And uh, sure enough, crew has decorators as well. And you can see that they there is a few of them. It has crew base as the decorator you put around the whole class that's going to manage your crew. And then there's a decorator agent for each of your functions which creates an agent, and there's a decorator task for each of your functions which creates a task. And then there's decorator crew for your final function that generates your crew. And you can see that it creates an instance of crew and it returns and it passes in the agents, the tasks. And this thing that's telling it that it is sequential, not hierarchical. It's where you set that important different mode of processing. And you can see that there. It says agents is self dot agents. And you might wonder where is that coming from. Well that's really one of the roles of this decorator at agent that that is making sure that any function that has that decorator automatically, the agent that comes from that will be associated with the instance variable agents will be added to that list. So that's why you can just simply refer to it at the bottom. So anyway, this this will be more concrete when we look at a proper coding example. My main point is that there is this very important module crew, and it's where you define your agents, tasks and crew, and it uses decorators.
------
Video 50. Week 3 - Day 1 - Crew AI & LightLLM: Flexible Framework for Integrating Multiple LLMs
------
One of the things that's really nice about Cru is that it has a very flexible, lightweight approach to interacting with underlying LMS. It uses a framework called Lite LM under the hood, which it uses to interact with with the actual providers and with the LMS themselves. And I love Lite LM because it's so simple. It's so vanilla. You may know I have a somewhat love hate relationship with Lang Chain, which comes with with a fair amount of, of, uh, of structure to it. Lite LM is almost the other extreme where there's almost nothing there. You can just immediately connect with any LM you can imagine. And that really is reflected in the way it's used in Cru. Within Cru, you can just create an LM, you pass in a model name, and the structure is that it should have the provider's name followed by a slash followed by the model. And you can just use that to to be working with GPT four or mini with Claude through anthropic you can use three, five, three, seven, whatever you want Gemini flash, grok with a Q. And you could also use grok with a K in the same way. Olama if you're running the model locally, that's how you'd set it up and supply a base URL. And if you wanted to use Open Router, which itself is a kind of abstraction onto other LMS, but it's an actual service running. Then this is how you would configure Open Router as an example. But really the idea is very lightweight, very flexible, allows you to to really connect and switch around whichever models are running underneath your crew. And I would argue that in this way, crew really has an edge over open AI agents SDK, that this is really flexible and simple and I love it. And so the final topic before we get to do some coding and some crew making is to talk about crew AI projects. So in previous weeks we have just been coding within Python notebooks within cursor. And then occasionally we've gone to real Python modules. Crew AI doesn't work that way. With QR. You do need to work with Python code, and in fact, cry builds an entire project and directory structure for each of your crews. So it comes with a bit more scaffolding than than we're used to. And there's a particular way that you need to use it. Now, the crew AI framework itself is already actually installed because I typed this command here you've tool install crew I. And that means that when you've cloned the repo you've already got the crew AI framework right there. But when you want to create a new project, you actually want a crew of your own. You type this command crew, I create crew, my crew or my project or whatever to create that project. And by the way, as a side note, you could say crew, I create flow, my project if you wanted to use the flow idea, the workflows, the more fixed workflows rather than a crew. But we're going to be sticking with crews. So crew I create crew my crew. And what that does is it's going to create a whole directory structure that's going to appear immediately. There's there's going to be at the top, my crew or my project, whatever you call it. And then a subdirectory source src and within that subdirectory will be the name of your project again micro or whatever. And then under that it's a bit nested. Under that will be a directory called config which is where you put your YAML files. And we'll see. In particular by default there'll be an agent's YAML and a YAML, which is where we can put our configuration for our agents and tasks. And then there's a module crew.py. And that module is the one I just showed you, which is where it all comes together with the decorators and the way the place we actually create our crew. And then there's a module called main. And that module is where we actually kick off. We initiate the run. And when we actually want to run it we just type create run. And that will actually do that. It will execute main I think behind the scenes it just simply does. You've run Main.py and that does bring up a good point that this whole thing, when you do this, when you when you type create, create crew, my crew, it sets up a you've project crew uses you've which which makes which is great for us since we're using you've as well. But it makes a project. So you'll also see in this micro directory you'll see some of the the you've project configuration files as well. So we're going to be having these you've projects within our bigger you've project for the whole course. And that will make more sense when you see it in action. And what better time to see it in action than right now. Let's go and give this a try.
------
Video 51. Week 3 - Day 1 - Crew AI Tutorial: Setting Up a Debate Project with GPT-4o mini
------
And welcome back to cursor. And here we are in week three the crew folder. How exciting. Let's open it up to see what treasures I have in store for you. And it's completely empty. It won't be empty for you, but it's empty for me. Because this is starting from nothing. The way that some of you like me to do it. Which is, uh, gonna happen this time. Because that's the way that crew works. So we will begin then by opening up a command line like this, a terminal and crew, which you'll remember is control and the tick on a mac. That is really the control button on the bottom left. And, uh, then, uh, or it's the view menu and terminal if you're using menus. So then what I'm going to do is I'm going to first change directory to go into the third directory. Right now we are in crew and I'm going to create a new crew project, which I do by saying crew I create crew. And then the name of the project. And we are going to call this project debate For reasons that will become obvious, but probably already are, and it first asks us a question, and to read the question, I'll have to make this a bit larger. Um, and this is because when you first create a project, it puts in place, as I mentioned, some scaffolding, some sort of basic project framework. And in order to set that up, it wants to know which model would you like me to start with. So we're not necessarily fixing this, but we can start by saying open AI and we can always change it later. Select a model to use and we can choose GPT four or mini to keep it nice and cheap. And then enter your key. Now at this point we should definitely just press enter because we already have a dot env file. It's trying to build env files for us. We don't need that. We press enter and you'll see right here that it's created a bunch of files. And I'm going to remove the terminal for a second so that we can take a look at it, because they have appeared over on the left. Now one thing to get in mind is that when when you're looking at the explorer in cursor or in VSCode, if there's a directory like crew that only has one other directory called debate, then it shows like this. It doesn't bother showing multiple files, multiple folders under it. And that can be a little bit confusing. So we might go back into the terminal and just make a new directory. Let's let's make a directory and call it other. And when I do that you see what just happened. Now if we come in here, you'll see that there's both debate and there's an empty folder called other, which is going to make it a little bit easier on the eyes. Okay. So now let's have a quick look at what has crew created for us. So under debate, which is the name of our crew that we're about to make a debate crew, as you've probably guessed, there is a folder called knowledge and that has a file user preference, and that has some stuff about the person who's the the user. And you can change this. This would be background information that would be given to the model if we made it use of it, which we will not. So this is a sort of an example of some of the scaffolding that's created in case we need to use it. Then there is a folder called debate. Sorry. There's a folder called source. And under that folder there is a folder called debate. And again you're seeing it in this way in the explorer because there are no other folders other than debate under source. So sorry to recap. Under crew we have a folder called debate, which is the name of of the project that has a sub folder subdirectory source SLC and that has a subdirectory also with the project's name debate. And that is where we are right now. And this is the this is the most important directory of them all. This this directory named the project underneath source. And that has a few things in it. It has a config directory. The config directory has two YAML files agents and tasks. And here they are. And they're set up with some default example code in there that we will come back and look at in just a second. There is a tools folder that has just, again, some scaffolding, some some, some basic stuff here that we might want to fill in later that we won't do this time. And then if we come into the root folder of debate, you'll see that there is a file, a module called crew dot Pi which has a bunch of scaffolding again, and then a main dot pi. So there we have it crew, the module, which is actually the one which brings together our crew, and it has the decorators that I mentioned to you, main. And then the two YAML files under config that has all been set up for us, ready for us to build our first crew. All right. So we are now going to go and define our YAML, our configuration for our agents and our tasks, starting over here with the agents YAML file. And this contains some default some sort of scaffolding. Some example agents that are called the researcher and the reporting analyst are the two examples that's given. And we're going to change these to being what we're looking to build. And of course, we're looking to build a debate team. And in fact, we only need two agents for what we're looking to do. We want an agent that will be the debater. Our one agent is going to play both roles of being for and against the the motion. And then we will have a judge and those will be our two agents. And now we need to describe what they are. And so the, the role is the first thing that we say here. And we're going to say a compelling debater right there okay. And now the goal is what what it's it's looking to achieve the objectives. And so let's see I'm going to actually copy across one that I did earlier. So you don't have to watch me typing everything. But we're going to present a clear argument either in favor of or against the motion. And the motion is and this is important, this little thing in in curly braces. Here is where you can effectively make this a template. That is something that's going to get defined when you run this agent framework. When we end up saying, Q I run, we are going to specify what we want motion to be, and we're actually going to specify that in Main.py. So keep that in mind. In Main.py we're going to have to set what is motion, the motion of the debate. The thing that we're putting forward the proposal that they will debate. Okay. And so now we have a back story, which is where we set the the it's really we know that this is the system prompt or a big part of it. But in crew land, we want to think of this as the, as the sort of framing of this, of this agent, the back story. You're an experienced debater with a knack for giving concise but convincing arguments. The motion is so on. So that is the definition of our agent, our debater agent. And that will apply both for one that is presenting for an argument and against it. And of course we'll be using tasks to distinguish between them. But now let's define our judge. So we are going to give the judge a role. And the role we will say is decide the winner of the debate. Look at how cursor even suggests what this should be. Let's say based on the arguments presented, let's keep it short. And then the goal I will take one that I wrote earlier because it's a couple of sentences. You can see that that amazingly cursor comes up with natural language. It's just great for these things already. So you can even define your agents just by letting cursor describe them. And the backstory here I'm going to copy and paste in a nice juicy backstory, and here it is. You're a fair judge with a reputation for weighing up arguments without factoring in your own views and making a decision based purely on the merits of the argument. The motion is blah. So that's the sort of good way to set things like the backstory. And again, on the one hand, one way of thinking about this is that this is the kind of backstory that an LLM is going to take into account in setting the context for this role. The other, more scientific way is just to always keep in mind that what Llms are doing is predicting the most likely next token to follow an input. And the point is that if this backstory is part of the input, then you are increasing the probability that the output tokens will be consistent with this, because things that are seen at training time that have that kind of backstory often predict tokens consistent with that backstory. So that's the the more scientific way of thinking about why this tends to work really well. All right. And then the final thing I'm going to add in here is you can also specify what model to use. And you can actually you can just have GPT four or mini. Or if you want to really spell it out then you say OpenAI GPT four mini, but it assumes OpenAI by default for that model. What am I doing here? Model colon. There we go. Luckily, cursor to the rescue. And this then is our YAML definition of the agents. Next up is tasks.
------
Video 52. Week 3 - Day 1 - How to Create an AI Debate System Using Crew AI and Multiple LLMs
------
And now we move on to tasks. And here you'll see that there is a research task and a reporting task. By default. But we're going to change this and we're going to have a few different tasks. We're going to have a task called propose. This is of course going to be the task which is about proposing a motion. And so the description what is this task. It's going to be your proposing the motion and then the motion that gets passed in when we do that part. Come up with a clear argument in favor of the motion. Be very convincing. Then we're going to have an expected output, which is going to be quite simple. The expected output is going to be your clear argument in favor of the motion in a concise manner. The agent. So this is where you associate a task with an agent. And obviously this is going to be associated with the debater agent. And then there's one other thing we can do here which is an output file. We want to put this in a subdirectory output and we will call it propose.md. And so there you have our proposal task. So now we're going to have another task which I'm going to copy and paste. And this one is going to be oppose. This is the debater that's saying no description. Instead of proposing you are in opposition to the motion, come up with a clear argument against the motion. Looks like Kostas can help there. It does help. Be very convincing. Your clear argument against the motion. And that should be oppose. It's amazing, isn't it? So cursive does all the work for us. And now we have our final task, which is going to be called decide. And uh, the as a little tip here, something which is worth knowing is that you cannot call your tasks the same thing as you call your agents, or you will have a problem with conflicting names. So sometimes it's better to call this like proposed task, opposed task. But I've kept it short. But we couldn't, for example, call this task judge. Otherwise that would conflict with the agent that we called judge. All right. So description we will say review. Let me see. Now review the arguments. Here we go. Review the arguments presented by the debaters and decide which side is more convincing. And we'll just change this to delete all of this. We don't need any of that. We just need to say the expected output is your decision on which side is more convincing. And let's say and why. All right. And then the this should go please to a folder outputs and decide folder output. All right. That seems good. So there are our tasks. Okay. So the final step really is is the final steps is setting up pi and Main.py. And then we'll be ready to go. So this is the default module crew dot pi. And you can see it's got some stuff in here based on the standard scaffolding. It has created a class. And it's got this crew base decorator around it. And this class is named the same as the name of our project debate. So it's set that for us and it called it Debate Crew, which is exactly right. Now one of the things I dislike a bit is that it does generate all of this scaffolding code, all this standard code with lots of comments, and you can read some of the comments and follow the links. And it's got stuff like there's, there's ways that you can add in functions that get called at the beginning and end and stuff like that. But I do find that these comments are get in the way a bit, and I usually start by coming through and deleting everything in these just to keep it nice and clean. But what you'll see that it's done that is nice is that it brings in the agent's config and the tasks config. It just brings them in from the config folder right here. So those are set as variables for this class. And you can see how it refers directly to our configuration. And of course, that means that if you had different configuration files, then you could bring them in just like that. Then this is stuff about how you can bring in your own tools. And then this is where we set up our agents. So obviously we don't have an agent called researcher. We have an agent called debater. And the agent decorator is telling crew that this is an agent. It's going to return an agent, the config. We're going to want to change that to debater and we'll leave it verbose. True. Which means that we'll get a nice sort of print as it's running with what's going on. And then we'll have another agent and remember the name of this guy. Well, even if you don't, luckily cursor does and it's filled it in in both places. There we go. The judge is our other agent. All right. And now I'm getting rid of the comments about tasks. But it tells you about how you can do things like track outputs and dependencies and the like. But we're going to go on and define our, our tasks. They have a again the decorator task. And now we're going to have a task called propose. And there we go. Cursor fills it in. Now it has an output file listed there. But we don't need that because it's already specified in our config file. So that's not actually needed. And you know now that I look at this I do I have to say I think this would be cleaner if we have this all on one line like this, we don't need to be on multiple lines. And now you can see how very simple this whole function is going to look. And look cursor is going to do it for us. And then we can just delete that. Sorry I'm tidying this all up when you see how much better it is when this is in JupyterLab. And you don't have to watch me doing all this. So don't hate me. I'll be quick, but I think it's nicer if we have it looking nice and sharp like that. You see how nice and simple and clean this is? And that's the benefit of using the the config that we don't need to, to have too much here. And then the other task is oppose. And obviously it brings in the config for oppose. And then the final one is decide. And there we go. Thank you cursor for filling that in for us. So those are our two agents and three tasks. And we are down to the final section here. Okay. So now we now create this the crew. And this is, this is in the the function crew. And it has the decorator crew. And I'm going to take out the, the piece there which we will be coming to later. So we just simply have to create an instance of crew. We populate the agents with agents and as cry helpfully tells us, this is automatically created by the agent decorator. We populate it with the tasks that are similarly created. This is where we choose to be sequential rather than hierarchical in our process, and that we want to be verbose. And then there's some stuff about being hierarchical if you want to. And uh, look, if you want to use that nice little comment from the crew team in there. Okay. that's it for our crew object. We're almost finished. That was our crew module. We're now going to the main module where we finish things off. So this this is the again there's all of this stuff that you can read. This main file is intended to run your crew locally. So refrain from adding unnecessary logic. Gotcha. So inputs when we're running the crew, this is where we choose those template values that we put in our YAML file. This is where we choose what we want them to be. And so what we want them to be here is we want to have a motion and we want to give ourselves a nice motion. We don't we don't have this. We only have the one field motion. And what motion? What do we think that we would like them to debate? Let's have the motion is there should be a that was that was where I was heading with this uh, cursor. I must be super predictable, but it does seem like a good one to have our eyes A's debate. Let's not say there should be. Let's say there needs to be strict laws to regulate llms. There we go. That's a nice meaty topic for them to debate. And then we've got our stuff down here. We will come back to this stuff. We don't care about any of that. That is all it should take. We should now be ready to run our first crew just based on that. Actually, just before we run, let me make another little change that in this main module. I'm just going to make sure we print the result to the screen just to be satisfying. It will be saved to a file as well. Results equals debate. And then we will print result dot raw. That will print the raw output from the final task sent to the final agent. And then maybe one other little change I'll make is I realize we're right now sending both the debater and the judge to be OpenAI, but it would be more interesting to switch this one up. Let's let's flip it. As cursor suggests. Let's use Claude 37 sonnet latest. The latest Claude 3.7 model so that we have anthropic judging OpenAI's debates. And this also means that we'll be using anthropic this time, which was harder for us last time. Okay, so with that now I think we should be ready. We can bring up a terminal. Here it is. We will want to change into the directory for crew and then the directory for debate. And now we simply type crew. I run to kick off the run. And the first time you do this, it's going to do some UV stuff to build the environment. But I've already done it, so it's running right away. You can see it's executing tasks. The debater, each of the agents is running it's debate right now. It's the the debater for the proposer has already said the against is now debating. And now we're deciding on the winner of the debate based on the arguments. It is anthropic. That is thinking. And with any luck, we will soon see whether anthropic decides for or against the motion. Here we go. Let me see. In evaluating, I found the arguments in favor of the motion to be more convincing. So the view is that there should be strict laws to regulate large language models, such as the very one that is making the judgment here. Okay. So that ran. And look, that's quite a wordy response that came from anthropic. There you get to see in here the trace of the thinking that happened. And you'll see that an output directory has been created. And if we open this up and if I close this terminal so it's not in the way we can go into the output, and you'll see that there is weighty argument in favor of LM law regulation that you can read about about the ethical, safety and social challenges necessitating strict regulatory frameworks. And then the opposing thinking as well. It's no doubt going to be about hindering creativity. Yes. Very good. And I'm sure there's some compelling arguments there, too. And then the decision is the thing that we already saw printed a moment ago that came from anthropic. And so that is our first experiment into the world of crew. We set up our yamls. We set up our overall our our module for crew, the crew module. And then the main.py was where we set the motion, which was the thing that was templated in the various YAML documents. And we actually ran our debates with debates, crew kickoff passing in the inputs, this dictionary of the templated keys with their values. So I hope you enjoyed our first foray into crew AI, and I very much encourage you to do the same thing. Well, of course you will actually see this debates project in there, but you can go and make a second one. Or just run this by typing query run and get a handle. Get a good sense for how crew AI works.
------
Video 53. Week 3 - Day 1 - Building AI Debate Systems with CrewAI: Compare Different LLMs
------
And so to recap, we just experienced a crew I project, which is in fact a UV project under the hood. We created it by saying crew I create crew debate. It created this directory structure which now should land with you. You should be more familiar debate census, then debate again, then config. We set up our agents YAML to define each of the agents, including the model. We set up the tasks that included the expected output and the output file. Crew is where we had the various decorators, and we brought it all together with our functions for agents, tasks and the crew. And we said that the process was sequential. And then we typed query run within our directory and it kicked the whole thing off, generated the output. And it was successful. And apparently there should be stricter regulation around llms. I encourage you to try debating some more controversial points of your liking and choose different models. So yes, we'll of course be getting much deeper into crew. But in the meantime, the assignment for you is to now play around with this. You could have a separate agent for the the agent that is proposing and opposing the motion. Break that into two different agents and have the tasks going to them separately. And the reason to do that is that then it's easy to have a different model. So that you could have OpenAI debate with deep seek or something like that, and then switch who's opposing and who's proposing to see whether it changes the outcome. And that's an amusing and entertaining way to battle Llms together, to see which are better at forming coherent arguments, persuasive arguments that convince a different model that is being the judge. So that allows you to come up with your own little leaderboard based on debating skills. So please go away and enjoy yourself with that. Get a good handle, get comfortable with the framework around crew and the sort of minimal scaffolding there. And next time we'll get a little bit deeper into crew and start building some more crew projects. I will see you then.
------
Video 54. Week 3 - Day 2 - Building Crew AI Projects: Tools, Context & Google Search Integration
------
And a very warm welcome to day two of week three. Our second day playing with crew, this time continuing doing some building and exploring the crew framework. To quickly recap what we did last time we learned about an agent. The agent being the smallest autonomous unit. It has an LM associated with it. It doesn't actually need to. You can have an agent with that LM, but they typically do. It has a role, a goal, a backstory. And it also has memory and tools. Not that we've looked at either of them just yet. And then a task. This is the concept which doesn't have an analog. In OpenAI agent SDK. A task is an assignment to be carried out with a description, expected output, perhaps an output file, and it's assigned to an agent. And then a crew, which is a team of agents and tasks together assigned to those agents. And they can run sequentially or hierarchically, in which case you'd have to assign a manager LM to figure out which task is assigned to which agent. So that's the overall structure of crew, which now should be pretty familiar to you. And you'll remember that there are five steps that we went through when we set up our first crew project. First of all, we created a project. We did create Create Crew, my project, or we did debate, but it could be whatever we want and it will set up that whole file system structure for us. We then go into source and the name of the project and then config, and that is where we find the YAML files, which we can call them whatever we want. But by by by default there are two there one for the agents, one for the tasks, and we fill them in with the details. We then go to the crew.py module in our source project folder, and we create the agents, the tasks and the crew using the functions that are already set up there for us and using the decorators. And we reference the YAML config, although we don't need to, we could actually manually create a pass in those fields when we create each of those objects. But the config file makes that easy for us and keeps some of this prompting text separate from our code, which is a nice separation. Then fourth, we update the Main.py module to set the config to set up the run parameters that we did when we specify the inputs. The fact in our case that the topic of debate the motion was, as it happens, about AI regulation, and then we can run and the way we run is we type crew. I run from within the project folder, which behind the scenes does a run, and then it's off to the races with our crew framework. Okay. So we're going to go a little bit deeper in two ways. And we're going to set up another project today. And one of those ways is with tools, something that we're very familiar with already equipping agents with capabilities. We will see how to do that crew AI style And then the other one is about context. And this is how you tell crew what information is to be passed from one task to another. So these are the two extra details we're going to get into today when we build our second crew. And let's go and do that right now. So just before we go into crew, there's another of these APIs that I need you to sign up for, but you'll be pleased to hear it's another free one. And this is going to be quite a contrast with our experience with OpenAI agents SDK. We're going to use something called Serpa, which is a fast way to run Google queries from code. It's an API to run lightning fast Google search at an unbeatable price, and indeed it is unbeatable. So you go to Serpa Dev and you sign up by pressing that button there, and I've already done so and so I can sign in with my account. And here I am. And the thing is that you get 2500 free credits, which is more than enough for this course, and this will allow us to do plenty of searches quite happily indeed. And you will go to API key, which I won't do. Now you'll get my API key, but you go there to create your API key and as usual, copy it into your clipboard because that is something that you will need for your env file. And in your env file, you should have entered that in as Serpa underscore API underscore key cursor will actually prompt you for that if you start typing it. Serpa API key. And if you're interested Serp where that comes from. Serp stands for Search Engine results page, which is the common name for these kinds of services. And there is another one that's called something similar to Serpa. So be sure that you use Serpa API key. Not I think this one called Serp API or something, which is different. So be sure to have the right key there. Okay, so with that we are we are still in the debate folder. We will close that. Here we are in our crew folder in cursor for week three, and we are going to go into bringing up a new terminal. It's still got what we had before. Let's let's clear this let's exit that. Let's start a fresh terminal. Here it is. We're going to go into the third folder crew. And it's time for us again to create a new project. Do you remember how to create a crew project? Well here you do it. It's crew. I create crew again, as opposed to a flow, which is different. And then we're going to make one that's called financial researcher, somewhat inspired by the default one that's already there. Let's have a financial researcher. Let's create that right now. So we as before, we just choose OpenAI and GPT four or many. And we skip setting up the key. And it has created the financial Researcher project and a bunch of scaffolding for us, which is great. And this is where we will get started.
------
Video 55. Week 3 - Day 2 - Building Multi-Agent Financial Research Systems with Crew.ai
------
Okay, so let's close the terminal. Come into financial researcher. You know the deal. Now there is a source folder. It has a config. And that is where we want to begin. We want to begin by looking at our agents. So this is the agents. And we are in fact also we're going to have a researcher agent. And we're going to make this a bit shorter and just have an analyst agent, a researcher agent and an analyst agent. So what are they going to do? So this time I'm going to to simply copy and paste a whole new section here and talk it through with you. So the role is going to be a senior financial researcher for a company that we will specify in the inputs like before, it's going to research that company news and potential for that company. The backstory. You're a seasoned financial researcher with a talent for finding the most relevant information about a company known for your ability to find the most relevant information, present it in a clear and concise manner. And that's where we specify the LLM, which we can we can mix this up if we wish. And we now choose to have our second second agent, the analyst. Let me put in what I've got here. Market analyst and report writer focused on a company. You analyze the company and you create a comprehensive, well structured report that represents insights in a clear and engaging way. Meticulous, skilled analyst with a background in financial analysis and company research, a talent, etc., etc. it's interesting that this construct of having us tell the back story, it really does encourage better prompting. If I were just writing a system prompt instructions as we did last week, I probably wouldn't think through expressing myself this way in terms of using a word like meticulous and meaningful insights and so on. So it is helpful. Putting us through the discipline of having to give a backstory, helps give more context, and helps make sure that we're going to get the best outputs, the best outcomes from running the LM. All right. So with this, let's now talk about our tasks. Okay. So we're moving over to tasks. And we are going to have a research task and an analysis task. So it's not too different to the the boilerplate one. But we are going to do a little bit more work here. So for the research task going to really spell out what it is that we want this task to involve. Conduct thorough research on this company. Focus on the current status and health, the historical performance challenges and opportunities, recent news, future outlook, potential development. Make sure to organize your findings in a structured format with clear sections, and then the expected output is a bit repetitive, but it says what we want the well-defined sections and we again mention the input the company. And that is going to be assigned to the researcher. Okay. And the analysis task. Now you'll notice there's no output file there. So you might be thinking what what's going on here. Well let's see. Now let's give this a description that is going to be quite detailed. Again analyze the research findings and create a comprehensive report. The report should. And then we have the different sections in the report. Executive summary information insights market outlook and be formatted in a professional style. And for the expected output will again be a bit repetitive here. There's never a harm in being repetitive. Polished professional report on the company representing the research findings and the agent that we want to mention here. Of course should be the research. This is the the sorry the agent is the analyst. That's the agent that we that we made for the analysis task. Of course. Okay. So there's a few more things that we want to add here. We want to make sure that the output from the research task is included as part of the analysis task. And crew gives us a super easy way to do that. You just type context. And yes, of course cursor tells us what to do by saying that context. Context can take a list like this. And we are giving the research task as part of the context that's needed for the analysis task. And just with that simple step, we ensure continuity and we ensure that the output is included in the research task. And then finally we are going to have an output file for this. And we're going to let's let's call that report.md in the output directory. That seems fine. Okay. And now to the all important crew dot I. And here we have as usual, some gunk. We have the financial researcher I am. I'm going to be bold here. I'm obviously I'm going to leave in oops. Say that and I delete it. I'm going to leave in the config file that we want. And I am just going to delete everything else. And then we're just going to rewrite it. You want me to rewrite. Well we're going to rewrite this entire file right now okay. So what do we want here. We want an agent. So we put in the decorator for agent. And uh let's see. Well let's just press tab and see what we get. We want an agent that is the researcher. That's correct. We probably want to say verbose is true. There we go. And then we also want an agent that's called the analyst. And that is going to be an agent that is going to what's happening all over the place. Sorry. This is an agent and this should be an analyst. And now we're getting help from cursor. That all looks great. Okay, so now we want to have a couple of tasks. So we want to have a task that is called the research task. That sounds right. It's going to be a task that is going to the research task. That's perfect. And the analysis task that looks good to me as well. So all is well. And now we want the crew and yes thank you. Very nice cursor. It's filling it all in for us. We want the process to be the process sequential and verbose to be true. Crew cursor is faster than me. I'm playing catch up here but that is perfect. So remember self dot agent is populated. Sorry. Self dot agents is populated because we have agent decorators around our agent functions methods and then tasks is also decorated here. So everything looks great to me. Okay, now we go to Maine, and I think, again, we're going to want to, uh, delete all of this and start again. And I will type it for you manually. Okay. Here we go. So we're going to delete the gunk. We are going to delete everything but just only leave in run. That's going to be it. Okay. So run the researcher crew. Isn't it amazing that cursor realized knows that our input is a company that that inputs. We did indeed just have company as the templated curly braces thing. Just in case you don't know what I mean. I mean, we have this right here. Company is the one input that needs to go in to our YAML. And if we go back to main, then automatically cursor realized that we needed to set company and populated it there. All right. Then we're going to say yeah Result is the financial researcher. And that all looks great to me. And then we're going to print the results. Thank you. Cursor. And that's really it. I think that's all we need. Now you might be thinking hey, didn't we set up Serpa? Are we not going to use that for something? We are going to use it for something. But all in good time. Let's first just try running this without making any any proper Google lookup and see what happens. Okay, well, just before we run it, let's let's change the models. Let's give ourselves some more interesting models to experiment with for our researcher. Let's go with deep seek. Now of course you could do whatever models you would like here. This should be deep. Seek chat. That'll be the right one for us to use. And you could stick with OpenAI. Or you could use a different model, or use a llama and run it locally if you wish. I will try a grok model for this one. Let's use this this here llama three 370 billion. Versatile. That's one of the most powerful open source models from from meta. And we'll use that via grok the high performance inference. You can also run something locally like use a llama to to run llama 3.21 million or 3 billion version of it. So that's just a mix up the models to try something a bit different. And now let's go and open up a new one of these guys a new terminal. Let's make it nice and big. And we will go into project or folder number three for crew. We will go into our financial researcher. And you remember we type crew I run to kick this off and we'll see what happens. It's thinking now I will tell you that uh, you'll see, by the way, that the agent has been assigned the task financial researcher for Tesla. That shows how that those inputs, those parameters automatically got set as the, the, uh, the agents, uh, roll. Now, I'll tell you that the deep sea takes takes its time over this. It takes a good 30s, I guess, because it's producing quite comprehensive research as part of this. So we will allow it to do its thing. Maybe next time we should have them both be grok so that it's nice and fast. But it's going through. It's, uh, we've gone off to deep sea guys servers where it is. The 671 billion parameter model is busy at work, and once it's finished, we're then going to flip across to our other agent, and our other agent will use grok running in the cloud a 70 billion meta model. And then we should get our financial report on Tesla at the end of it. Here it is. Grok was so quick we didn't even see it. There's uh the grok taking the task and completing it. Here is the result. Our financial report on Tesla. It all looks great. Wasn't that easy?
------
Video 56. Week 3 - Day 2- Enhancing AI Agents with Web Search: Solving the Knowledge Cutoff Problem
------
And of course, the key point for us to have focused on here is that this second agent that did the summary of the research report on Tesla was taking advantage of the output from the first agent, because that was included in its context. And that's how it was able to give what it gave. And you'll note, if we look into this, that it's as of October 2023. And that's a bit disappointing because that's clearly not right now a key financial metrics Q3 2023. And yeah, we're not we're not super happy about that. And that is, of course, because we're relying on the context on the on the knowledge cut off from deep seq that did our research and deep seq didn't have more, was last trained back in 2023 and doesn't have more recent information, which is unfortunate, but something that we can now fix. So the way we're going to fix it is by adding in a tool, which is the big plan for for this week. And it's going to be quite easy. So we do that by going back. Let's close this and go back to the other crew module which is where we define our crew. And we are going to add another import here. We're going to import from Cru AI tools. And yeah it's there's not going to be much of a job for us in, in the not not too distant future. So yes indeed. Clever old cursor knows that we want the Serpa dev tool here, which is indeed what we want, which is a tool from crew AI that's able to do Google lookups using our Serpa dev account. And so you need to put your Serpa API key into the EMV file. But now the now is the challenging, the difficult task of giving the researcher the ability to use that tool. That's what we want to do. And the way we're going to do that is we're going to say that we only want our researcher tool to have it. So it's not difficult at all. It is really rather easy. We simply create an instance of the Serpa dev tool, and we pass that in in this tools list right here. That's all it will take. I save that and we should be good to go. Maybe we'll go back to models, though, and pick a different model. Let me see. We're going to go back to agents. And why don't we just use OpenAI, GPT four or mini so that we have a faster time of it? Okay, let's give this a shot. Let's bring up our terminal. Let's exit that one actually. Let's just start again. I could have typed clear. We go into the third directory into crew. We go into our financial researcher directory and now we type crew. I run and we're hoping now that it's going to go and look up using that tool. It's going to look up Google about Tesla. And we're going to get some information. Financial researcher in progress. There's something happening here. There's lots happening. Search the internet with Serpa. You can see it's doing its stuff. Plenty of search is happening. Tesla latest news today. I see 2025 appearing in the search results. This is promising. This is good news, but we'll soon see. So the researcher is still working. It's now on the report writer. We're now talking to grok. And here is the summary. Here is the report. And it's great to see we do indeed have a report as of early 2025. And we've got the most recent news from Tesla hasn't done as brilliantly as usual in the last month or so in the last few weeks, including. It's definitely got relevant recent information in this report. And it is, of course, a clear and accurate report. And, you know, it's fantastic to see this stuff. And I guess the the usual things for you to appreciate. One is that this is actually a high quality output. This is something that would genuinely, legitimately be useful if you spent about ten minutes, 15 minutes yourself searching the internet, doing some searches, gathering some information, synthesizing it into an email like this, you probably would come up with something quite similar. It's it's it's proper work that's been done with a good output. And the other part of this is how easy it was to build this whole infrastructure. Thanks to crew, thanks to just a couple of of commands, building this and then writing in plain English in these YAML files, Our objectives for our couple of agents and tasks, and then just giving it the tool, it was able to handle all of this. And unlike OpenAI's costs of 2.5 cents per lookup, these costs have been coming out of our free credits. And so this has cost us nothing. Okay. Well, I hope you enjoyed that and I hope you've done it yourself and had a similar outcome. And I will now see you for the wrap up. Well, I hope you enjoyed that. And you've tried it yourself and experimented with some different models. If you're using a llama, experiment with some different free open source models, try messing around with the instructions and the backstories and see what it what it takes, what sorts of differences you can make, and try other kinds of quick agent projects that involve using searches. And you're getting more and more familiar with Crewe. And I imagine, like me, that you quite enjoy using Crewe AI as well. It has a lot to love. All right, next time we're going to take this project a step further, and we're going to experiment with some of the more advanced features of Crewe. See you then.
------
Video 57. Week 3 - Day 3 - Building a Crew AI Stock Picker: Multi-Agent System for Investments
------
And welcome to crew week three. Day three. It is time for us to build a new project. The stock picker that I'm looking forward to showing you a quick reminder on the five steps that we use to build a crew project from last time we use crew, I create crew. We fill in the YAML files, we complete the crew module, we update Main.py to set any inputs for doing the run, and then we call crew I run. This time we're going to be going deeper again in three new ways. First of all, structured outputs will have a comeback. We looked at them last week. We'll do them again with crew. We'll use a custom tool in addition to the tool that we'll use again. We'll also build our own tool. And then thirdly, we will try out the hierarchical process allowing crew to manage the process of what task goes where. And as you'll see we'll have a bit of an adventure with that. All right. Let's get started. Okay. Here we are back in cursor. Back in Project Directory three crew. And let's open this up and we will bring up a terminal window again. As before, we'll go into that directory and we're going to create our new project which is of course crew I create crew. And then the name of the project which is going to be Stock picker, a project to create and recommendations for investing in the stock market, bearing in mind that this is purely for our own investigatory purposes and you should not use this to make any trading decisions, please. Okay. We're going to select open AI as our provider. And we're going to choose GPT four and mini. And we're not going to set up any env variables. And the crew has been successfully created. All right. It's now first of all time for us to open up our stock picker. And as usual go into source, go into config. And the first step is to create our agents. So we will go ahead and do that now. Okay. So here we are in agents YAML. And we're going to create the agents for this project. And there's going to be a few of them. So I'm going to bring them in an agent at a time so that we can talk them through. So the first agent is called the Trending Company Finder. And it's responsible for looking in the news and finding trending companies in a particular sector. So you read the news. You find 2 to 3 companies that are trending for further analysis. And we'll use GPT for a minute here. But you can of course substitute in whichever model you'd like. And it might be fun to play around with different ones. The next one we will use, and let me make sure I paste this in properly. There we go. Is a financial researcher the financial researcher. Given details of trending companies you provide comprehensive analysis. So financial expert with a proven track record of deeply analyzing hot companies. Again we'll go with GPT four mini as the backstory. Okay one more agent that we're going to work with, and that next agent is going to be a stock picker. So we've done we've we've found trending companies in the news. We've researched those trending companies. What's left to do is to pick one. So the stock picker given a list of research companies with investment potential, you select the best one for investment, notifying the user and then providing a detailed report. Don't pick the same company twice. You're a meticulous, skilled financial analyst with a proven track record of equity selection. These are all great ideas for well crafted prompts to be using. And again, we'll use GPT four mini for now and let's save that. I'm going to add in one more agent as well. A new agent for us to be exploring this time. And the new agent is going to be a manager. But I'm going to keep this manager very simple. The role is a manager, and I'm saying you're a skilled project manager who can delegate tasks to. In order to achieve your goal, which is your goal is to pick the best company for investment. And that's it. So very simple vanilla description of that agent. And therein there is our four agents for this project. So far so good. You like those four agents. It's time for us to define the tasks. And when defining tasks the trick is to be very clear, very simple. And so the first task is a find trending company's task. Find the top trending companies in the news in this sector by searching the latest news. Find new companies that you've not found before and the output, a list of trending companies in that sector. And we are going to say that assign it, of course, to the agent. The trending company finder is the agent that will work on this, and it will put it in an output file. Trending companies in output that. You may be wondering why that says JSON. It will become clear in one second. Okay. So then going to have our next task. And our second task is going to be a research trending companies task. The description is given a list of trending companies provide a detailed analysis of each company in a report. And the agent of course will be the financial researcher. The second agent that we made. So this this 1 to 1 correspondence between a task and an agent right now. And we're giving it we're telling it to have some context. And the context is the find trending companies task. And we're asking it to put it in an output as well. And you can see that already cursor is one step ahead. But we're going to ignore cursor. And I'm going to put it in myself. The pick best company task is the final task in the list. It's of course assigns to the stock picker agent, and it is to analyze research findings, pick the best company for investment, then send a push notification to the user. That's a new one. You can imagine that it's going to be the tool that we will add, a tool that we've used before. But the first time in crew and we'll, we'll ask for a one sentence rationale and then respond with a detailed report on why you chose this company and which companies were not selected. And then the agent I just said was a stock picker. And the context is of course, the research trending companies. So there we have it. We've now defined our tasks. And again, it's worth noticing how I've made these prompts very instructive, very crisp. I've used consistent language with things like trending companies. All of these, these are all small steps that help make sure you get coherent responses. Although as you'll see, it's not going to be perfectly coherent. But it definitely helps. And I've experimented with this a fair bit. And in previous versions, when I was iterating on this, I had inconsistent language between agents and tasks and definitely causes less stability. So I think that's a that's a pro tip for you.
------
Video 58. Week 3 - Day 3 - Implementing Pydantic Outputs in Crew AI: Stock Picker Agent Tutorial
------
And now it gets juicy. Now we're going to go to Crewe and we're going to start building a bunch of different things. So the first thing is that I want us to use structured outputs. In other words, we are going to ask our different tasks to be providing information according to a particular JSON schema as a way of making sure we're getting the information that we want from these agents in a robust sort of way that is kind of on rails. And so, as you probably remember, the way to do that is to create classes that are subclasses of base model and use this as a way of describing what we want. So let me give you an example. Let's make a class that's called trending company. And so this is a subclass of base model. We give it an explanation. It's a company in the news attracting attention. And then we set up name ticker and reason. And we give them descriptions like this. And you can see this is a way of, of laying out the information that we're going to want to be gathering. And it helps guide the agents and the tasks to, to to produce information that we want. And now I'm going to make a second class. That is basically just a list of these. So it's called trading company list. And it contains a single field companies which is a list of these objects. It's a way to organize it so that one task can result in a bunch of trading companies. So far so good. We're going to do pretty much the same thing again for our research. So we're going to have a trending company research schema. Here it is. So again it's a it's a pedantic class a class that is a subclass of base model. And it's detailed research on a company. It has a name, market position, future outlook and investment potential. And by giving the fields with that name and with that description, we are going to force the agent to produce that information in its response. And so it's a really clever way of making sure that we guide the agent's behavior. And similarly we're going to add a list trending company research list, a detailed list of of all of the research on the companies, and it's a list of trending company research objects of these objects with these descriptions. And again, I find it helpful to be mindful that you use consistent, clear terminology. I used to have. I used to my first version of this, I called these newsworthy companies, and I was just introducing concepts that weren't crystal clear in terms of what I was going for. And and this has helped make it more reliable using simple and and common terms and using them consistently. Okay, now it's time to define our crew, our class stock picker. And as usual we look at the YAML files. We bring them in here. And I've deleted the rest of the gunk that it auto generates so that we can build this ourselves from scratch. And we're going to start by creating our trending company finder agent, which I'll do like this. And we need to give it of course the decorator agent like that. So this is a trending company finder. It's going to return an agent configured with the config and we're going to use a tool which will be let me tidy this up for you. There we go. It's going to use the super dev tool as before so that it can look for trending companies on the internet. That seems pretty good. All right, let's make our second agent. And this second agent is going to be our financial researcher agent. And so let's see we're going to want to again use Sirpa. So we're going to have it look at the config again for the financial researcher and the tools we. I was wondering what was going on there. I was missing the deaf deaf financial researcher like so that's going to have an agent. And there we go. Now it should work fine. All right. That seems good. And it has suggested cursor has kindly suggested that we do this, which seems good to me, except we don't really need this. We don't need to have the Sirpa dev tool in there, because the stock picker has been given all of its information, it's not going to need that at all. All right. So that is the definition of our three agents. Let's go on to defining our tasks. Okay. So it's time to define the first of our tasks. And the first task is the task to find trending companies using the trending company finder. And here it is. Find trending companies is creating a task. This is the config. And we also have this field output pedantic. And that is telling it that this task needs to output some JSON in the schema that conforms to trending company list. And that of course is this object right here. So we are constraining it to make sure that it produces something in that format, which is exactly what we want. And so this is the way that you do structured outputs in crew. And now the research trending companies task. This is going to again be hooked up to the right config. And this of course is going to create the trending company's research list. Pedantic object. So here we go. If we look back up, this is, of course, this object right here. A list of research results. So we are saying this task to research is going to be forced to produce information that conforms to that schema. And that way we know we're going to get research. Hopefully you're following along with this. If not then do look at the code and try this out for yourself. We're almost done. We're just going to add in the best company picker, which is right here, and which is going to select the best company. And that's an easy one, an easy one to end with. And now we've got left to do is actually create our crew. All right. Time to create the crew. We've made the agents. We've made the tasks. Here is the crew. You may be thinking to yourself, we only created three agents. Did we have a fourth agent? Yes, we did, but that fourth agent is a bit different. It's the manager, and we don't want that to be in the list of the of the, the general agents that are going to be working on the task at hand. We're going to want to create this separately and handle it separately. So we create our manager agent like this just as a, as a as a separate variable manager, an agent that has the config from that's called manager. And we also use this field allow delegation equals true. That is telling crew that we want it to be able to delegate to other agents. That would be the equivalent of the handoff in the OpenAI agents SDK. All right. And so now it just remains for us to actually return our crew from this function here from Def Crew. And we're going to do it like this. Return crew. The agents is self dot agents. And that is a three agents with the decorators above. Those are the agents that form this team, this crew. The tasks are the tasks that we defined. We're now saying the process is process hierarchical not process sequential. And that means that we are going to assign an LM to figure out which agent does what task. Verbose is true. And this is where we specify the agent. It's the agent we created right here. We could just just create that agent right there in the code. But it's a bit neater to do it this way and that is the end of it. Now there is actually an alternative here. You don't need to create a separate manager agent. You can actually say manager LM equals and just define an LM like GPT four or something. But um, I found it perform slightly better if I get the opportunity to describe the role of the manager. But both did work. But as you'll see, neither works perfectly. Haha. It's an adventure. As I say, it's a very interesting insight into some of the challenges of of autonomous AI, but still it works better if you define the manager separately. And one other point I'll make is that in defining that manager, I don't know if you spotted this, but I'm actually using GPT four, not GPT four mini. I'm using the bigger version, which means it's a slightly more pricey, so you can make that for a mini if you'd prefer, but I found that that helped it stay more coherent with the mission at hand. So anyway, that is defining the crew.
------
Video 59. Week 3 - Day 3 - Custom Tool Development for Crew AI: JSON Schema & Push Notifications
------
Okay. Well, it remains for us to write the the main the run function in here. I deleted the default template one that's there usually, and I'm replacing it here with a simple run. The crew we're going to pass in the sector as technology. We don't actually need a current date in there at all. That will do fine. We don't use current dates and the result is Stockpicker kickoff passing in the inputs. And then we will print results at the end. So there we go. This is our starting point. I think we should give this a whirl okay here we go. So we're going to bring up our terminal as usual with control and the tick. And then we're going to go into our third folder. And we are going to go into the stock picker. And we're going to type crew. I run to kick it off. And now here's the thing. It's going to be interesting to watch this. The crew process is, by its very nature, a little bit less predictable than we might want. It is autonomous and it goes off and does its own thing, and that can involve all sorts of searching around and using multiple agents. It can sometimes go back to do some news and analysts and then using the researcher, and it can take quite a while as it goes through its processing. And my machine is chugging away like crazy, as will yours. And it's both. The great thing, and perhaps the downside of autonomous agentic AI, that we have a bit less control over how this process is followed, and so I will let it do its thing. I will break and come back when it completes its recommendation. And here we have it. It actually completed pretty much right as I was pausing there. So it was quite quick. It made a decision to recommend anthropic and it says, which was interesting because it was OpenAI that was doing this, this processing, which is and it turns down a couple of a security company, Peregrine and a crypto company, circle, and we can look in our outputs folder where we'll see that this decision is shown. And we'll also find a research list here. And also the trending companies list. And these of course are in the JSON format that conforms to our schema. So we are seeing here the proper formatted results because we required that by using structured outputs. And so despite my misgivings about the autonomous framework it actually performed really well. It did just go through the process. It should do. The manager that we'd set up had some autonomy in how it assigned out the tasks, but it did so well and as a result, we did indeed achieve a stock pick recommendation, which is not intended for real trading decisions, which is in this case for anthropic. All right, it's time to make our solution have a few more bells and whistles. First of all, we're going to add a tool. You'll see if you look in the source folder in the stock picker folder that's generated for you that there's already a folder called tools that the people at Cru have kindly made for us. And there's one in there called Custom Tool that has a kind of typical layout for these sorts of, of tools. And we are going to change the name of custom tool, and we're going to make it into push tool. So let's do rename and make this a push. Hang on click there. Try that again. Rename to push underscore tool dot pi. So the way that it works if we look in here is that you'll see that there is a when you set up a custom tool you have to first describe using a pedantic object, the schema of what will be passed in to your custom tool. And then you end up writing an underscore run method, which is going to take that schema as its parameters. So to make that feel more real, I'm going to, uh, actually implement that. And let's just start with a couple of, uh, imports here and now. So we're going to have a push notification input which will be a subclass of base model. And we're going to change the description that this is uh what is the meaning of this input. It's going to be a message to be sent. Sorry to be sent to the user okay. And now instead of an argument we're going to say message. That's what we're going to call it. And the description will be the message to be sent to the user. That seems pretty clear. Okay. So that is then defining the schema for our tool. It's in other words this here is actually going to be message to to correspond with that. That's what we will run when the tool is invoked. So or invoked rather than invoked. So uh we will um give it a name. So first of all we should change the class to be push notification tool. There we go. Thank you. Cursor. The name let's say is going to be send a push notification and the description is going to be this tool is used to send a push notification to the user. That seems pretty decent to me. Okay. And the schema for the args. This is where we say what kind of arguments do we need you to pass in. Well, that's exactly the schema that we've just defined right here. So it is something which just has a single argument message. And there you go. You see we've written a run method with a single input message. And that is going to actually send a push notification. And how do we send a push notification. We use the fabulous Pushover tool. And hopefully in your environment you have Pushover user and Pushover token set in your env file from prior week and then I just used I've just pasted in exactly the same code we have from the prior week, and we will push this message to the user and then return that that was okay. So that is our push notification tool. Now we have to put it to good use. Well this should be pretty easy actually. We're back in the crew.py module and we add in an import to import in the push notification tool. Now which of our agents do we want to be able to do this. We want the stock picker agent to be the one that will have this power the stock picker. You can see cursor of course already tells us that's exactly what we want. We want the stock picker to be able to call the push notification tool. That's as simple as that. All right. With that, let's go and run this. We'll do that right away. We will bring up this. We will clear what we have here. And we will just call, cry, run and give it a whirl. Let's see what happens. I will let this run and I'll be right back. And here we go. It's completed. It's this time recommending circle, which was one of the runners up last time. And I can assure you, you can see it right here that I did indeed get a push notification about the, the the opportunity, which is great. So it is working. And that then is the extra piece that we've added into this. So just to recap, we built this agent framework and the three things that are different than the way we've done it before. First of all we've used structured outputs. So we've required that tasks respond conforming to a JSON schema that we set. Secondly, instead of using the sequential process, we use the hierarchical process, which means that we can either pass in an LLM by model name or by passing in an agent that will take care of assigning the tasks to the agents. And we did that. And we saw, I think, both the good and the bad of doing so. And then thirdly, we added a custom tool, a tool that we wrote ourselves. And it's a familiar one. It was to send a push notification. And we armed an agent with that ability and it pulled it off successfully. And so that then is wraps up this week's project. And I'll see you for the wrap up. I think I might have just said that. That wraps up this week's project. Of course, it wraps up this day's project. We've got lots more to go. And in fact, tomorrow and in the next day before we move on to the next project, I do want to add one more bell and whistle to this project too. So we're not completely through with the stock picker either, so stay tuned for that. But again, to recap, we just did structured outputs, custom tools and hierarchical process. And tomorrow we're going to we are going to add a little bit extra to stock picker but then also work on the next project, the developer agent. And I'm really excited about that one. Good old crew is coming through strong. See you then.
------
Video 60. Week 3 - Day 4 - Crew AI Memory: Vector Storage & SQL Implementation for AI Agents
------
And week three. Day four is a go. Let's get started. So last time we did a stock picker and we just have a little tiny bit more to put into that. And then we're going to move on with our next project. A developer agent. But first another repetition. I hate to repeat these things, but sometimes it's good to drum it in. Building a crew project involves five things. First of all, crew I create crew. The name of the project to set up your crew, build those directories and those files. Number two, you find the YAML files for your agents and tasks, and you fill them in to define your agents and tasks. Number three, you go to the crew.py module, which is where you actually create the instances. And you use decorators to identify the agents and tasks that you'll be using. And then you create your crew itself. And this is where you can have structured outputs to make sure that the outputs conform to a schema. And you can use tools, both tools like Serpa, that crew provides for us that run remotely, and then custom tools that we build locally, like the thing that sends a push notification. And then number four, you update Main.py to set any inputs so that we can pass something in, configure the fields that are templated with the curlies. And finally we run with crew. I run and off goes our project. So I now want to cover a feature of crew called memory. And this is a feature that is a bit more prescriptive, a bit more opinionated in the crew framework. Memory, of course, is talking about how you provide information, contextual information to Llms each time you call them, and you can implement that yourself just by storing variables and then passing them in when you do things like creating tasks so you can do it the sort of manual way. But the crew framework also comes with some building blocks that lets you use their constructs around memory out of the box, and that comes with pros and cons. The pro is that you get up and running quickly, and you can use a lot of the thinking that they put behind this. The con is that there's there's a learning curve, and it obscures some of the detail of how prompts actually work behind the scenes. So as as with many times when you're adopting a framework like this, it's something to be aware of the benefits and the trade offs of doing so. But let's say that we are going to embrace cruise way of handling memory and talk about what it actually does. Well, it has five different types of memory, five different frameworks that you can include. And one of them is called short term memory. And this is just about storing recent interactions using a vector database in in a in a rag way. If you're familiar with retrieval augmented generation. And you don't need to be for this course because we're just going to put the code in there and see it run. But if you do no, then this will make more sense. So this will allow agents to access recent relevant information when they are currently executing. And then a different concept called long term memory is when more important information is stored in a SQL database for longer term recall to build up knowledge over over a longer period of time. And then there's something called entity memory that's very similar to short term memory, actually. It's it's basically when there's things about people, places and concepts, then those can also be stored in a Rag database for vector based similarity search and to be included in the context. And then there's crew describes this as another kind of memory. But I think it's a bit misleading. I think what they call contextual memory is just a sort of umbrella term for the short term, long term and entity memory that can all together be queried and passed in as context when prompting an LM and crew abstracts all this away from you. So it's just going to be a few lines of code to have all of these types of memories running. But in doing so, as I say, big benefit. A lot of work happens behind the scenes. And perhaps also there the trade off is that you've got less visibility into it. So if things don't go the way you want, then it's a bit harder to debug and figure out what's going on. And then there is another kind of memory called user memory, which is to store user specific information. And actually, at least as of now in Crewe, this is a concept that they support and have some frameworks around, but it's mostly left up to you to be querying user memory and then inserting it into the prompt or providing it at the right time. So user memory is a bit of an odd one out here, and I suspect that they're looking to build more into that in time. And for now, for the code that we're about to look at, we're just going to really look at contextual memory. So short term long term and entity memory and seeing how we can incorporate that into our stock picker solution. And so we're back in the stock picker project in cursor. And we are looking at the crew.py module. And I'm going to start by putting in some new imports in here, which are interesting ones from crime memory. We're going to import long term memory short term memory and entity memory. The three types we'll be working with. And I do believe you can also have user memory in there too. But then you have to manually manage it yourself. And then from my memory storage rag storage, we're importing a class called Rag storage for vector based retrieval. And with that we're also going to import from long term memory SQLite storage, long term memory SQLite storage object like that a class. All right. So that's a few things that we're now going to put to good use. We're now going to go to the crew function the function that creates the crew within this this module. And you can see where we made our manager. And we've got a few more things to make. As as cursor is trying to prompt us, they're trying to ignore cursors, insisting let's do it ourselves. All right. So we are going to to want to be creating a short term memory a long term memory and an entity memory. And we're going to do them one by one. So let's start by saying that the short term memory, which is the one that we'll begin with, short term memory, trying not to press a tab or it's going to fill it all in for me. But I'm going to I'm going to do it right here. Short term memory is going to be something which has rag storage. We we come up with a provider which is OpenAI, and a model, an embedding model to generate vectors from text. And we'll be using this one. And you can substitute in whatever models you would like here. It's going to be short term. And we give it a path to where we'd like it to create that memory, that vector store as memory. And it will use chroma as it happens, something which people who've taken my, uh, alarm engineering course know very well. I love chroma. All right, so that's the short term memory. Let's also create some long term memory. So here it is. The long term memory is going to be just simply creating an instance of this of this class long term memory SQLite storage. And we'll give that also a place to go. We'll make a database file also in the same directory. So that's the long term memory object. And now finally, thirdly, we're going to create a, uh, entity memory object. So entity memory. And there it is. So oh I see. Hang on. We've got two of them. Let's do that. Looks like that's good. So entity memory is going to be an entity memory object. It's also going to be a rag storage object. We give the provider and the embeddings model and we put it in the memory folder. So here we have our three types our short term memory our long term memory and our entity memory. And now we get to the place where we create our crew. And now it's going to be very challenging. It's not going to be challenging at all. We're going to say memory equals true. And we are going to then just do exactly that. So crew was a little bit off base with the memory equals part. But it's got the rest of it right. That's all you need to do. You set the long term memory the short term memory and the entity memory. And we are almost done with memory. Just just as simple as that. And I said almost because there is just one or rather two very small extra changes we need to make. We need to go back up in the module to where we created these agents the trading company, the financial researcher and the stock picker. And we need to give them memory. Now what we want is for the trading company finder to have memory. And we just do it by saying memory equals true. We don't actually want the researcher to have memory because we want it to go and do research every time. But we do want the stock picker to have memory, because we don't want it to recommend the same thing more than once. And I don't know if you remember, but in the prompts, in the YAML files, I said a couple of times, uh, don't, don't recommend the same stock twice and things like that. And, and surface new companies for the for the trading company finder. And that would normally be the final change you need to make is go back and make sure that your instructions and your YAML files are very clearly making sure that it will take advantage of memory. Because remember, whilst memory, these abstractions are trying to make memory seem quite magical and taken care of for you. At the end of the day, memory just means more stuff shoved into the prompt, more relevant context put into the prompt so that when you call an LLM it has knowledge. It's in the input is included information about prior conversations or about prior information that it retrieved. So with that we have set up set up the memory. And we are now going to bring up our terminal and then run this. And so as usual I go into already in the stock picker. So all I have to do is type a query I run and we'll be good to go. Let's see what happens. So just right off the bat, we expect it to be able to take advantage of memory without needing anything more. What we should see is it should create a memory directory in here. And it just has there is a memory directory. Stuff is going on. My computer is hard at work and I can see already within memory there is a Croma database that's being created. There is a long term memory. If I expand this, there is indeed a database that's been a SQLite database that's been created there. And so things are happening and we can see that, that, uh, various companies are being surfaced by the market watcher and more is going on and we will, uh, let this thing run. My computer's hard at work, and I will see you when we have a conclusion. And that definitely took a bit longer. It was going around around the houses a little bit, but it completed it. Recommended Microsoft this time. Remember, don't use this for real decisions. But it was, uh, entertaining to see it at work and bouncing around between the different agents. And whilst we don't have as much visibility into what's happening in terms of its use of memory and what context got provided, we can see that it's certainly built and populated different data stores and both the short term memory, the long term memory and the entity memory in the Rag chroma data store that's been created there. And the main point I want to get across is that, of course you can see the benefits of what this brings us. It was so easy to set up quite a complex situation, multiple types of memory with both vector similarity queries and SQL queries too. And we didn't need to know anything about it. We simply created the objects. The short term long term entity memory objects passed them in. And then we told our agents we turned memory on by saying memory equals true for the agents that we wanted to remember things. And that is a wrap on the stock picker project. We saw a lot of different aspects of Q AI with this project. We reminder, we saw structured outputs. We saw our own homegrown tool as well as Serpa. We also used the not the sequential but the hierarchical process. And now we have added in the memory feature all, all the three main types of memory in there as well. And that is a nice tour of a lot of the functionality in crew.
------
Video 61. Week 3 - Day 4 - Crew AI for Coding Tasks: Agents That Generate & Run Python Code
------
And now for something completely different. We are going to work on making an agent that knows how to code, how to write Python code, how to write software, and a little bit more than that, it's not only going to know how to write software, but how to run it as well. So this is challenging and complex, but it's possible to have an agent in Crewe in the Crewe environment, which has this ability. Not only is it able to to take a problem that you set it, it can write code to solve that problem. It can then execute that code in a Docker container. It doesn't need to, but it's good to have it run it in a Docker container, which means that it is a sort of sandboxed, ring fenced environment that doesn't have access to do damage to your computer because it's in this protected world, and then it can run the code, it can execute it in that container and look at the results and interpret those results to take on some more action. So as I say this, this is this is a really advanced Task. This is something that's taking us to the next level, and it's going to be hard and complex. Except it's not going to be hard or complex. You probably saw that coming, but it is going to be as simple as when we create an agent, we're going to say allow code execution equals true. And that's going to be it. And then it's going to be able to execute code. And if you say code execution mode equals safe, then if you have Docker installed on your system and I will show you how to install Docker, if you don't, it will run it in a Docker container. And it seems almost so good. It's uh, it's crazy. I had to like close down docker to make sure that that it that it failed when I, when, when uh, when it was closed because I almost couldn't believe it was happening. It's that simple. And this is where frameworks like crew really stand out because they make things like this so very easy to do. It's worth mentioning that these this kind of system is sometimes called a Coda agent. It's various other names for it, you hear. And it's worth knowing that when people talk about Coda agents, they're not just talking about agents that can deliver code. Agents that are able to generate Python, but also this ability to generate Python and then run that Python as steps towards solving a greater problem so that this is more a means to an end than just saying it is something which which writes code. Now, confusingly, our project for this week we are going to build something that writes code. But but the fact that that it can run it too is kind of cool. So that is the introduction. And with that let's go and actually build something like this. Let's build something that can write it. We'll make something that can write code and can run it. Let's do that. See you in a sec. Okay. Welcome back to cursor. Welcome back to the crew folder. And I'm going to open up a new terminal and go into that third folder. And you know how to create a crew project. So well now that you have it committed to memory. But you know that it's crew I, which I can't spell crew, I create crew. And then the name of the project, which we will call coda, and it's going to build the directories, it's going to do the scaffolding. All we have to say that we want OpenAI and we want GPT for our mini, and we don't need a key. And Kabam, we have all of the folder structure. Great. Okay. So as always, the first thing we do is we go into the config and we start by defining our agents. So what agents are we going to have. We're only going to have one this time. It's going to be a very simple agent. Let me just remove this so we can see what we're doing. Let's delete the the scaffolding and put in our coda agent. So we're going to say you are a Python developer. That's the role you write Python code to achieve this assignment. And then we pass in the assignment that is the input that will be set in the run method. First you plan how the code will work. Then you write the code, then you run it and check the output. Backstory. A seasoned Python developer with a knack for writing clean, efficient code. And we can use GPT four or mini. Yes, you can do it either way, but we'll do what cursor suggests. Change it to have the provider and the name of the model which looks a bit better. Okay, so there we go. That's pretty simple. That was our single agent. And we're going to have one single task. Let's put it in here. It is right Python code to achieve this assignment. A text file that includes the code. So so the output we want from it is a text file that includes the code and the output of the code. We want it to put both in the output. So I've called it code and output text as the output. So that is what we want it to do. So because I really want to make the point that it's going to write code, run it, get the output and it's going to think all of this through and then respond with all of that. We don't just want it to give us the output because we want to double check what it's doing is right. Okay, now we know we have to go to the crew module. Let's do that next. Okay. Here we are in the crew module that I've just gone through and deleted the templating stuff that's in there by default. And so we're just leaving our config relationships here. And I'm just going to put a little comment here. Make that a comment as well. Whoops. So this is a link to how you can install Docker if you don't already have it installed. And this is just literally and this is the Docker desktop web page for you. And as it does claim, it is a one click install for Mac or Windows or Linux. And it should be as simple as that. Many of you, I suspect from an engineering background, already know and love Docker. If you don't, then welcome to it. This should be as simple as installing it and then it's installed and you can then leave it be, but you will need to have it installed for this to work properly in a Docker setting. Okay. So now we are going to create our agent. We start with an agent decorator to make sure it goes in the right place. And we are going to create a coder. I'm going to ignore the fact that cursor is already probably several steps ahead of me, rendering my job here useless, but we'll keep going anyway, I'm going to return an agent. This is the usual fare. So what is our agent going to do? Well, the I will press tab there. The config is going to be the uh config that we set in the YAML file. But then we'll have verbose being true as usual. Now what else do we want. We now want this super complex, super hard step of making sure that this agent has the power to run to, to, to execute code, which is as simple as allow code. Oops. Allow code execution equals true. There it is. Now it can execute code. There's this step of saying code execution Execution mode is safe, and now that ensures that it runs it within a Docker container so that it's not just running the code on your platform. Now we can say Max execution time. This is a good one to have in there. And we're going to say 30 30s. This is going to be a simple task. And max retry limit. It's funny that cursor prompted me wrong there. Let's give it five retries. It can have up to five times of trying this. And that is our coder agent. So now we have a task. And the task is going to be a coding task. And I rather suspect that, uh, we, uh, can just use what cursor does, but we can take out expected output because we already defined the expected output in the task itself. So it's not needed again. All right. So there we go. We have our agent and our task defined. Now we just have to do the crew function method just below. 
------
Video 62. Week 3 - Day 4 - Create a Python-Writing AI Agent: Practical Implementation with Crew AI
------
Well, this might be a bit of an anti-climax. I'm just saying we have to write our crew function. We don't have to write our crew function because the default thing, there is all that we need. We don't need to do anything else. We've already got something which can do exactly what we want. What we do need to do still is come into main and rewrite this. And of course it's got all of the usual stuff. We don't need any of the stuff that's there. We'll just delete everything and come back here and write our own run function. Def run. This is the thing that's going to run the crew. And so we are going to say inputs equals assignment. How do I spell assignment. Assignment. There we go colon. Let's just give that a variable assignment for now. And then we're going to say results equals coder dot crew dot kickoff passing in those inputs. And then we want to print result Raw is how the result will come back and then we will indeed have. We don't need that main in there because it will automatically run that for us. Okay. And so what's left to do, of course, is to set the actual assignment. So let's come up with assignment assignment. And we're going to do something challenging here because I want to check if I said something like the default the cursor has got for us there. Right. A Python script that prints Hello world. Then we might not know for sure that it's actually running it, because Llms are perfectly capable of knowing what a script like that would yield would result in. So it could generate the Python code and then pretend it ran it. So we want something that it can't do any pretending for. And so I have one thing to show you, although knowing Llms, it probably could pretend this as well, but I certainly think this is a harder one. So I wanted to write a Python program to calculate the first 10,000 terms of this series, multiplying the total by four. So I put it in quite a complicated way. And here's the series one minus a third plus a fifth, minus a seventh plus dot dot dot. And so I'm just kind of it's a classic case of just sort of asking the LM to figure it out like a reasonable, intelligent human would. And who would have thought a few years ago that you could write software that would be able to take something like this with all of the nuance, all of the implications, and figure out that it needs to write code to put this into a loop, to keep it going for 10,000 terms, and then to take the total and multiply it by four. And the mathmos around will probably have already spotted this as being a very slow and very, uh, boring way to calculate pi. So if it does this right, and if it calculates the first 10,000 terms and multiplies by four, then it will be an approximation. A bad approximation of pi. And now the Llms are perfectly capable of recognizing this formula and knowing that the answer should be pi. But what's much harder is for them to realize that 10,000 terms would be approximately pi, and be wrong after a few decimal places. And so that's how we can tell that it's really doing its thing. So I hope that all makes sense. It's time for us to actually run this. All right, everyone, this is exciting. The drum roll. We're going to try and have our coder do its thing. So let's bring up a terminal. Let's go into the third directory. Let's go into coder and let's type crew I run to kick this off. And remember the challenge is now being set to our agent. You can see it started. There's the command and it's now going to be thinking about it. And something just ran and failed in the code interpreter. So it's that one means it's its first try and it's trying again. So it's not like it always works. At least it shows us that something is trying to run in an interpreter. Okay. It's done some code. It's got an answer. It just all happened really fast. So, um, this is the code that it's got, that it's written, and this is the output. It's used an interesting technique I can see it's it's obviously, uh, it's, it's going through the number of terms as it should do. It's a number of terms it sets down below. That's nicely done. And then it's using this idea of take minus one to the power of the index, which is either going to be minus one or plus one. It's a clever trick. And dividing by that which looks right to me. And then where do we see it multiplying the result by four. There, right there. I was going to say that would be suspicious. It does indeed multiply the result by four. And that's why we get 3.14149, not 3.14159, because it is just an approximation of pi. And it seems to have worked. And indeed, if also we see that it's created an output file. Yes it has. And here in the output file we will find the same output which has got the code. The output below it. And I gotta tell you, I'm blown away by how easy it is to do this. And all of the machinery that's happening behind behind the scenes to be able to start a Docker container, come up with the code, run the code, come up with the answer to the way that I expressed it. In quite a simplistic way. It just figured the whole thing out. And I think that's that's really cool. And there you have it. We just gave coding skills to an agent. We have our coder agent and it was so simple. And I hope you enjoyed it as much as I did. All right. Well, it ain't over yet. We've got more to go because the natural extension, now that we've got a coder, we can build an entire engineering team. It's really the goes to the meaning of crew. We're going to build a crew, an engineering crew, to be able to solve problems front to back. And it's going to be so great. I'll see you then.
------
Video 63. Week 3 - Day 5 - Building AI Teams: Configure Crew AI for Collaborative Development
------
Well, this is a bittersweet moment for sure. It is the final day of our journey with crew. Fifth day of week three. While we complete the work we've got with crew, and it's going to be a great one. We're going to end on a high. We are going to build on the project we did last time. On day four, we're going to turn our coda into an engineering team. We're going to have an engineering lead depicted here by this person chilling out because, you know, that's what what leads do. Uh, we're going to have a back end engineer. We're going to have a front end engineer, and we're going to have a test engineer seen here as someone that's jumping on top, trying to trying to destroy, trying to break everything that they are given. So this is going to be our team. We're going to really use the crew and crew. I let's get to it. And welcome back to cursor. Welcome to our final project for crew. Let's bring up a terminal with control and the back tick. Let's go into that third directory. And now, of course, it is time to do crew. I create crew and it is engineering. Underscore team is the name of our project. Let's do it. Let's choose the options that it tells us. We want OpenAI. We want GPT four or mini. And then we skip making a token. And then there we have our directory structure. And now of course we go into engineering team. We go into source, we go into config and we open our agents YAML file. As usual we see the boilerplate ones there the researcher and the reporting analyst. So what are we going to do. Well we're going to first of all delete what we have here. And we're going to put in our first agent. And our first agent is going to be an engineering lead, the engineering lead for the engineering team directing the work of the engineers. Let's make that plural, take the high level requirements described here and prepare a detailed design for the back end developer. Maybe we'll leave this singular for now. If it's just one developer, everything should be in one Python module. Describe the function the method signatures in the module. The Python module must be completely self-contained and ready so that it can be tested or have a simple UI built. Here are the requirements. So this is going to be one of our input attributes that we can pass in. The module should be named module name. And the class should be named class name. So we are going to bring in those as well. So we're going to have this be a little bit configurable. And the backstory you're a seasoned engineering lead with a knack for writing clear and concise designs okay there we go. So so look, I'm going to put on an LM now and let's have something different to usual. I think we're going to go with Gpt4. We're going to have the big guy you. I've also run this with Gpt4 on many and it runs just fine. So you can do that too if you wish. And you get, you know, you get a bit more comprehensive, extensive solutions if you use its bigger cousin GPT four. Oh okay. so. And whilst that will work, of course the full way to do it is to show this is OpenAI slash like that with the provider. That is the first of our agents. Okay. So the next agent is going to be called a backend engineer. So the backend engineer is going to be a Python engineer who can write code to achieve the design described by the engineering lead. So the engineering lead will do the design will lay out the sort of foundations, and the back end engineer will do the coding write Python module that implements the design described by the engineering lead, but will also pass in the requirements again. So it's going to get the requirements along with the design and the module name and class name again. And it's given a decent backstory saying that it's a Python engineer with a knack for writing clean, efficient code. Follow the design instructions carefully. So again, we'll use a different model. We'll mix things up, and this time I suggest we use Claude 3.7 sonnet latest. Now sometimes we get an overload error from Claude? At least I do. As of now. So it's a little bit less stable, but nonetheless, it's a great coding model. Particularly so. I think it's a good one to use in this case. All right. Let's also add a front end engineer. And I say front end engineer. But it's going to be a gradio engineer a gradio expert who can write a simple front end to demonstrate a back end writer gradio UI that demonstrates the given back end all in one file to be in the same directory as the back end module, giving the requirements. Giving the back story. A seasoned engineer highly skilled at writing simple gradio UIs for a back end class. So there we have it. We, uh, explained that the UI needs to be in one module, and we'll use again Claude for this too. Okay. And one more agent. Our final agent is going to be a test engineer, but I'm slightly abusing the word there. It's not. It's not like a QA engineer. This is going to be a Python coder who knows how to write unit tests for a given backend module, because I want to see unit tests being built out here. So that's what the test engineer is going to do. They're going to create a test which is going to be called test underscore. And the name of the module which will be the backend module. And they're a seasoned QA engineer and software developer who writes great unit tests for Python. Code is the backstory. And you know, for for all of the coding part, we'll use anthropic. Maybe we should shake it up. Why don't we? Why don't we take a risk? We'll use deep seek instead. Deep seek. I'm just going to stick with deep seek chat because deep seek chat is has is also able to write terrific code as well. So we'll we'll stick with that model and that will be our test engineer. Now it's time for us to go on to the tasks. So look it's a bit repetitive. We're going to let me get rid of the gunk. We're going to have a task for each agent. And it's funny. This is one of those times when it feels like OpenAI's agents SDK is sort of more straightforward that they don't have this extra indirection, this idea of a task being a separate first class object, because there's a lot of times that you will have this 1 to 1 mapping between a task and an agent. Obviously, crew has built this up because they really want you to think in terms of situations where you will have series of tasks that will be given to the same agent. And we did see that and at least one of our examples. But in this case, we are again going to have the case that there's going to be a task for each of our agents, and it gives us an opportunity to spell out very clearly what's required. So the description of the task is to take the high level requirements described and prepare a detailed design for the engineer. This is the design task that, of course, is going to be assigned to our engineering lead. And the output is a detailed design for the engineer. You'll notice that I make it important that it should only output the design in markdown format. Otherwise it just wants to start writing the code, which is not exactly what we want. And here we specify the the output as the module name underscore design. And it's worth noting that you can have these templated tags in the output file name as well. You can have it anywhere in this YAML, and it will get substituted in for the actual name of the the module that we're building here. So that is the design task. My next one cursor wants it to be called coding task. But when I wrote it I wrote code task. There we go. Uh, I'd like to be unpredictable. Uh, so the description is write a Python module that implements the design as described by the engineering lead in order to achieve the requirements. And then here I'm saying important output only the Python code without any markdown formatting or code block delimiters or backticks. And that's important because these models love to output markdown. And if you don't put that note in there, it's going to put that tick, tick, tick python at the top of the file, which means it won't be a valid Python file. And it just loves doing that. So unless you really highlight this is important, it's going to do it. And so that is the task. And we're saying context design task, which you remember is going to make sure that that information is made available to it. And generally speaking you can think of the tasks as being the user prompts and the agents as defining the system prompts, and that that's kind of how it all comes together. And this context is showing us what kind of information is going to be included in that prompt from others. Okay. So that is the code task. How about the front end task. So this is where the plot thickens. The front end has got to write a Gradio UI in a module App.py that demonstrates the given back end class. Assume there's only one user and keep the UI very simple indeed. Just a prototype or demo. Here are the requirements. And so yeah, we keep it. Keep it nice and simple. Maybe I'm going to take out the very simple indeed. Let's just say keep the UI simple and keep the UI clean. Let's leave it at that. So, um, important. Only output the raw Python code again. The agent is the front end engineer agent, obviously. And of course, for its context, it needs to get the output from the code task because it's going to write a user interface, which is going to directly use what the code task has written. So that's going to be very interesting to see how how it's able to keep that context so well that it's actually able to do this. And then cursor is correct that the last one is indeed called test task, but it's got a little bit more detail than cursor was suggesting. Write unit tests for the given back end module and create a test in the same directory. And this is. Got more information one more time about outputting raw Python without clever tags. And of course it also depends on the code task. The unit tests are not going to test the user interface, it's going to test the code. And that will be the output file. And that is it for our tasks our 1 to 1 again between the task and the agent.
------
Video 64. Week 3 - Day 5 - Collaborative AI Agent Development for a Stock Trading Framework
------
Okay. Okay. It's time for us to go to the crew module as well, you know? You know that that's the next step. And of course, you also know that I like to get rid of all of the gunk. No more gunk for us, but we'll keep in, of course, the hooking up to the YAML files. Let's get rid of everything else from here, and we will do our work to define the agents. So agents, the first agent is called the engineering lead. And good old cursor is giving us some good suggestions here, but not perfect because we don't need the engineering lead to be able to execute code. That's not its idea, it's just meant to be designing stuff. All right. But now the back end engineer. So thank you cursor for that. But you have missed this time what we really need here. So we do indeed want it to be able to have code execution. We want the code execution to be safe in a Docker container. Absolutely. The, uh, max execution time. We probably want to give it a bit more. Well, not not that long. That's rather too long. Let's give it a few minutes to work on this. Why not? And then Max retries. We'll give it five times before we are upset. That seems pretty good. Okay. The front end engineer. Oops. Need to remember my decorator. The the front end engineer. Let's have a look. So actually we're going to have it right front end. But we're not going to have it try and execute because that would bring up a gradio UI in the Docker container. That'd be a whole different, different ball game. So we're not going to do that. But we are going to have it do its creation. And now we just have the test engineer. So thank you cursor. And that seems like a good enough start. But the test engineer we absolutely want the test engineer to not only write the unit tests but then to run the unit tests. But it probably doesn't need. Well we'll give it we'll give it a couple of minutes to do that, and we'll give it five retries as well. That seems fine. Okay, those are our agents. Next, it's time for us to define our tasks. And this is going to start to get very complicated. The design task is just going to look like that. And I bet curse is going to know everything else. That's all we want for the code task now. And, uh, let's see if it can help us more. That's probably great for the front end task. And, uh, the test task. Is it going to help us with that? Yes it is. It's got a strangely indented, but no, it's brought it back. That's nice. And I think that's it for our tasks. I don't think there's anything missing here. Perfect. All right. Thank you. Cursor. Those are indeed our tasks. It's time though. It's time for our crew. Let's see if, uh, cursor can do this. Oh, no no, no, that's all. All a disaster. This is, uh, a moment to, uh, definitely mention that you do have to be mindful of this. The whole vibe coding thing. As I said before, it is common for Llms to generate too much code and to to just put stuff in there. And often people send me problems they've got and I can see that they've used an LLM to generate lots of code, because it will tend to err on the side of more. Of course, in this situation, we don't need to list out all of the agents like this. We can just say the agents are self dot agents. That's the why we decorated them. And we can say that the tasks is self dot tasks. And we also need to mention that the process is process sequential and verbose is true. And that really is it. But we would normally call this def crew like that. And I think we've got it. Okay well you know what's left. We have to do our run function and we will come to that right now. Okay. This is the usual main full of stuff. Gonna delete all of that. Going to delete all of this stuff here and leave us with something very simple. So let's have a think about this, this run. So, so first of all, we know that the what we've got here is we've got requirements are going to be let's let's just give ourselves a variable that we'll have we'll have a variable requirements okay. Module name was one of them. And we'll have that be a variable module name. And class name is going to be a variable class name. Isn't it great the way again cursor just guesses since we had that before that. That's going to be our thing. And now engineering team. That's because that the the function, the method that we decorated with, with the crew is indeed called crew kickoff with our inputs. Okay. So all that remains now is for us to actually come up with the assignment. And I have produced an assignment which I wish to show you now and which I think you may find rather interesting. So here is the coding challenge that we have for our team of agents. Let me tell you about this challenge. And then I'm going to tell you why. This is an interesting challenge. So the requirements we would like a simple account management system for a trading simulation platform. The system should allow users to create an account, deposit funds and withdraw funds. The system should allow users to record that they've bought or sold shares and providing a quantity. It should be able to calculate the total value of the user's portfolio and the profit or loss from the initial deposit. Users should be able. The system should be able to report the holdings of a user at any time, and the PNL at any time. And it should be able to list the transactions that the user has made. And it should prevent the user from withdrawing funds that would leave them with a negative balance, or buying shares that they can't afford, or selling shares that they don't possess. And I'm telling it, the system has access to a function get share price, which returns the current price of a share, and includes an implementation that returns a fixed price for three shares there. So that should be built into this code. So that's the challenge. It's like to build a kind of framework for simulating trading activity, which you know that's a fair bit of work to do that properly. That's that's not going to be easy. And you may wonder why why this particular challenge? Well, here's the thing. In the last week of our course, we are going to try to build agent traders. We're going to be using back to OpenAI agents SDK. And we're going to be using MCP. It's going to be really cool. And the idea is going to be, can we build like a set of traders of agents that are able to monitor financial markets, look at actual live real time prices and make trading decisions? But for this to work, we're going to want to have a kind of framework that they can make trades in and that a framework that will keep track of portfolios and so on. And there are some frameworks that you can get off the shelf, but they're very heavy weights because they're sort of big financial markets, backtesting frameworks. There isn't something that I could find easily that's just a lightweight account management stock portfolio management framework. And so I thought, why don't we get AI to build it for us? And what could be better than having our own crew, our own engineering team, take on this challenge and try and build out this, this whole framework for us. So that's why this is a two for one. Not only are we doing this week's project, putting together a crew of software engineers and a test engineer and so on, but also this is going to hopefully provide us with a framework that will then be able to reuse, give us a head start in week six, so we don't have to build our own and do a whole ton of coding. So it's actually going to save us some time as well to boot. Well, with that introduction, I think it's time to try and run this thing.
------
Video 65. Week 3 - Day 5 - Building a Trading Application Using GPT-4o & Claude
------
Ah, not so fast, you say. You notice that I'm missing the setting? The module name and the class name, which we also have to do. There we go. Module name is accounts.py and the class name is account. And actually that wasn't the only mistake also in task's YAML. I had some some missing. I had some stray tabs going on that some of these things weren't formatted properly. And I'll tell you, the reason I found that out is because I tried to run this and it failed. The error message I got was was quite obscure. It was like some long stack trace and it was not immediately obvious what was wrong. And I think, you know, I had to go through quite carefully and look for problems. And I sort of had a clue about the kind of thing I was looking for. But I can imagine if you're new to to using crew, then that could be quite a painful experience because there's not many clues given to where to go. And I really think that that is the kind of that's the deal that you make signing up for a framework like this, you get a lot out of the box, you get a lot of head start. Think of the memory stuff that we did last time and the whole way that this this fits together and the code execution stuff. I mean, it's amazing, but what you you get are that you are trading off is the fact that some stuff is is hidden from you. And when things go wrong, it can be harder to debug and figure out what's actually happening behind the scenes. But anyway, with all of that, it should be time now for us to actually give this a try. So we go into our directory, we go into engineering team and we get ready to run. Crew I run. Okay, here we go. And it's off. Thinking okay, the engineering lead is starting off the engineering lead. Of course that is GPT four zero. The way I have it set up, you may have picked a different model. You may even be trying with a llama. Now, I will say that if you are trying it with a llama, if you're looking for the free service, I do imagine that this will stretch llama to the max. This is going to be very, very hard challenge and depending on which model you pick underneath a llama, you may have limited success with this. This may be a case of where you have to watch what I'm doing here, and I'm saving the results in example output folder so that you can have that to hand. And you know, this might give you something to watch and learn from rather than running it yourself. But if you want to try it with GPT four and mini, that should work just fine. Anyway, we can see that it's done the design. It's now in the hands of the Python engineer, and I know that this can now take a couple of minutes. In fact, I think last time it took about five minutes to run all the way through. And that wasn't with deep seek. And deep seek can be a bit slower. So we'll see what happens. I will be right back in a minute. Okay. It just finished. It's exciting. I guess it did take about five minutes I think. Let's have a look at what was produced in the output folder. So I guess well there's a lot of files there. That's a good start. Let's start with the design document. It is indeed in markdown. It's uh, let's get rid of this terminal. It's got certainly like a design written out there and it's got like method signatures and things. That seems great. And now the big guy accounts dot pi. Okay, so it's got the get share price dummy at the top, which is what we wanted to to return a share price for Apple, Tesla and Google as example to to seed it. Okay. Class account. That's good. And it's got the right kinds of, uh, setups deposit accounts, withdraw accounts with comments with docstrings. Okay. That's that looks good from eyeballing. I'm seeing good looking stuff. Obviously we'd expect to see, uh, price times quantity. And then, uh, I see they, they're going to, uh, return false or return true depending on whether it was successful or not. That's interesting. Okay. And yeah, this all looks pretty comprehensive, doesn't it. Calculate profit or loss. Get the holdings takes a copy. That's that's an that's a pro thing to have done. I wouldn't have even thought of that if I was doing it myself. That of course is the right thing to do. Okay. Very nice. This looks very comprehensive. This looks like the Kobe we're looking for. Let's look at the test accounts. Sure. I mean, these are the kinds of tests you'd expect plenty of of assertions in here. That's great. And then that's the right kind of scaffolding around a unit test. And there is an API. But we're not going to look at that App.py, because we're going to look at it in a whole different way when I'm back. Let's, uh, let's try this. Okay. So I've gone into a terminal now as we try out the code written by our engineering team, by our front end developer, we're going to go into the folder called output. Here we are. Now I imagine that we're going to need to install gradio. I imagine if we just do app.py it's going to have an import failure. Let's try this. You've run App.py which you remember. You've run is the equivalent of Python when you're using UV environment. Let me see. No module named Gradio. Okay, so I have to do UV add gradio. And it's now added. And now we try this again. You've run App.py. Okay. It's thinking it's doing its stuff. Thinking thinking hasn't crashed. It's loading the various gradio dependencies right now, I believe. Okay. So it's it started a radio app. This is kind of crazy. Follow the link. Okay. Here we go. Here is a user interface. Okay. So first of all the first thing I'll notice this is you're getting like a raw reaction video here. I'm like astonished. You can see that it's done an audio out with with these tabs. Account management trading and reports. That's cool. So create an account. I get to give it a user ID, so I guess I'll call it ad initial deposit. We'll put that in I'll press create account. Account created for ad with initial deposit of 10,000. And there we go. Holdings. No holdings. All right. Fine. Now we go over to. I guess I can withdraw and, uh, and add. That's cool. I'll go to trading. We'll do symbol apple quantity one, buy shares. And here we go. Successfully bought one share of Apple at. Let's zoom out a bit. Let's see if we can. There we go. Zoom out again at that much. So the user is editor. That's now my cash balance that's come down. The portfolio value is of course the full amount because I've I've spent some money but I've bought equivalent Apple shares. That's calculating that. Right. And there's the holdings one share at 150 each. That's great. Let's go over to reports. Portfolio value is uh 10,000. Profit loss zero. Current holdings. Is there transaction history uh deposit and buy shares. This is great. This is so cool. I can't believe this. This is this isn't a simple user interface. This is a really good user interface. Look at the way things are organized into groups and are, like, nicely laid out. I'm kind of surprised about this because actually, in the past I've used GPT four or mini, and it's not looked as good as this. It's worked for sure, but this is a sharp user interface with with tabs, with sections organized quite nicely. And, uh, I guess we better let's try selling a share of Tesla that we don't have. Quantity one sell shares. Error. Insufficient shares to sell. Okay, we get an error message. Now let's try selling an Apple share Sell. Share. Successfully sold one share of Apple. And now we have no holdings. And if we go and look at our transactions, hopefully we'll see that we both bought and sold. Transaction history. Buy one share of Apple. Sell one share of Apple. Well, I got to tell you, this is astonishing. I'm, uh. There's there's this. This isn't a fake. I haven't run this in advance to to see this before. You're getting my raw reaction. And it's definitely surpassing my expectations in terms of this being a nice user interface. Like, if I had built this, I would be quite pleased with it. I'm, uh, frankly, I'm astonished. So that's a great conclusion. I will be sure to make sure I'll put some examples in example outputs. I'll try and put one with with mini so you can see a more raw user interface and then something like this. And uh yeah it's really amazing to see. So a collaboration between GPT four and Claude 37 and deep seek to build this platform with, frankly, a really impressive user interface. Wow.
------
Video 66. Week 3 - Day 5 - From Single Modules to Complete Systems: Advanced CrewAI Techniques
------
So I am genuinely blown away by how well that did. And and you know, again, how easy it was for us to get to that point of having these different, this crew of different agents building that product and the fact that that user interface just worked right away, like it just came up and it ran and, and it looked so great and that the functionality behind it there was like a front end and back end and it all held together and did its thing. So I hope that you're going through the same as what I'm going through, somewhat some disbelief and I hope you're seeing similar results. I've put that particular example in something called example Output new. And there's also you'll see a couple of other examples in there. If you you want to try out the ones that I generated. But you should try it yourself. Play with different models and experiment. And that's not all you should experiment with. Now this this week has some important projects for you. This this is where you really learn. You learn by building stuff yourself, by coming in. Take an example like this and just build it step by step. Take it a piece at a time. Gradually add to it. So the first way you can add to it, the easy way you can add to it is to have your team grow. Your team is hiring. Add in some more. The role that I call test engineer wasn't really a test engineer because this was some something that wrote test cases, but you could actually have a test engineer that's responsible for going through maybe writing a test plan and then executing the test plan. You could add a business analyst that fleshes out the requirements. You could you could have some more detail. You could have an even richer user interface. Honestly, the sky's the limit with this. We're just scratching the surface. We only had four people in this team, and and you can just keep on and on going and try different models and see where it takes you. But that's the relatively easy change there is then a harder change for you, which is the real challenge of the week. And here it is. So the thing is that this was all very well, but ultimately it only produced one Python module for the back end code. And then of course front end module to go with it and a test module to go with that. But it was all still very much on rails in that there was just the one, the one module that was already fixed at the beginning. Now, it would be a lot better if your team could build a whole system piece by piece, working on the different classes to build the different modules and then assembling them together. But therein lies a problem, because you would need to have a workflow that's a little bit more interactive in that potentially you'd need to have different classes being created by different agents. Now there's a few ways to do this. One way is, of course, you'll want to use structured outputs as a way to get more clarity from the engineering lead about who's doing what. Use structured outputs as a way to construct the different modules that need to be written. But ultimately, you're probably going to want to call an engineer a certain dynamic number of times. That will depend on how many modules the engineering lead wants to create. So it's not like you're fixed when you write the code that you know exactly how many tasks will be run now. Crew allows you to do that and it makes it quite easy for you. You can create a task object at runtime while it's actually running. You can have a task object, and one of the the fields you can have is have a callback. And actually the callbacks with the agent and the callback can make it create another task. So you can use this approach to build a more dynamic system where the results of completing one thing causes another thing to happen. And so you can assign tasks for each of the modules that need to be built. And that is the challenge for you. Add structured outputs and then add the dynamic creation of tasks so that an entire set of modules can be created, and you can build a whole system and then apply that. And I don't necessarily have a system to be built. You should apply that yourself to your day job. Think of a system that you would like, whether it's building a website, whether it's building an e-commerce platform, whether it's building something to organize medical records, whatever line of business, whatever profession you work in, think about the challenge that you could set your crew and have it be something which which is dynamic and which could involve building an entire system with several modules. And once you've done that, of course, or while you're doing it, you must share your progress. This will be hugely something that that will generate excitement on LinkedIn. So post updates there. Tag me so that I can weigh in and get it more visibility. These are projects that really count, and it really helps you to build and solidify your expertise and demonstrate it to others. Actually, to correct myself there, I was right the first time. If you want to to work with callbacks, then the way you do it is at the task level, not the agent level. And I'm looking right now in the crew docs. At docs. Or just just click on tasks right there. And it's worth looking through this because there's lots of lots of interesting information here that you might want to try out. So the callback you can specify when you create a task, the in the code you would say callback equals. And then the name of a function that should be called back. And that is where you could potentially create a new task. So it talks generally about the fact that you can create tasks from from YAML as we know well. Or you can also do it just by having the full code version of it too. As you can imagine, you can just pass in the description the expected output instead of specifying the YAML config. Then there's a lot of interesting stuff here that's worth taking a quick peek at. There's stuff we know about, but then there's also task guardrails, a analogous concept to what we looked at with OpenAI agents SDK. Using guardrails a way to validate and transform outputs before they're passed to the next task. It looks like it doesn't have the same constraint that OpenAI has that it needs to be input of the very first one, or output of the very last one. You can implement this at any task, it appears. And then there's stuff about how to handle errors with with those guardrails and then structured outputs. We know about that. That is of course using the output pedantic or output JSON is another way of doing it. And now integrating tools with tasks is of course something that we know about. And creating a task with tools is is listed out here, which is. Yes, sorry, that is what we're used to. Again, that is using a tool like the surfer dev tool that we already used, actually, right in there referring to other tasks. The output is automatically related to the next one, but you can define the context that should be used. And and we we did that. There's some stuff on asynchronous execution. You know I remember I said last time I think that we would be using async every single week, and we haven't actually used async this week. We're going to it's going to make a comeback, don't worry. But if you miss it then you can read up around asynchronous execution of your crew tasks here. This then is the callback mechanism. This is where you can implement a callback function. So if you had referred to it like so, then your function will get called subsequently when the task completes. And then there's a more interesting stuff here. And uh, more now on the, uh, the guardrails as well. And there we go. So I, I do encourage you to take a read through this, read a bit more about things like guardrails, which are interesting, and also about using callbacks. And some of this may be helpful when you look to build this more advanced flow. When completing one task can trigger setting dynamically, creating multiple other tasks. And very sadly, that brings our crew week to an end. It's the end of week three, the end of crew week. We've done some great things, so we built some fun projects. The stock picker was was really interesting. You haven't used that for investment decisions, I hope. And the engineering team was just mind blowing. Wow. Um, so I really hope you enjoyed it. I hope you have the same feeling as me about crew. I really love crew. I prefer open agents SDK, but but I love crew too, and I'm excited to now move to a heavier weight framework in the form of Landgraaf for week four. It's going to be great and I will see you then.
------
----------------------------------
Section 4: Week 4
Video 67. Week 4 - Day 1 - LangGraph Explained: Graph-Based Architecture for Robust AI Agents
------
Well, I'm so excited to see you back here for week four. Day one. It's already week four. Here we are. It is Landgraaf week as we really change things up, Landgraaf thinks about the universe differently. We're going to have a great old time, but it's going to require some different ways of thinking about things. But we're going to have fun. Uh, the project this week is particularly good. At the end of it, there's something really fun to show you with real business value that I've actually I've had business value from this already myself from from this project that we built. Um, but generally I want to say that this week, week four is actually quite a short week because I feel like we've got deep into OpenAI agents, SDK and crew, and we've done a lot that's covered familiar ground. And now the things that we do both actually with Landgraaf and with Autogen, it's going to have a lot in common. We're not going to need to go into quite the same amount of detail. So we'll be moving a bit more briskly through, but I'll be giving you plenty of the briefing and giving you the ability to go off and build your own projects with Landgraaf if it if it happens to tick the right boxes for you. All right, let's get into it. But before we even get into Landgraaf, I know what you're thinking. You're confused. You're confused about Langshan and Landgraaf and maybe also confused about Lange Smith, if you've heard of that. It is a trio of products offered by Lange Chain, and you may be unclear about how Landgraaf fits into it. And why aren't we going through Lange Chain? And it would be a great question, and it's one that I intend to clarify for you right now. Uh, and this this is the Lange chain ecosystem, and I could think of no better way of showing it than by taking little, little snapshots from their website. So Lange chain. Lange chain is where it began. It's been around for for many years now. And it was one of the earliest of the abstraction frameworks that that was there. And its initial kind of raison d'etre was that if you were building very bespoke integrations with different APIs. It was painful. And if you needed to change, say, from using GPT. To using Claude, you had to redo a lot of work. And so they had the idea of building abstractions. Then when it turned out that a lot of people were writing applications, which involved a call to an LLM, followed by another, followed by another, it sort of turned into this idea of chaining together your calls. Um, and, and, you know, Long Chain really took root and became something that's, that's quite advanced and supports things like rag uh, for people that do my engineering course, we use lang chain for a Rag implementation. Uh, it supports things like prompt templates, a sort of higher level construct built on top of, uh, prompting and supports memory in a very robust way, allowing you to to build memory that you keep in memory or that you keep in memory, keep it in Ram, or that you persist in a database. Uh, and it has various memory models. Uh, I guess not unlike the things that we saw from Crewe. Uh, but but there's a bit more, uh, stuff to the a few more abstractions and things to learn about. They also have their own declarative language, LCL as well. Uh, so there's, there's a, there's a lot of depth to Lang Chain, and it's really building a kind of engineering discipline around the, the, the art of working with llms and putting some scaffolding and some, some templates and some, uh, well, solidified code with things like good prompt practices around calling llms. And it's been extremely successful, uh, in that regard. It, it also it allows you to do things like abstract using tools. And so from that point of view, it does, in fact support building a genetic infrastructure. So you can use Lang chain and you can use lang chains workflows to build agent platforms. But it sort of predates the the recent explosion and excitement with, with agents. And so it's working in a more simplistic level. It's not their main agent platform offering. It's more of their glue code for building any application using LMS. Now, you probably heard me say that I have something of a love hate relationship with Long Chain. I definitely appreciate its power and the way that with very little code indeed, you can get up and running with with a lot of functionality like building a Rag pipeline in like four lines of code. Having said that, I do also see some drawbacks, and it's very similar to the drawbacks I was talking about with the more, more opinionated aspects of crew. It's that by signing up for a lot of the abstractions and a lot of the glue code that comes in the box with long chain, you're signing up for their way of doing things, and you have a bit less visibility into the actual prompts going on behind the scenes. And over time, the API's into LMS has become more and more similar. Anthropic is a little bit of an odd one out, but everybody else has really converged on OpenAI's endpoints and on their that structure. And so it's become extremely simple to interact directly with LMS. And handling memory is something that is also very simple to do yourself, because memory is really just the JSON blob of the conversations that you've had with the model. And so you can handle that JSON yourself. You can persist it as you want. You can combine memory in different ways. And so I see in some projects there's less need to sign up for a big ecosystem around, say persisting memory. But again there are pros and cons. There's definitely strong benefits to working with long chain. And with all of these significant engineering and problems that have already been solved that comes with it. Okay, so that's long chain and my mini rant. Thank you for putting up with that. Let's go on to talk about what is graph then. So on the website this is how it's positioned run at scale with graph platform. And as we'll talk about in a minute, Landgraf platform is actually one of the parts of Landgraaf, but Landgraaf itself is a bit bigger than that, so it's confusing on the website that they really push Landgraaf platform in this way. But let me tell you what I think Landgraaf is. Landgraaf is a separate offering from the company Lange chain from the same people it. It actually is independent from Lange chain. So whilst when you're working with Landgraaf, you can use Lange chain code to actually call LMS and to do various things with LMS you can do it's optional. You can really use any framework or you can just call LMS directly with Landgraaf. Landgraaf is all about a platform that focuses on stability, resiliency, and repeatability in worlds where you're solving problems that involve a lot of interconnected processes, like an agentic platform. So it's an abstraction layer that allows you to organize your thinking around a workflow of different activities that could have feedback loops. It could have times when humans need to get involved. It could have moments when you need to keep memory, and it allows you to organize all of that in a very repeatable and easily monitored and stable and scalable way. That's what Landgraf is, and the word graph gives some of it away that it's all built around graphs. Graphs being kind of tree structures of how to think about your workflow. So it imagines all workflows, anything that you might have going between agents in the form of a tree, a tree of nodes which are connected together, which represent different things that can happen at different points in your agentic workflow. And by thinking of it in this abstract way, and by putting sort of belts and braces around each point in this graph, they're able to bring stability and resiliency to a world that is a bit unpredictable and has has, you know, people have resiliency concerns about agentic AI. So that's really their approach. That's the problem they're trying to solve. And you can see if you if you read the detail there that they're saying you use this to design agent driven user experiences featuring things like human in the loop, multi-agent collaboration, conversation, history, memory, and what they call time travel, which is all about being able to checkpoint where you are in the process of being able to step backwards if you need to, to restore where you were as of at any point in time. A deploy with tolerant scalability, fault tolerant scalability, meaning that anything can go down and it will keep running. And that's a bit of the Landgraf platform thrown in there. So that's what Landgraf is all about. It's not related. It's not necessarily related to Lange chain. It is a framework for robustly running complex agent workflows, uh, giving you that kind of stability and monitoring, although I use the word monitoring there, and that was perhaps the wrong word to use it. It gives the ability to monitor, but it doesn't actually do the monitoring itself, because Lang Chain has a third product called Lang Smith, which is their kind of monitoring tooling. And Landgraf connects with Lang Smith. So you can use Lang Smith to monitor what's going on in your Lang graph graph. But Lang Smith is a separate offering. And Lang Smith can be used when working with lang chain or with Lang graph. And we will use Lang Smith. We will use that so that we can see things going on. And it gives you, as it says here, visibility into your calls and your reasoning to quickly debug failures. So that is how the different products line up. It is a bit confusing because you can use Lang chain to build agent workflows. It has an abstraction layer over things like tool calling, but Lang Graph is the core offering. That's the modern offering that's designed to meet the excitement of today's agent AI. And the particular thing that they're focused on is the kind of scaling in a resilient, robust, repeatable way.
------
Video 68. Week 4 - Day 1 - LangGraph Explained: Framework, Studio, and Platform Components Compared
------
So there's one more thing to mention. One more way in which it's just slightly more complicated. And that is that Landgraf itself is, in fact three different things, as I alluded to this already. But Landgraf is in fact Landgraf itself, which is also thought of as the Landgraf framework. It's a user interface tool called Landgraf Studio, which is one of these kinds of ways that you can hook things up visually. So like a visual builder. And it's something called Landgraf platform, which is the thing that they're sort of promoting on their website, as if that is Landgraf, uh, Landgraf platform, which is their, um, hosted solution for deploying and running your agents at scale. So, Landgraf, which is the framework which is analogous to Cray's framework, Landgraf studio, I think in Korea it was also called Korea Studio, right? I think so. And then Landgraf platform is analogous to CRI enterprise. And so these are the three offerings. And I mean, I think to me, I would make the same point that I made with crew, which is that what what perhaps part of what's going on here is, of course, Landgraf is looking for ways to commercialize and to monetize their offerings. And so looking for the enterprise play, the Landgraf platform is about deploying and running your Landgraf your graphs in their environment, uh, taking advantage of the fact that they've they've got hooks into everything that they've built. So I'm sure that if you have built all of your software using Landgraf, then it will be very convenient to use Landgraf platform. But I suspect that's what's going on, and I imagine that's why it is sort of promoted and framed so heavily on their site, as if Landgraf and Landgraf platform are the same thing. Uh, it's probably because this is the core commercial idea, uh, for for Lange chain and building this out. But we are going to be focused, of course, uh, as you can imagine on Landgraf, the framework, that part of it, that's what we're building to. We're going to be building our own. And we'll also be using Lange Smith as well to see what's going on. Uh, that's going to be interesting for us too. Now, at this point, I also wanted to do something I wanted to show you the blog post about building effective agents on Anthropic's website that I mentioned way back in week one, when we were looking at design patterns. But I think it's interesting and relevant now because it does show Anthropic's sort of positioning on this, and it's useful for you to keep this in mind when you think about the difference between what we're going to do today and what we're going to be doing in week six. So if we switch to building effective agents, then this this is the blog post you'll find on their website. And it's a brilliantly written post. I've got to tell you, it's really clear it contains within it the different design patterns that I mentioned. I hope I think I mentioned that I took the anthropic design patterns right from here. I thought they're so clear, so, so well explained. Um, and in addition to that, it has some discussion about using abstraction layers. And that's really what I wanted to show you here. So the, the, uh, um, it's says when and how to use frameworks. So they say there are many frameworks that make agentic systems easier to implement, including Landgraaf from Lange chain. They mention a few others that are not actually so popular, and not as far as I know, although I know of vellum the business. But but I don't think that that quite is as big as things like Cru and Autogen and now OpenAI agents SDK. But anyway, this is the point I wanted to make. These frameworks make it easy to get started by simplifying standard, low level tasks like calling llms, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction and that can obscure the underlying prompts and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice. You can see what Anthropic's getting at. You can imagine from their point of view, they've got an API. They've got something that is relatively simple. Memory can be handled with JSON objects. Llms can be hooked up simply by calling multiple times. And so for them, this this idea of building all of this abstraction around it, taking you further from actually working with the LM itself isn't necessarily something that resonates strongly. And so they conclude with we suggest that developers start by using LM APIs directly. Many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error. And by customer they mean us customers of anthropic. So there you go. I found this really interesting. It's obviously very clearly written and something that I'm very passionate about. It is an alternative, a different school of thought. It is, of course, somewhat, uh, not not in sync with the graph, uh, philosophy thesis of building this kind of structure. Uh, so it's interesting to keep that in mind. And of course, in week six, we're going to see what anthropic brings to the table in the form of MCP, which is a different way of thinking about it, a protocol, uh, for, for connecting things rather than building the actual glue itself. So I wanted to highlight that and give you that perspective, but it's not going to deter us from getting deep into Landgraaf right now.
------
Video 69. Week 4 - Day 1 - LangGraph Theory: Core Components for Building Advanced Agent Systems
------
All right, so let's get stuck in. So before we get stuck in, let me set your expectations. There's going to be terminology. There's going to be some new concepts coming with Landgraf just as we've had in the past. And I realize that it's a bit jarring to say put all the crew stuff to one side, just just put that away. Compartmentalize. We're on to something new now, but it will be quick. We're going to get through the terminology quickly, and I'm going to be drilling it into you by repeating it several times, and then it's going to be second nature and you'll forget all about crew. All right. So what is the terminology. Here we go. So agent workflows are described in Landgraf speak as a graph. A graph is something that'll be very familiar to people in computer science. But a graph you can think of like a tree structure. It's something which has, uh, I'm about to say the names of the things I'm about to describe, but it has things connected together in, in something which looks a bit like a tree and some sort of a hierarchy where one thing depends on others beneath it. And so this, this idea of representing workflows in this kind of graph way, that is the the core idea of land graph as the name gives away. And then a state. So the state is something that represents the current snapshot, the current state of affairs, the status of your whole application. It's an object that that needs to encapsulate the state of the world. And that object is something which, which, uh, is is shared across the whole application. And it's something very fundamental to land graph. So you need to remember state. We're going to be using state a lot. And that's something which is it is uh it's a variable. It's like um it's, it's a, it's information. Um, it's not not a function. It's information. Nodes are functions. So a node which when people talk about graphs, they talk about nodes. They are the points on the graph. They are the, the, the things that are going to get connected together. The nodes are actually representing a function. Every node is a Python function. And that can be confusing the first time you hear it. Because when you think about graphs, you sometimes think of nodes as being things, as being data connected in some way. But no nodes in a graph are functions. They are Python functions, and they represent a piece of logic, a piece of ancient logic, a thing, an operation of some sort. They receive the current state as an input. They do something. They do something that might, might involve calling an LLM. It might involve some sort of a side effect. Changing something, writing something to a file, doing something that affects the world. And then they return an update, an updated state. That's that's what comes afterwards. So they take in a state, they do something and they return a new state. And I should say that the state you should think of as being like an immutable thing, the state object itself. You don't. You don't change it. You receive a state and then you return a new state, which is a difference from the state you had before. So far, you're with me. State and notes that the the two words. And then edges. And anyone that's ever looked at graphs will be familiar with the term edges. The edges, of course. The lines. The words I was trying not to say earlier is that graphs consist of nodes and edges. Edges are the connections between the nodes, and edges in the graph are also Python functions. They are Python functions that determine what node should be executed next based on the state, so the state can tell it what happens next. Now there are simple edge connections, which just means that is the next thing to happen. So if you have one node that is a function, the Python function that uh, that prints hello to to the output. And you have another node that prints goodbye. And they're connected. Then it will print hello, and then it will print goodbye. And that's it. But you can have, uh, connections which are conditional, which depend on something happening. And in that case, that would mean that one operation would happen and then subject to the condition, maybe the other one would happen, maybe not. So that is an edge. And then simply put, nodes are the things that do the work. They are the things that carry out tasks. They do stuff and edges are the things that determine, okay, that stuff was done, what's the next stuff that should be done? And that's all there is to it. That's the that's the main terminology. There are a couple of other things, but this is the core stuff. So if you've got this then you're most of the way there with Landgraf. And of course it would be nice to show this in some sort of a diagram. And this is the obvious diagram. So no surprise this is what it looks like. These are On nodes. Uh, the three orange circles there representing the three nodes, three operations happening in this graph, the edges. I'm trying to show there that one edge is not conditional. It just wants the top node is run, then the next one will run and it will have the state that's passed in will be the result of the, the the output from that first node. And the other edge is meant to show some sort of conditional edge that in some situations the node on the bottom right will run. So again, nodes do the work, edges choose what to do next. So that again is the terminology. So we are going to build our first graph. We're actually not going to do it today. Today is just a theory. Only one because I'm going to leave the practical for tomorrow. I'm going to let you brew over this. I'm going to let this sort of sink in so that the new terminology is something that's super clear to you. But the five steps to building your graph. Your first graph running it. And it's going to have some more terminology. First of all, you have to define your state class, the class which is going to store state. You don't you don't to create a state object because there'll be new objects created all the time. Uh, and, and so it will define the state class and you'll find out when we actually do this. There's something associated with that state class known as the reducer. And that's a very important concept. But I'm going to leave that concept to tomorrow. Secondly you start something called the graph builder. And the graph builder is a thing you're going to use to lay out all the nodes in your graph. This is something you're doing up front. Nothing's actually running. It's not like we actually have an agent system that's running. This is all. Before we run the agent system, we first start a graph builder. We then create a node that is one of these functions that's going to represent some operation that we're going to want to happen as part of this Landgraf and then we create some edges and we may repeat steps three and four. We may create lots of nodes and lots of edges to lay out the story. We're laying out the story of what we want our agent system to do, and this is all before it's actually live, before it's doing anything. It's not like we create nodes and edges and then things happen. No, we create nodes and edges as part of defining the graph, describing what this will do. Think of this as a bit like writing a program, but we're sort of doing it dynamically at runtime. So once we've done that, once at runtime, we've we've described the node, we've created the nodes, we've created the edges. We've laid out our whole agent workflow. We then run an operation called compiling the graph. And that will turn this into something that's ready to be executed. And then we kick it off and it runs. So it is important to get your head around this. What I've described there, this 12345. This is all stuff that that is running when we when we start our our system running, we do this process of defining the state class, starting the graph builder, creating nodes and edges, and compiling the graph. It's almost like part of running our system involves the sort of two phases, a phase when it's like defining itself, laying out the whole workflow, and then a second phase when that actually runs. And so that first phase, we're not really used to doing that when we code. We don't normally have a sort of meta phase when we're describing what it is that we want to do and then running it. But that's how it works with graph, and I hope that's made some sense. I'm going to explain it one more time when we go through this tomorrow. But this is the, uh, this is this is the overall approach that's used for building agentic workflows with graph. And with apologies for belaboring the point, let me say it one more time. When you run a graph application, when you kick it off, there's two phases. Two things happen. The first thing is that it runs some of your code that lays out the graph that defines what it is that you're trying to achieve with agents. And then once you've done that and that is these five steps here. Once you've done that, you then initiate it. You then run it, you invoke it. And then off this graph runs. And those two phases, both of them are part of running your application. And so what we're going to do next time is go through these five steps. Write code that will do all of this. And then call it. And you will then see the results all in one go. And if this if this is completely clear to you then sorry for saying it three times. Uh, hang on in there. And if this is not completely clicking for you, then then fear not when you see the actual code, I feel like it's going to come together and it's going to be easy peasy. And that indeed is what we're going to do tomorrow. So in day two we will do some actual code. We'll get to the lab. We'll build our first land graph agent I can't wait. I'll see you then.
------
Video 70. Week 4 - Day 2 - LangGraph Deep Dive: Managing State in Graph-Based Agent Workflows
------
And we're back. I left you with a day for the land grant ideas to marinate, and hopefully they have marinated well, and you're now ready to actually go and do something. But as always, I do like to repeat a couple of times because repetition helps with these things. So one more time in Landgraaf an agent workflow, a particular pattern for how you will run an agent system is represented as a graph. Like a tree structure, use state to represent the current snapshot of affairs, the state of the world at any one point, and you build nodes and edges which are Python functions, and a node decides how to go from one state to another, and edges or nodes do the work, they actually carry out something which has consequences and edges decide what to do next. Once a node has run. And you'll remember I talked about five steps, and I tried to make it clear that the thing that's a bit of a like a head twister is that these five steps that I'm talking about, they all happen when you run your code, and they happen before you can even run your agents. So when you run your code, there is this prething that has five steps in it, and then the agent running and that prething is what would be called graph building that that is laying out what you want to do, and then you kick it off to actually do it. And those five steps, of course, you define your state class describing the information that will be maintained. You start the graph builder. You then create a node or many. You create edges to hook it all up or many. And then you compile the graph and you're now ready for primetime. All right. So I want to get a little bit deeper about the state before we get to the code. The final thing before the code so state is what's known as immutable. It's a word that that people like to say a lot and hopefully know what it means. But you might not. So let me be very clear. What it means to be immutable is that you will never change the contents of this object. It is something which, once you've created it and assigned the values to it, it keeps those values. And you don't ever do what's called mutating it, which is changing it. And, and that's, that's important because, uh, state is something which is going to represent a snapshot of the system. You want to be able to always go back to that snapshot. It's something which which in itself, if the contents of that state changes, you wouldn't be able to maintain that snapshot. Uh, so just to get concrete on what that means, what that means is that if you when you write the function for a node, that function is going to be a function which quite simply receives a state as its input and it returns a state as its output. And the thing that it returns is different, a different object, a different instance of state than the one that came in. So in this example called my counting node, let's imagine that we have a state object. And the only thing that it has is a field called count. And count is something which is a number that counts. Uh, and the purpose of my counting node is to add one to count. So I realize this is quite artificial, but just to give you a clear sense of it. So this is a function, my counting node. It takes a state, old state. It it collects the count that is stored within old state. Old state dot count. It adds one to it and then it doesn't return like the old state object doesn't try and set count in old state. No, it creates a new state object and sets the new state's count to be this incremented count and it returns that new state object. And I realize I'm belaboring the point as I often do, but it's an important one, and you need to keep this in mind. And otherwise you'll get in all sorts of traps. So there's one more complication for each of the fields in your state. You can optionally specify a special function to be associated with that field. And you're specifying this to Landgraf. You're saying to Landgraf, hey, I want you to know that for this field in my state, it's a special field that has a reducer, this function called a reducer. Reducer is the technical name for this kind of function. And the job of the reducer function is that when you ever update, when you ever return a new state, I was going to say update the state, but that might be confusing. Whenever you return a new state, which is a new version based on an old state, you've done something. You've returned a new state. Landgraf can use this reducer function to decide how to combine each the specified field in your new state with the current state, you see that. So it can use this function called a reducer, so that if you have a field maybe it's count, maybe it's a different field. It would use that to combine it with the old state. Now, now that I've said that, you're probably thinking about that for a second and you're like, hang on, why, why, why would you need to do that? We've got a node like this. We've got the old state. We can do that ourselves. We can just simply update the field in whatever way we want. Why do we need to specify a reducer function separately? Well, here's why. Because this means that that graph can run multiple nodes at the same time. They can all be running and they're all returning state. And there's no risk that one node runs and it overwrites the progress that a different node made at the same time. If for any field that has a reducer like this, it will always be able to work, they'll be able to combine different states behind the scenes. And that's that's the kind of that's the clever trick that allows that to be so. So you'll see when if this is seeming a little bit abstract, it's going to be concrete. When we look at the code, we will indeed use a reducer. And it's going to be super clear why we need to do that and how it helps. 
------
Video 71. Week 4 - Day 2 - Mastering LangGraph: How to Define State Objects & Use Reducers
------
And a warm welcome back to cursor. And also we are coming now into week four Landgraf and into lab one. And it's not only welcome back to cursor, it's also welcome back to notebooks. You know I love these things I know the mixed mixed popularity. Uh with with with others. But but hopefully you'll put up with them for a bit. We'll use code as well this week too. Don't worry. So we're going to start by doing some imports as we get started. And then we also have some constants. Ignore these for now. We'll use them in a bit. They're silly. Uh and we're also going to have our favorite, uh, little thing here. And you may be wondering why we didn't have that during crew week. It was absent. And the reason is that crew just does that for you automatically. Crew uses the env file itself, and so you don't need to run it. Okay. Now I need to explain something called annotated. So hopefully you're somewhat familiar with things called type hints in Python, an optional feature that is often used in engineering. To give you an example, supposing that we have a Python function called shout that takes some text, and the job of shout is to print the text in uppercase. That seems fairly simple. And let's just shout hello like that. Hopefully no surprise. Hello in capitals comes up. Well, uh, the type hints is when you are clear to Python, uh, what is the type of variable that you're using at each point. So you can say that that text is a string by putting colon str. And you can put here that will return a string or it's not returning anything, it's returning none. But if we also have it, uh, if we leave it like this, oops. If we have it be returning a string, then it will be unhappy. But we would need to then also return text upper as well. And then it will run. So these are called type hints. And they are a useful way of specifying what's going on. There is a feature that you can use called annotated in which we could type annotated. String comma. And then we can put some message here. Uh, something to be shouted. Uh, and this is something I need to close the square brackets. This is just extra information. That's a sort of FYI that's included in here. Uh, Python doesn't make any use of this at all. But if we are going to provide this function to, to, to another platform or in some other context, someone might want to read how we've annotated it, and that might be useful for someone. So annotations can be used for this purpose of kind of tagging variables to have a purpose. So this should run without any change at all. That is completely ignored. Uh, and it's just just a useful way of adding some extra information in case, in case it matters to somebody else and it is going to matter to somebody else. Landgraf is going to want us to annotate, in order to tell it something. Let's come on to that. So I explain here again, you could have a variable like my favorite things that could be a list. And you could say that my favorite things, you could actually not just describe it as a list, but say that it is annotated. It's a list. And these are a few of mine I added as an annotation to my favorite things. Uh, so why do I tell you this? Well, it all comes back to these things called reducers. We are about to define our state object. And when we do so we're going to give it a few fields. And when we define those fields we have to give them a type. And when we give them a type, we don't just specify the type, but we use annotated to be able to specify a reducer. If Landgraf is expected to use a reducer, that is the technique use annotated to do that. Um, and that that is what we're going to do. And as it happens, Landgraf comes with with one out of the box that's very useful for us called Add Messages. Uh, and that is one that I just imported up here. So if you look up here, one of my imports was from Landgraaf. Message import add messages. So that is it's like a function. And it's a function that you can, you can annotate with. If you want to say, hey this is the reducer I'd like you to use. So to make this feel real let me just let's go ahead and define our state. So you remember step one in the in the puzzle is to define the state object. That is what we're doing here. We are going to have a state object. Now state objects. You can define them in many ways. They can in fact be any Python object that you want. It's most common to either have it be a Pydantic object that we met in the last week. Uh, it can be. Or we made it the last two weeks. Uh, it can be a pydantic object, meaning it's a subclass of base model. It can also be something called a typed dict, which is a particular type of, of dictionary in Python where you specify what the keys need to be. Uh, but but it can also be anything but it is quite common to use either pydantic objects or type dicts and we will use Pydantic, since that's something that we're familiar with. So we are going to have a state object defining the state of our system. And it only has one field and that's called messages. And that is going to store in it a list of messages. And these this list of messages is going to be passed around our graph. And over time it's going to build up as messages get added to it. And so we are going to say that it's it's something that's annotated. It is a list. So it consists of a list of things of messages. And we are going to then because we're annotating it, we can provide an annotation that's ignored by Python, but it can be used by Landgraf. And that annotation is where we get to specify the reducer, the function that will be called in order to combine one state with another. And we're going to use one out of the box called Add messages. It is a reducer that is used to add messages. And it's very simple. It's very vanilla. All it does is it assumes this is a list. And if you return something with with an and items in the list. It just combines it with everything else in the list before it concatenates these lists together. That's all it does. So hopefully that makes a bit of sense. Uh, if not, then you'll see it'll become clear as we use this exactly what's going on. And and why were the use of this and the reducer and how it's working. So step one we define the state object. Step two we start the graph builder. And that is just a matter of calling this thing called state graph. Instantiating a state graph passing in state. And one thing to to get your head around here is that what I'm passing in there. The thing I've just highlighted. It's not an object I'm not instantiating. I'm not I'm not creating a state and passing that in with with messages and so on. Now I'm passing in the class. I'm passing in the type of thing that represents our state. That is what I'm using to create my state graph. And this is beginning the graph building process. This is part of the five steps before we actually run our Agentic framework.
------
Video 72. Week 4 - Day 2 - LangGraph Fundamentals: Creating Nodes, Edges & Workflows Step-by-Step
------
Okay. So here we are looking at our node creation. Remember a node is a function. So we're going to create a function. It's called our first node. It takes an old state and it returns a new state. Its states are immutable as we say. And so we are not going to do anything. We're not going to come and mutate old state. In fact, you may see from the way that it's grayed out here that we don't actually touch Old State, which is unusual, but just for this example. So what are we actually going to do? So we're going to make a string called reply. And that reply is going to be some random word choice of a noun. And then the word r and then a random choice of an adjective. And we're going to create one of these kinds of message structures and OpenAI familiar message structure and put that into messages. We're going to say that the new state is R another a new state class. We're going to instantiate it passing in this messages. And we are going to return that the new state. And that is the end of our first node. And we then call graph dot add node. This is how we officially add it to the graph that is being built. We give it a name, we call it first node and we pass in the function the function that represents the node called our first node. Okay. So we're going to run that cell. And now we're coming on to look at creating our edges. So you can see here that I now call Graph Builder add edge to add a couple of edges. And you'll see that these words start and end there. So what are they. Well these are things that we've imported at the top in from graph graph. They are constants start and end. And of course they signify the beginning of our workflow and the end of it. And so here what we say is we want an edge to take us from the start to our first node. And then we want another edge to go from the first node to the end. Okay, that sounds logical doesn't it? So we'll run that. And now with that we get to step five which you'll remember is compiling the graph. It's saying we're done. This is our workflow. And we can now display it. This is rather nice a quick way to show visually what it is we're talking about. And hopefully this will be no surprise. We've got a start going into our first node going into our end. What could be easier? That's lovely. Okay, so we've done the five steps, which is of course the what the first part of running a graph system. We've compiled our graph and we've done it by adding nodes and edges. It's now time to run it. Okay. And what we're going to do is to run it. We're going to create a gradio chat function. Why not. Remember gradio chat functions take the user's current input and the history of prior inputs, and it's meant to respond with the next output. That's just what you do with a gradio chat function that we're going to pass into gradients chat interface right there. So that's what we have to do. So what do we want to do. Well, we're going to turn the message into a standard OpenAI format and put that into a message. We're then going to create a state object with that as the is the message. We are then going to invoke our graph. And this is the key long chain word invoke. You may be familiar. Sorry I said long chain. It's the graph word invoke. But you may be familiar with it because it's the word in long chain as well. So you invoke a graph in land graph with the state in order to get the result. And that's what's going to execute our graph. And what will come out will be the result. And we will print it and we will also return it. And that will come out of our chat function. And so with that let's run this and see what happens. Well we have a greater UI. That's good. Let's say hi there. Muffins are haunted. Is that so? Penguins are sparkly. You don't say penguins are outrageous. For real. Pickles are untrustworthy. Ha ha ha. You get the idea. Uh, so, uh, this is the results of our small language model that is picking a random noun and adjective. And I show you this. If you're wondering what on earth are you doing? I'm doing it to show that the graph setup has nothing to do with Llms necessarily. The node is just a function, in this case a silly function, but it's a function there and it's taking in a state and it's returning a state, and it doesn't need to have anything to do with llms. Now down here, I'm printing the result of this. And let me just show you that print statement. Uh, we're printing what's coming back from calling invoke on the graph. And I want to point out that that it may be something a bit different to what you're expecting because it's not it's not messages with just a list of, of strings. It's got a list of these things called human message, which you may know from, from Lang chain work. It's like a, it's a construct to package things up. And this is of course the result of the reducer running. And when I said before that the reducer simply concatenates things into a list, I wasn't telling the full story because it also does some of this packaging up that comes with Landgraf. So if it just takes the text that's come back, it knows how to package it into a human message. In terms of the things that I was saying and an AI message in terms of muffins or haunted and penguins are sparkly and the like. So that's this is some of the of the stuff hanging happening behind the scenes as a result of Landgraf, which doesn't really matter to us. We're taking advantage of that. But we just wrote a simple, a super simple state. Uh, super simple node. We made a state that contained messages. We made a node we take in an old state, we return a new state which is using this random sentence, and it works. We can invoke our graph and get a response, and we can have a conversation of rather one sided conversation with a silly language model. All right, let's do something more sensible.
------
Video 73. Week 4 - Day 2 - LangGraph Tutorial: Building an OpenAI Chatbot with Graph Structures
------
Okay, now it's on to a proper example that we'll use with Landgraaf. Just take the obvious. The reason I showed you the silly example was because I wanted to show that nodes don't need to have calls to llms, and they still do what they're meant to do. But now we are going to add an LLM. So we start by defining the state. We create a graph builder with that state. And now we create an LLM a real LLM using chat OpenAI. So chat OpenAI is a construct from Lang Chain the sibling to Landgraf. Uh and that's what we'll be using to connect with our LLM. And now you don't need to use lang chains llms for this. You can use any Llms. You could directly call the LLM yourself. You could also, uh, use maybe OpenAI agents SDK. But it does make things a bit simpler sometimes if you use Lang chain and most of the community examples of course go from Landgraf to Lang chain. So it's it's easy to do it that way. And that's what we'll do for here. So we're going to create a new node called chatbot node. It takes an old state and it returns a new state. And what does it do. Well it takes the the LM and it invokes on that LM. So it's again it's the Lang chain. Landgraf word invoke uh, passing in the messages from old state. So old state has a messages field and that is what we pass in. And then for the new state, it creates a new state object which contains within it as in its messages field, it contains the response. And we return the new state. And we add that node called chatbot, uh, into our graph builder. Done. Now we'll add some edges from start to chatbot from chatbot to end done. And now we will compile our graph. Step five and we'll look at the graph. And sure enough it goes start to chatbot to end. And then we put it all together in a simple gradio chat function. It takes an initial state, which is a state object set up with these messages like so. We then call graph dot invoke to actually call our graph. We print the result and we will also show the results back in Gradio. So here it is. And I can say hi there. And it's actually calling OpenAI. Now it's not using our silly adjectives. And you'll see down here that there's the user message and the response coming in these objects, this human message object that is coming back. Sorry, that human message is my. Hi there. I mean, there should be an AI message. There it is. There is the AI message, which is the response actually coming from OpenAI. But one thing that's worth noting is that if I continue this conversation every time we are invoking this graph, and you will see what you have probably already suspected, which is that we're not actually keeping track of any history here. Let's see that in action. Uh, if I, uh, say, um. My name's Ed. Nice to meet you, Ed. How can I assist you today? What's my name? I'm sorry, but I don't have access to your personal data. So there's a sign that it's not able to keep context. And you can see it yourself if you read the information that's going to and fro. So because we've just got this simple graph that we were invoking each time, there's nothing particularly interesting happening here. And the state uh, is just, just contains that that uh, doesn't contain the history or anything. So, uh, that's one of the things that we clearly need to address. And the good news is that we will indeed address it. But the bad news is not until tomorrow. But we'll also address things like tools, our old favorite, along with a couple of other things. So, uh, look forward to it. I'll see you then.
------
Video 74. Week 4 - Day 3 - LangGraph Advanced Tutorial: Super Steps & Checkpointing Explained
------
And a very warm welcome to week four, day three. Here we are. This is going to be the time that Landgraf is going to start to pay dividends. The investment that we've made in understanding the new terminology is going to come together, and we're going to see some real value. But first, as always, I have to give you a quick recap. Again, I'm going to get bored of it, but it's better that you get bored of it and you understand it. So before you can call Graph Invoke, which is how you kick off your graph, you have to define it. And defining it is these five steps to find your state class. The graph Builder. Create a node edges and compile the graph. Just a quick refresher. I'm sure you've got this all committed to memory now. Okay, so what are we actually going to cover today? Well, there's going to be a few ways we're going to go deeper into the world of graph. First of all, we're going to look at Lang Smith and have a moment of understanding how information gets logged there. We're then going to look at tools tool calling something that we've done many times now using the out of the box tools that come with Lang graph. And then we're going to build a custom tool again as we've done many times. So we'll see that working. And we'll finally end with checkpointing which is a very important part of it. Indeed. And to to tee that up to motivate checkpointing. I want to talk for a moment about something called the super step. So what's a super step? Well, a super step. Start with a docs. A super step. They define as a single iteration over the graph nodes. Nodes that run in parallel a part of the same super step. Nodes that run sequentially belong to a separate super step. So what does that mean exactly? Now this is super, super important. Super steps are super important. And it's something which you have to get your head around. And it's it's a it's a definitely may not be what you're expecting. So a graph defines one set of interactions between like agents and their use of tools and perhaps delegating to other agents. If you think back to the handoff of, uh, OpenAI agents SDK. So one invocation of the graph is just one kind of step. It's like when the user says one message, putting that message to our LLM, that is one invocation of the entire graph all the way through from top to bottom, and if then it comes back to the user with a response and the user types in another message. That's another invocation of the whole graph. And each of these invocations is a superstep. Every time you invoke the graph, that is a superstep. And so yeah, it's important to get your head around it, because you might initially think that maybe you could imagine that a node is like a human and a node is a chatbot, and that a graph is human and chatbot going backwards and forwards. But no, every time that there is that kind of human interaction, you should think of that as a whole invocation of the graph. Or in some situations, you might be resuming the graph from one point if it was paused for for a human to respond. So there's various ways of doing it, but each of these interactions is considered an entire superstep within a superstep. One one invocation of the graph belongs to to activities which happen in parallel as part of that step. So this is a bit repetitive, but yeah, the graph describes one full superstep an interaction between agents and tools and potentially multiple agents to achieve an outcome. Every user interaction. It's a fresh invoke call. It's a fresh time that you're calling Graph Invoke. There's also a graph resume. But but the point remains. It's a fresh call. And the reducer that I talked about, the thing that is able to combine the state that comes out of a call with the original state that applies during carrying out a single super step. That is how state is managed across the graph. That's how if multiple nodes update the same state state, it gets combined at the end. That's what the reducer is handling. But the reducer doesn't handle, uh, the separate super steps a separate super step is an entirely fresh invocation of the graph. So that's going to be a bit confusing. So what does that mean to show that visually. Next I want to draw you a diagram. So with this diagram it's now all going to become crystal clear I'm confident it all begins with defining the graph. As I keep saying this is the five things you got to do, including defining the nodes, the edges, and compiling the graph. And then you're set. The next thing you do is perhaps the user has a question, and that question is what you then use to invoke the graph. There you go. And that is called a super step. And out pops some kind of an answer after the agents and tools have done their thing, and then the user says something, a follow up question, they have something else. And that would be another super step. And then that might happen again with another follow up or with another external activity. Each of these are super steps, complete invocations of the graph. So just to make that really obvious, I'm putting a little picture of a graph by each one. The whole graph is is invoked each time. And that is what it means to have a super step. And why am I going on about this? Because when it comes to memory, when it comes to preserving context between these different calls, we need to involve something called checkpointing, which is something that landgraaf makes available to us to be able to keep track, to sort of freeze a record of the state after each super step. So we've got that tracked, and then next time we call a super step, it can recall the state exactly as it was Checkpointed. And that is one of the things that we're going to be doing in the lab right now.
------
Video 75. Week 4 - Day 3 - Setting Up Langsmith & Creating Custom Tools for LangGraph Applications
------
And a very warm welcome to cursor. Here we are. We are in week four. Landgraf, we're going to lab two, which is the lab for day three. It's confusing. Uh, so we're going to start with some imports as we do. And we are also going to do the low dot env that always needs to be done. And we are now going to go and set up Lang Smith. So there's a link to Lang Smith here. When you follow this link you'll get to their website. And the first thing you'll need to do is create an account. Uh, and it's free. It's free as long as you stay within some very high number, which we definitely will. Uh, so you can set that up. And when you come in, you're going to get this sort of dashboard a bit like this, except it will be empty. And the first thing to do is to press the setup tracing button right here. And this is going to fly up. You have to generate an API key so that you've got an API key. And as soon as you press that it will then fill this in down here. And you just simply then want to copy that to your clipboard, all of your, uh, fields, all of the key variables that you'll see here. Um, but just don't obviously don't take the OpenAI API key as well. You don't need that, But you need this. And then you will it will actually populate that API key with whatever you generate right here, which is great. And then you will go into, uh, back back here into your EMV file, which is just there, and you will add this in to your EMV file so that we have these variables set. And then once you've done that come back and run load dot env again so that they load in. Maybe I should have done that in the other order. So that's great. Then you have langsamt configured and you'll be able to see things. Now before we, uh, go further, I will just just talk through a few of the things we can expect to see. Um, so I called the project mastering agents, but you can call it whatever you want. Of course. Um, and you can see in here there's there's lots of stuff that I've been doing. You'll see that it for every time that I've invoked Landgraf, I get some sort of a of an entry here. I get the input and the output. If there's an error, which I had a couple of errors, then we will see them. It tells you when it happened. It tells you the latency, which is very helpful. And there's a bunch of other useful stuff here too, which we don't often get to see. The cost in this column is telling us how much this cost us to make this call to OpenAI. And for those people that are concerned about about API costs, do have a look at how insanely small these numbers are. It just really brings it to life. I realize that the main problem people have is with the upfront $5 you have to put in at the beginning, but I hope that you see that that until you get to such point as you are scaling, we are talking about not just fractions of a cent, but fractions of fractions of a cent for most of these. Um, it also tells you the number of tokens, which is helpful to know. I'm not sure the zeros were because I had some bug that I luckily then fixed. Uh, so you get to see a bunch of stuff in here and this is going to be useful. And when we actually have things to to look at, we will come back and take a look at that. Okay. Onwards. All right. So I found a useful function that's in the long chain community folder. Uh and it's called Google Sherpa API wrapper, which means that it's a nice little convenient wrapper around the same thing that we set up before the Sherpa API, where we already have the API key in our env file, which is convenient to use. And so we can create an instance of this thing Google soap API wrapper. And we can call Sherpa with something like what's the capital of France? And if we do this, then it will think for a moment. And indeed we get Paris is the capital and largest city of France, and with some stuff about it, from doing an internet search on this, which is great. And again, this is something that's part of what's free and included in the plan up until some several thousand calls. I think unlike OpenAI, which is charging us two and a half cents for each one. So that is, uh, that is good. Now, this is showing you some of the nice stuff that's actually in Long Chain rather than Landgraf, but Landgraf uses it to. I mentioned to you that Long chain already does sort of simple agent interactions. It already gives some some wrapper code, some abstraction code. And here is here is one of them. I kept the import here so we could see that it's from, from long chains. Long chain. So? So you can say that, uh, you can wrap a function like this with a tool object. You give it a name like search a function serpent run. That's what it will actually do when it needs to call this tool. And a description is what will be will be called in order to, um, when, when building the JSON around this tool. Uh, so that's wrapped it in a tool object, a long chain tool object. And as you can probably guess, this is going to take away all of the gumph about building that JSON object with all the stuff and the properties and everything else. It's going to take care of all of that for us, so we can now try out the same, the same question, what's the capital of France which we can do using tool search dot invoke the long chain way we are calling this tool and there we go. We get an answer of the capital of France, same thing. But now we're doing it by calling invoke. Okay. And now let's just show you what it's like to build a tool from scratch ourselves. It's super easy. Uh, so we'll use the same old thing that we like to do because it's nice. It's so tangible. Uh, using pushover, this is the same kind of function that is able to send a push notification to me. So we give it the usual docstring, we give it, uh, the information about it. Um, and, uh, here we go. That's that. I put that into a tool again, the same construct, the long chain construct. The name is send push notification. The function is push and that's a description. And now I can then call tool push invoke with me. There we go. Oh, yeah. Hello. Me. Uh, so now we have made, uh, a tool ourselves. A custom tool. So now we have an off the shelf tool for searching using the same API. And we have a custom tool for sending a push notification using the nifty pushover package. Okay. And we can put both of these tools in a list of tools. All right. Then we actually get to use it.
------
Video 76. Week 4 - Day 3 - LangGraph Tool Calling: Working with Conditional Edges & Tool Nodes
------
Okay. So now we're going to build the same graph that we did last time. But we're going to add in some tools. Uh, so um, one small change I'm going to make though is this time for the state object. We're not going to use a Pydantic object. We're going to use a typed dict. And it looks almost identical. It's just a it's a subclass of typed dict instead of pydantic base model. Um, but otherwise the same thing. Messages field. It's annotated. It's a list. And this is telling Landgraf that we're going to use the Add Messages reducer as a way of reducing. Otherwise it's just the same. So you can stick with Pydantic if you'd prefer, but this is a different way of doing it. Uh, so then we start the graph builder right there. And now we have something different. So it's worth keeping in mind, in your mind that when when we work with tools, there's actually two different places in our code that typically we have to worry about tools. And I've explained this up here first of all, when we're, when we're, uh, making the call to the model, to OpenAI. That's the point at which we have to look at our tools and create all of that JSON nonsense. All of the stuff to describe what the tool is, so that when we prompt the model, it knows what it can do. So that's one piece of work that has to happen. And then another piece of work is when the model then responds. We have to test to see whether is the finish reason tool calls. And if so then we have to do that stuff, which was the quite long winded handle tool call uh, method in the first week where we unpacked the tool call and then we did some clever stuff. You can either have just an if statement, or you can have something clever that goes and finds the function and calls it. But that whole piece of the puzzle, the, uh, receiving the message to call a tool from the model, is like another place where you have to do all of your coding. Those are the two points, and we'll see both those points separately in how we have to handle this with land graph. So this is the first one. When we create the model this is the chat OpenAI object which is the long chain wrapper around calling OpenAI. We also, once we've created it, we make another version of LLM, which is LLM with tools, which is after calling Bind tools to the LLM. And this is just some nice sort of magic, which is going to then figure out what are the tools that it can call and make sure that whenever we call that model, whenever we call this version of the model, it's going to to provide all of the tools it can do. So it's one of those examples. This is really powerful. It's lang chain here. And it's lang chain being really nice, making it simple to package away all of that JSON stuff and do it for us by abstracting around an LLM to have an LLM with tools which whenever you call it, it's going to automatically pass in all the tools it can call. The flip side of that is that it's it's sort of hiding from us some of the implementation and some of what's going on which which can be which can make it hard to debug and things. But but you certainly I mean hopefully you see the positives very much. It's really easy. That is done that now. Whenever we call liquid tools, it's going to automatically handle the the building of that JSON and the parsing it in. And so look look how simple this is. This is our chat bot function. This is our node our function for the chat bot which just as before we return messages. And then what we've got here is we just call invoke but not on the LM but on the LM with tools that knows already about those tools. It will build the JSON, it will parse the JSON in as the tools. So it's really clever. And then we add that node. And then there's this line here. This is part two. This is the other thing. This is handling the results back. So what's happening here is that we added that chatbot node. We're adding a second node called tools. And it is a special type of node. It's a tool node. And when you create a tool node you have to tell it okay. What are these tools. What is it that you do? And this tool node is basically like a canned node that when it's invoked, it's going to see whether any of the message is asking to call one of the tools that it's been assigned, that it was passed to it when it was created. That's the job of the tool node, and if so, it will run them. So one more time, this piece of code here is handling the request to the LM and packaging up the tools JSON. And this node here is handling what happens if a if there is a tool request, it's handling the unbundling of the request and the actual calling of the tool function. And if any of that didn't make sense, I think it hopefully will do very shortly. We're going to create the edges now and again a little bit more complicated than before. So there is an edge which is going from the chat bot to the tools from this node, which is the actual chat bot, the LM with tools to the thing that calls tools to the node that actually runs the different tools we need to connect those two. But that connection, it's a connection which which has like an if attached to it. We don't want to always call tools. We only want to call those tools if the model has returned a response that the finish reason is tool calls. We had to like code that ourselves in week one. If finish reason is tool calls so that if has to be included in here because it's only if that's true. It's only if the model returns that it wants to call tools that we actually want to to call this node. And so and so that is why when you look at this edge, it's not a normal edge. It's called a conditional edge, which means it's an edge that's only triggered in certain conditions. And like an if statement and you give it the node name, you give it the condition and you give it the the node that would then be called. And that condition called tools condition is again pre-canned with Landgraf. And of course it looks to see if the finish reason is handle tools. Uh, so, uh, sorry, the finish reason is tool calls. And so that is what. That's exactly what that does. So when I used to say that, that, uh, at the end of the day, tool calling is just an if statement. This is the if statement. This thing here is handling that if statement. And if the if statement is true then then it's calling a node called tools. Uh all right. And then the, the uh there's one more quite subtle point here, which is that you also have to make an edge between tools. Back to chatbot again, because the result of this, the result of running the tool, the output needs to get fed back into the chatbot and it needs to continue processing from there. So this is subtle and, and, uh, you know, once, once I've said it, it's probably obvious, but it's one of those things that if I hadn't said it, it's not something that would naturally have occurred to you. And if you don't do it, then things will go wrong in strange ways. So it's one of those times. Again, it's elegant, it's neat. It's great that Landgraf does that, but it can make it a bit harder to debug. All right. And then we're going to add that start edge. And then we're going to. Compile and picture it. And this will hopefully bring this together for you. Start goes to chat bot. This is a conditional line. If if the finish reason is is a tool call then it will come here because the tools condition will be true. Then it will it will be responsible. This node is a canned node that's responsible for calling the tools that are relevant. And then the results comes back. And this is a solid line because if it's got here it should always come back. And this is a dotted line because only in the event that this didn't happen then it will come to end naturally. Landgraf automatically adds an end node for any any unresolved condition like that. And so that is our graph. And I hope that seeing that graph has made it come together for you. So the two places where tools are incorporated. One is the chatbot itself. We're using the LM with tools and the other is in the conditional branch and this tools node itself. Well, I think it's enough chit chat. We should get on and run this thing. So here we go. We're going to. Oh, I think I'll make this consistent with how we've done it before. We'll call this chat not invoke graph. But it is. It is how we should think of it really. So we're going to um, we're going to, to bring up a gradient screen and we're just going to call it let's see if this works. Okay. So I'm going to say hi there. Hello. How can I assist you today. Well I'm going to do something fun here. I'm going to say uh, please send me a push notification with the current US dollar to great British pound exchange rate. So I'm hoping that it's going to look up the exchange rate, uh, by doing a web search, find it out, and then send me a push notification. This is ambitious. 2 to 2 tool calls in one shot. Get that out of me and we'll see. Uh, we'll see how it does. So it's going. It's off, it's pausing. It's as you heard it certainly sent me something. The current USD GBP exchange rate is 0.78. Well, I happen to have a Google screen sitting over here. Let's bring this into view. There you will find the current US dollar GBP exchange rate is indeed 0.78. So that's cool. That's cool. I hope you agree. Sorry. Get that out of your way. I hope you agree. Uh, it's interesting to see that this has happened and that it's achieved this. And what we can see here is that we can come into Lange Smith and see the outcome of this. I'm interested that it's put that in red the four seconds that it took us. So let's go into this and see more. Please send me a push notification with the current USD GBP exchange rate. So we can see that that you can see the whole graph playing out here. And it's fascinating to see, uh, the, the, um, let's scroll down. It's doing a search right here. Click here current USD to GBP exchange rate. And that's the the result that came back. Um, this was from, from from doing the search just as we expected. And then it came back to a tools condition to a send push notification. Uh, and uh, back came the uh, the, the answer. And then that was the, uh, the conclusion. Um, there we go. And that's how it responded. So there we see the full, uh, the full chain of, of, uh, discussion happening within Lange Smith and how you can come in and debug this and see what's going on and see, indeed convince yourself that it did call the couple of tools. And this was great fun, and I hope you'll do the same thing. Come in now and run this and let it cool. Both tools go into Lange Smith and Trace through to make sure that you can also see all of these calls happening, uh, just just as we did right here.
------
Video 77. Week 4 - Day 3 - LangGraph Checkpointing: How to Maintain Memory Between Conversations
------
And I've come into Ladysmith one more time just to show you. I think I've read from the wrong place that the final output from GPT was right down here. I have sent you a push notification with the current USD GP exchange rate. So that's where you saw the end of the whole of the trace right down at the bottom there. Okay, so here we are back in this chat. And now let me just show you something. Despite its incredible intelligence with this conversation, what we can now say is, uh, my name's Ed. Uh, nice to meet you, Ed. How can I assist you today? And I can say, what's my name? And it says I don't have access to your personal information. Uh, so I use this as a way of showing you that, uh, despite the fact that we have all of this state management, despite the fact that we've we've built up this, this, um, these reducers and this clever structure, it doesn't have any memory. It doesn't have the ability to remember between the different conversations. And you might be thinking, well, I guess you're not thinking because I explained it already, but but if I hadn't explained it, you might be thinking. But but hang on. Uh, why? Uh, we've got all these reducers. We've got all of this stuff to handle state and to add in state. Then why, why isn't it, why isn't it just naturally remembering what's happening? And yes, it's all down to super steps. The point is that every invocation, every time we call this graph, it's like a separate fresh invocation and state is managed and maintained within any one invocation. That's what that's what it's there for. It's to handle so that you could have multiple nodes running in parallel. You could have all sorts of stuff happening, and the state would be managed through the whole graph in a, in a reproducible way. That's the power of the state management part. But that doesn't help you between separate super steps. Each super step is a different invocation of the graph. And the way that you do this is you use checkpointing. That's how you add memory. Well, I have to tell you, I have some misgivings about Landgraaf from time to time. How heavyweight it can be on occasion with some of the things we have to learn. But this is one case where I am very impressed indeed, and it's a moment for me when I get it. I realise why people like this so much and I can really appreciate the power of it. Checkpointing is very elegant, very simple and it leads to robust, repeatable processes. And you will see that too. And I imagine you'll be impressed like me at the end of it. So we begin by creating a new object called memory saver. And a memory saver doesn't mean like a saving memories. It's talking about saving to to to in-memory stores, to in-memory, rather than to disk or to to a database. It's a bit confusing. So that's what Memory Saver means then. This is the same code we had before. Exactly the same code. I put in a print statement so we can see what it's doing and otherwise. The only change is when we compile the graph, we pass in check pointer equals memory. We're passing in that memory object. But there's our graph. It looks identical exactly the same. And now this is where we actually run the code. There's a little bit of stuff here which is a bit hacky a bit hokey, but but the the idea is you have to create this object called config. It's a dictionary with one field configurable, and then that field you have to put in a thread ID and thread ID doesn't mean like a technical thread, it's meant to be like a conversation thread, like it's something that refers to this, this thread of memories that needs to be connected together. And so that's how you specify that, that this is this particular grouping in memory. And then when you invoke the graph you have to pass in that config. That's how you make sure that when you're invoking the graph, it's being associated with the right sort of slot in memory. And that is all there is to it. So with that, let's bring this up and we say hi there. Hello. How can I assist you today? My name is Ed. Nice to meet you, Ed. How can I help you? What's my name? I think it's gonna get it. Your name is Ed. How can I help you further? Ha, ha. And no surprise, you can see down here the print statement that I mentioned is printing out the the, uh, the inputs. And we're getting the full history each time, and that is all there is to it. That is checkpointing in action, maintaining memory between each of the calls, each of the super steps. Now, I know what you're thinking. You're thinking, okay, what's the big deal? Why are you so impressed by this? I mean, it's just like you could also get there just by storing that list of disks. Or we could take it from Gradio UI here and put it in a global link and keep playing it in. Well, look, look at some of this stuff. So you can call graph the compile graph paragraph. Getstate. Given that the the config and we get back this thing called a state snapshot, and this has in it the messages and then the complete history the conversation so far. Yeah. All right. So so it's like a, like a global that that has this in it. But there's more you can get get state history for our config. And what we get now is each step in time, every super step, every time we invoke the graph, the complete, uh, snapshot at that moment, starting from most recent at the top and then going back in time. And this is where it gets cool. And it's not like it's super sophisticated, but it's definitely a cool construct. Langshan allows you to step back in time to any prior moment. When you're passing in that config, the configurable thread ID you can pass in a checkpoint ID to kind of rewind to a previous moment, and then replay that through the graph. Uh, and this, this gives you this ability to basically do what they call a time travel. That's the Landgraf official name for it, which is really to be able to move back, get your snapshot at any point in time and be able to rerun it from there. And this is this is really great because it allows you to build systems that are repeatable and robust. If something falls over, you can restart it from any snapshot, any point in time. And you've got this kind of full tracking on everything that happened. So yeah, it's it's it's simple but it's elegant. And this is a case of, of an abstraction that really makes sense to me. Uh, and is definitely something that I think is very valuable. And I just want to show you as well that this has got nothing to do with, with gradio and with kind of variables. I can come back in and I can I can just relaunch my Gradio UI right here and I can say like, hi there. It says hi again, and I was going to say, what's my name? But I don't need to. You can see right away it's got the memory. It's still got that memory. It's got nothing to do with the the information stored in radio's chat UI, which is what we've always used in the past with, with gradio, but rather it is the memory, the checkpointing that's happening inside this, uh, this memory object. And then I can actually I can come back and if I recreate, I can I can create a fresh new memory saver object and come in and then I have to re rebuild my graph with, with, uh, the new memory right there and then re bring up my chat interface. And now if I say uh hi there. And what's my name. It's obviously not going to know it. So that just explains that by recreating a new memory object we've reset everything. And that's, that gives you a good sense of what's going on. My name's Ed. Nice to meet you. How can I assist you today? Uh, what's my name? Just to prove the point, your name is Ed. And then let me just show one more thing here. We can also change the thread ID to two. Let's rerun this and, uh, take a look here. Say hi there. Hello. How can I assist you? What's my name? And it says it doesn't know because the thread is two. If we now go back to the thread is one. I know you get this, but it's worth saying. And now. Hi there. Hello again. Ha ha. So there. You see it? It's a nice, elegant. It's it's relatively lightweight and simple, but very powerful. And that's a nice combination that I can get behind. And you can come in and look of course at the different checkpoints for either thread number one or thread number two. Uh, using the same this, this same structure.
------
Video 78. Week 4 - Day 3 - Building Persistent AI Memory with SQLite: LangGraph State Management
------
And now for my next trick. And our last trick for today. I'm going to switch it to store it in SQL instead. Our memory is going to be in a SQL database. And again this is impressive stuff. It's great that it's so simple and easy to switch to using SQL. We just import uh, this SQL Lite saver instead. And uh we're going to write to a memory DB. We connect that way. Uh, we come in and it's just exactly the same code. But we're of course putting in the SQL memory instead of the memory before. I mean, it's just different name of a variable, but it's the exactly the same approach, uh, as we had before. Nothing different there at all. There we go. I'm going to make the thread ID a different ID because since it's SQL, it's kind of remember from my, my, my tests. Uh, but uh, then we can come in and we can say hi there and see. How can I assist you today? Uh, what's my name? And I don't have access. Uh, my name is Ed. Nice to meet you, Ed. How can I assist you today? All right, so with that, uh, we can now go back here, and this time, we can just recreate the whole thing from scratch. It's like I could. I could restart my kernel. I should restart my kernel. Why don't I do that? You'll have to bear with me. We will clear. We will restart kernel, clear all outputs. We'll go all the way back up to the top, run our imports. We will load the dot env. Just check. I guess I should load in the tools again because we're going to use these do this, do this. Get the pushover message me. My phone's on silent. Bring them together. Get our state object because we reuse that at various points. And now from this point onwards, I think now we can now go back here. So we're completely fresh. Uh, so now we're going to recreate this. We are going to rebuild our graph builder. There it is. And here we go. And hi there. Hello again editor. Ha there you have it. Restart the kernel. Uh, completely bring everything fresh. And of course, it knows who I am. And it knows that because you can see right here in this directory, there's a bunch of database objects. It's storing it in a SQLite database. And so we have persistent memory with changing like one word uh from, from my code. And I think that is really impressive. And I realized just before we, we closed this, I should also show that the tools are working and in action. So after saying this, I should let's do the same thing. Um. Please send me a push notification with the current USD GBP exchange rates. Let's let it do its thing. Off it goes. And I just got a push notification and it says 0.78. Ha ha ha! There we go. So, uh. Success. Uh, I think, um, maybe, uh. Interesting. Uh, what what if I say I can? I'm trying to think about how we can use memory as well. Can you send that? Can you send that push notification again, please? Let's try that. There we go. I got another push notification with the same thing. So there, there. I had to think for a moment. There's a demonstration of both memory and tool calling in one shot with the right answer. Success. And you can see the various calls happening down there. So that then is a wrap on this. Um, and uh, it might be nice for us just to bring that up in Lang Smith just to see those calls. And so here we are in Lang Smith looking at that most recent request. And it's good to see that, as I was hoping to see it did use the tool to send the push notification. It didn't bother doing the search again because it had that in its memory. And you can see at the bottom here that it sends that push notification, and it then confirms back there that it did it as just as we saw in the chat. And you can see all of the history that it's got up above it. So you can see in Lang Smith that true to its word, everything is working. The memory is working, the tool call is working. Uh, and it knew that it already knew the exchange rate. Uh, so I'd say that that is a success. And to wrap up, let me show you one more time this great picture, which now I hope is lands nicely with you. We define our graph. We run each super step with each interaction, and these red lines are showing the checkpointing that's happening so that we can resume at any point. We can replay the state as it was. Then we can time travel back, and we can persist this in a database so that it maintains knowledge of the conversation. And it's keyed off this thread ID. And with that that concludes day three. And it was a meaty day. We covered a lot of ground. And I hope that you're building some of the same enthusiasms that I'm that I'm growing. As we get into this. For Landgraf, and seeing seeing the strengths, seeing how resilient, how robust it is, uh, and this, this discipline that it puts around the process. But that's enough of the, of the, of the learning and the concepts. Next time we launch into our project, our project called sidekick. And it's a really great one. I can't wait to show it to you. See you then.
------
Video 79. Week 4 - Day 4 - Playwright Integration with LangGraph: Creating Web-Browsing AI Agents
------
So, look, I don't know if you're anything like me, but I'm having something of a change of heart. So I started this week quite clearly with OpenAI agents SDK as my favorite, and obviously with crew as my second favorite, and being a little bit skeptical of Landgraf because you sign up for a lot. But I'm coming around. I'm having a really great time. And what I've got to show you today, I feel like you're going to be there, right there with me. I'm actually becoming a serious fan of Landgraf. Uh, and there's some really cool stuff, so see what you think. Uh, I'm, uh, I'm suddenly feeling treacherous against OpenAI agents SDK. Uh, so, uh, welcome. Welcome to week four, day four, introducing a new project sidekick, and I can't wait to tell you about it. So what are we going to do today? How are we going to go deeper? You know, we always go a bit deeper with each of these. So first of all I'm going to introduce I'm going to unveil a new tool which is going to be incredibly powerful and which is going to allow us to do very different things. We're going to talk about structured outputs, something that we've I mean, it's been a recurring theme. We've done that in each of these. We'll use it here too. And we're also going to build a proper multi-agent workflow in Landgraaf so that you see it really coming together, the equivalent of the handoff in OpenAI agents SDK, or having a crew of agents. We will be doing that. And as a reminder, on the diagram I did last time, that brings it all together. Remember the terminology? A super step is a complete invocation of the graph, and each super step represents like one user input coming in and then flowing it through your graph of agents and tools. And then those blue diagrams are representing the graph that gets executed. The state is managed through that graph, and reducers are called to make sure that the state object, which is an immutable object, is maintained and managed through that. But between the super steps, you use checkpointing as your way of maintaining state. And checkpointing is very powerful, and it lets you do things like rewind the clock and put your state back to any prior point. So with that quick recap, let's go to the lab. All right. So we get started in week four and we're going to go to lab number three. Week four. Day four. Lab three. Uh, it's, as I say, the start of an awesome project. I am super happy with this. I hope you will be too. We do some imports. Takes a while. There's a lot of stuff to go. And then we've got this empty cell here. Why have we got empty cell? Because of course we have to do load. Dot override is true. This is second nature to you at this point. Okay. So I also I'm going to introduce this time asynchronous Landgraf using Landgraf in async mode, uh, which I promised was going to happen this week. And it's very similar to using long chain in sync mode. Uh, when we used to call tools to run a tool, you can now call await tool dot to run the tool asynchronously, but passing in the same inputs. Otherwise it's just the same. And to invoke the graph, if we would have said tool graph invoke state before, we can now say await graph a invoke state. So that is just simply a way to run a graph in asynchronous mode. And that is what we're going to be doing. Uh, for the state, I'm going to again be using a typed dict. So that means that it looks very much like when we use pydantic objects, you just have one field I'm going to have called messages. It is annotated. The type is a list. And this is telling Landgraf we want it to use ad messages, which is a canned function that you can import. We want that to be the reducer, which is going to keep on adding messages whenever we return that as our state from a node. And we will start a graph builder with that state. Okay. So far so good. Okay. Next up we are going to create a tool that we did last time. So we can do it nice and quickly. Here it is the push notification tool. We define our function push. And then we wrap it package it in a tool called tool push which describes what it is it calls the function. And that is all we need to do okay. Now now it gets exciting. Playwright. Do you know what playwright is? Playwright is one of these browser automation bits of software. It's considered the sort of the new the next generation of selenium, which many, many people have used. I've used selenium an awful lot. Playwright is by Microsoft and it's a, it's a it's a very nice framework for running a browser. And it's traditionally originally used for testing. So it's used for test purposes. And a lot of people also use it for web scraping, because if you just try and request a web page, you just get back like the server content, if you use something like Selenium or Playwright, you can actually render it in a browser, run the JavaScript, paint the page, and then use that to actually get the content. So playwright is a is a powerful tool for running a browser window, and it can do so what they call headless, which means you don't even see the browser windows, just like running behind the scenes or not headless headful in which case you see the browser screen yourself and you can interact with it. So that is playwright. You may already know it. A lot of people use playwright. This is how you install it. You type playwright install to do it on on Windows and Mac OS or on Linux. It's this longer command and it will then be installed. And if you have problems with that, then let me know. But hopefully not. Okay, one other thing to mention. One other little detail here. And this is just for running it in a notebook. Um, one of the problems, one of the challenges with async IO is that Asyncio runs an event loop. Um, and when you when it runs an event loop, it runs this thing that's just constantly making sure that it's running anything that's being awaited. And then when it's holding on IO, it runs something else that's being awaited. Uh, and one of the problems is that that async IO only supports one event loop. And if there's an event loop running, you can't within that event loop, run another event loop. And we're running an event loop as part of running, uh, this notebook right now. And so that can cause problems when you try and kick off async processes within async processes. Uh, and so there is this package called nest async IO that's quite popular, which you can just simply use this and then it like patches async IO so that you can have an event loop within an event loop. And as you will soon understand from me prattling away, we're going to want that because we're going to want to run playwright asynchronously, because we're going to build an agent that can drive playwright. And so we're going to need that concept of an event loop within an event loop. But when we later put this into Python code, we actually won't need this anymore. But we'll need it for now. All right. That's a long sidebar. That's over. But now comes the magic. So one of the incredible things about Landgraf and Long Chain, I'm becoming a convert of both, is that they come with so many great tools out of the box. Many of them are in the community package. There's actually a couple of different ways that you can run a playwright. There's two sets of tools. One of them is a more simplistic one that's just a bit higher level. This one that I've got here, uh, the Playwright Browser Toolkit. This is a lower level set of tools which provides a bunch of different tools. Let's just We're gonna create this. So this is this is, um, creating an asynchronous playwright browser, and then it's building a toolkit from that browser. And now we've, we've got the tools into a variable tools. So I'm just going to print these. And here are all of the tools that we get in this package. We get something that lets you uh, click on an element in a web page, navigate the browser to a particular web page, go to the previous web page like press the back button, extract text from a web page, extract hyperlinks, get elements, and then the current web page. Uh, so it gives you quite granular control over what's going on in this playwright browser. So again, to recap, playwright allows you to launch a browser window. And then this Landgraaf or Lang chain set of tools gives us a series of tools that will allow us to interact with that browser open windows, navigate, read the text, read the elements, do that kind of thing. That's pretty cool. And that is something we're going to be able to arm our agent to be able to use.
------
Video 80. Week 4 - Day 4 - Create AI Web Assistants: Playwright, LangChain & Gradio Implementation
------
Okay. So next we're going to actually put these tools to good use. So this thing here is a neat neat little bit of Python called a dictionary comprehension. You might be very familiar with it. If not check out the guides. We put together a dictionary by iterating through tools and creating something where the key is the name of the tool, and the value is the tool itself, and that allows us to pluck out the navigate tool and the extract text tool from this list of tools. And we're then going to call the navigate tool asynchronously navigate tool with the web address for CNN. And then we're going to call await to extract the text from it and call this. So this has got nothing to do with LMS. This is all about running Microsoft Playwright with the tools that come with Lang Chain. And we'll run that. Up comes a browser, it turns to CNN and stuff is happening and that just completed. And isn't that cool? It was like, I know it's got nothing to do with AI, but it's just great technology. Come back here and we print the text and there we go. We've got the text from CNN. Impressive stuff. So we can now package this together into one collection of tools. These tools that we just got from playwright, plus our push notification tool in it goes. And now it's time for us to make some agents. So the first thing we do is we do some, some pre-work to create our LLM, which is GPT four mini. Feel free to switch in your favorites. And then we bind it to tools. This you remember is what we do to deal with creating all of that JSON. For all of these tools. We've got a whole ton of tools now. JSON will be built for all of them. And now this is our chat bot function. This is something that is going to be one of our nodes. Because a node is just a function and it's something which will call it will invoke LLM with tools passing in the messages. And what comes out will be shoved into messages and returned. And that, of course, is our the new state, and the reducer will be used to combine it with the old state. So let's execute that. That's great. And now we have our graph. I think this is probably makes more sense to go here just to be nice and tidy. So we create our graph builder with the state that we've specified. We add the chatbot node. That's this function we just got here. We add in our tools. We then add in a conditional edge. This is going to be the if statement implemented behind the scenes. If the tools condition is true then tools will be called. We add an edge from tools to chatbot and then we add from the start to the chatbot. So I said agents. It's only one agent. That's it. We get some memory. We compile with that memory and we draw our image. There it is. It's a an image we're familiar with. It's the same as last time. The only difference is that these tools are a bit meatier than we had before. We now can control a browser window. Okay, so here we have at the end our final, our gradio, uh, flow. So we're going to create a Gradio interface. Um, we're going to use a function called chat as our callback. And here is chat. It's an async function. Gradio supports functions. It's callbacks being being async or being async. And it's um, we're going to to just simply call our graph that we've already built. We're going to call a invoke the asynchronous version we pass in our first state, which is the user input. This is a super step. So every input is like another run of the graph we pass in the config. So we have memory. We don't actually use the history because we're relying on checkpointing for that. But Gradio sends it to us anyway. And that is our gradio code. And with that let's run it. There it is. Okay. Let's say hi there. Okay. Let's say, uh, please send me a push notification. With a news headline news headline from CNN. See what happens there. There goes the browser. It's. I'm not touching it. Haha. Have you expecting that? We got a text. I could see the push and it says indeed this push notification about news going on. Very cool. All right, let's try another. Uh, please, uh, send me a push notification with the current usd GBP exchange rate. The browser is still running behind. Let me just bring that up there. It's gone to an exchange rates page and we got the push notification. There he is. Yep. And it is a different than yesterday. Down by a little bit 0.77 instead of 0.78. That's amazing. So there you go. There you have it. Uh, that is a really powerful tool in playwright. And isn't it amazing that we've now armed our agent to be able to drive a browser? And you can start to see when you think of the excitement people have around things like Manus and this idea of agents that can drive, you can now see how this could become an agent that can drive a browser and do work for you. And of course, that is what we're coming to for this week's big project. And I thought it'd be nice to just bring this up in Lang Smith for a second to show what it looks like. Uh, here we have it. Uh, you can see, if you look, that there was a fair amount of interactions that went on the process of going around the browser and and doing various things. It takes a little bit of interaction. It takes, of course, doing some navigation, navigating and reading. And if we look on the right, we can see what happened here. This is the human making that. So I said hi there. It said, hello. How can I assist you today. Please send me push notification and then you can see it uh it called to visit navigate browser to CNN.com. So I think yeah I just said CNN so it knew to look up that that web page um and then it knew to navigate it extracted hyperlinks. It looked at some hyperlinks, it did some stuff. And then it ended up sending a push notification. Uh, I noticed that I've got the tool returning just null, which is probably a bad thing. I should have it returning like some some fake JSON that says success or something so that it doesn't, uh, so, so that it just sort of lands better. It's more coherent with the, with the call of the tool. So I will make that change. Uh, and then, um, you can see that it then goes to exchange rates, it pulls that out, it gets the results, it extracts all of the text. And then once more it sends a push notification. So it's great to see this trace. And then you can see some other information about this too. I think I got to see over here. Yep. The total number of tokens that were exchanged with GPT four mini. And the fact that the total cost was, uh, $0.10 out of. Oh, no. That's from from all of the runs. I was going to say that would be expensive. Uh, and I've been doing a lot of runs, so, so so you should not have spent $0.10. Uh, that was, in fact, 0.8 of a cent for that whole conversation. Uh, and if you've been using a llama, then it wouldn't have cost you anything, but you might have had more coherence issues because we're expecting a lot. You probably needed to use a larger model. Um, but certainly worth experimenting with. But if you chose to splash out and use GPT four on many, it would have cost you all of 0.8 of a cent. Uh, so, uh, you can see that these things and obviously they can rack up over time if you use it extensively. But for any one given call, it is, uh, relatively low cost.
------
Video 81. Week 4 - Day 4 - LLM Evaluator Agents: Creating Feedback Loops with Structured Outputs
------
And so here we are back in cursor to go to lab four. Because this is a day when we do two labs in one day. Week four, day four preparing for tomorrow's big project, which is called the Sidekick. And it's time to introduce structured outputs and a multi-agent flow. So we've got a bunch of imports and we are going to set our dot env as usual. And now we're going to be using structured outputs. And you'll remember the first step with structured outputs is to define the schema. What are we using to describe the results that must come back from an NLM. And in particular, the thing that we're going to be working on is an evaluator, something which is going to decide whether or not an answer from an LLM is good. And so our evaluator is going to respond using this object in evaluator output. Or really it's going to respond with JSON. And the JSON is going to have to conform to this. So we just describe what this means. There's going to be a field feedback which is going to be feedback on the workers response. Uh Success criteria. Actually, let's change this to the assistance response because that's going to be more words that it will understand. Worker is what we will call it in this assist response. There we go. Now you get to see me type up. So feedback on the assistance response success criteria is whether the success criteria has been met and user input needed. True. If more input is needed from the user or clarifications, or if the assistant is stuck. So this is going to allow we're going to have an evaluator that's going to evaluate the results of our worker, the assistant, to use their terminology. And it's going to decide whether it's okay to forward that back to the user, or whether it needs to go back to the assistant for more work. And one situation is if the success criteria are met and it's done its job. But another situation is if the worker seems to be stuck or needs clarifications, in which case it should return. So there we go. Um, okay. And now to manage state. You remember with state it can really be any Python object. It can be a Pydantic object. But we often use type dicts. And that's what we're doing here. And now for the first time, we have some real meaty information to store in the state. We've always had messages before, but now we have a real state and we've got a bunch of things. We do still have messages, which is representing the discussion between the user and the assistant, but we've got other stuff which is going to represent the information being passed from the the evaluator. Back to the thing. I'm going to call the worker, the assistant. Um, and so we're going to have some more stuff. We're going to have success criteria, which is going to be set up front to define what does it mean to be successful, feedback on the work that's going to come from the the worker. And by the way, you use optional like this. If this can be null, it can be none or it can be a string. Success criteria met is a bool is true or false and is going to be true if the if there has been a successful outcome that the criteria met and the the, it doesn't need to go back to the worker for more. And user input needed is if we need to go back to the user to get some more information. So that is our state. So it's a much meatier state. And this shows you that you can have whatever you want in the state. The state is really up to you and the flow of your logic. And that state is like something that has moved through the graph and everyone gets their opportunity. Every node gets the state and gets its opportunity to to return a new state. That is some change to that state. And we're only specifying one reducer ad messages, which means that if one of these nodes returns with some messages, they will get accumulated. They will get concatenated with the existing messages. But if one of these nodes returns user input needed, that will overwrite whatever was in the old state. So when you return the new state, if you change one of these values here, that becomes the new setting for anything that is downstream of you in the graph. Okay. So next up we set up our playwright tools. This is the same code as before using the async browser, the playwright browser and the Playwright Browser toolkit. So we just run that. That's pretty simple. And now we're going to have two llms that we're going to initialize. One of them is called the worker. LLM plays the role of the assistant. It's going to be GPT four mini. And it's going to be bound to those tools so that it will automatically have the JSON gumpf in it. And then separately we're going to have an evaluator LLM. This is a separate LLM. And it is we're setting it up. Whereas before we said bind tools. And now we say with structured output and pass in the Pydantic object. And that means that the response will conform to this output. Now not all models are set up to support structured output. So you may find that some models can't do it if you're if you're going to be playing around with different models. If that is the case, of course, the alternative to this is to do it the old fashioned way, which means instead of using structured outputs like this and passing a pedantic object, you ask the model in the prompt to respond in JSON. You give it the schema. You give, you list out what kind of JSON it should respond in. You maybe give a couple of examples to make sure that it's really biased to do well, and then you have to parse that JSON in the response. And that's all that is happening behind the scenes when we do structured outputs like this. So that means you congratulations. You now learn how to do structured outputs with graph. It's that simple. Okay. So this is a bit of a long looking uh, function. This node, the deaf worker which is representing our worker node, our assistant. But it's only long because we've got a lot of prompting in here. So it's a node. And so as usual it takes a state and it's going to return a state. And you can see it's returning something with messages. And we know that messages accumulates because we have the reducer. So that is going to add on more messages. Okay. So we've got like a long system message. Let me talk you through it. We say look you're a helpful assistant that can use tools to complete tasks. You keep working on a task until either you have a question or clarification for the user, or the success criteria has been met. And this is the success criteria, and we take it from the state. It's something that should be held in the state. You should reply either with a question for the user about the assignment or with your final response. If you have a question for the user, you need to reply by clearly stating your question. An example might be this. It's not super necessary for me to have done that, but I wanted to make it really clear that it will say when it has a question and sort of force, force the point. If you finished reply with the final answer and don't ask a question, simply reply with the answer. And I say that because these models love to reply with things like can I help you with anything else? And that then might confuse our evaluator that's looking to see if it needs help. So I want it to be super clear on this front. Okay. And of course all these things are subject for experimentation. There's no hard and fast rules. It's not like that is a as a rule that you have to put this in the prompt. I hope you know this by now. This is this is really the the the part of AI engineering that is about experimentation, R&D. You can imagine I've been crafting this prompt for the last couple of hours. So it's something that obviously you hone in on something that works well. And you may find that particularly with different models or if you have different assignments, it's something that you have to tweak to get the kind of performance you want anyways. Uh, if, if we've got in our state something in this, in this field feedback on work, then that means that an evaluation has happened and it's not gone well. There's been feedback and it's come back for more. So then we add to the system message. Previously you thought you completed the assignment, but your reply was rejected because the success criteria was not met. Here is the feedback on why this was rejected. And then we give the feedback. And then with this feedback please continue the assignment ensuring you meet the success criteria. Uh so there it is spelled out in detail. Um, okay. And then I've got here some, some slightly hokey code that that looks to see whether there's already a system message inside there. And if there already is, then it just replaces whatever system message is there with this system message. If it doesn't find a system message, then, uh, it, uh, it creates a new one and puts it at the front. So this is, this is just to make sure that we handle those various scenarios. Now, this is a little bit hokey because, uh, Landgraf will have already perhaps have built a system message. So, so we have to be a bit careful about this, and it might be worth doing some testing to make sure that it works for different models. Okay. So at the end of all that we then call worker LM with tools and we invoke the messages. We get back a response and that is what we then return. So there we have it. Uh, okay. Now we now have something pretty interesting called the worker router.
------
Video 82. Week 4 - Day 4- Creating LLM Feedback Loops: Worker-Evaluator Implementation in LangGraph
------
So the worker router then is a Python function that we will use in our edge, in our conditional edge to decide which way to route control. And it's very simple. It's going to take the most recent message. It's going to see if it's a tool call. If so it returns tools if not evaluator. And that is to say that when our worker, our assistant has come up with an answer, if it's not involving a tool call, then it needs to be evaluated. And that's what we get to right now. So for the evaluation, we have first of all, this little utility function format conversation that's just used to take in. I just wrote that to to transform a list of these message objects into something which says like user assistant, user assistant in just a nice little text summary. And you'll see why right now, as we come on to look at the evaluator code, we run these two cells and we will talk about the evaluator. So this is the evaluator. This function right here. It is a node. It takes a state and it returns a state. And it's meant to represent the LM, which is going to be assessing our our assistant, our worker, and deciding if it's ready to return to the user or it needs to go back for more. And so it all comes down to some prompting and let me take it through you, through it. Remember that we're using structured outputs that will require that the model returns a particular type of object. So first we take the most recent response which is of course the assistance attempt. We take that out of the state object the messages collection. So then we come up with this system prompt. You are an evaluator determines if a task has been completed successfully by an assistant. Assess the last response based on the criteria. Respond with your feedback and a decision on whether the success criteria is met and whether more input is needed from the user and then the user message your. This is going to be a bit more detailed. You're evaluating a conversation between the user and the assistant. You decide what action to take based on the last response from the assistant, the entire conversation with the assistant along with the user's original request and all replies is here. And this is going to use this little utility thing, which is just going to say like human, sorry, not human. It's going to say user assistant, user assistant with with the whole conversation so far. So it's going to look very simple in language. The success criteria for this assignment is and then I'm plucking out of the state this success criteria. And as I hope you've guessed, this is something that's going to be set right at the very beginning when we invoke the graph. So that's going to be passed in by the user. And it'll be maintained throughout our graph. So we can pluck it out and just insert it in the user prompt for this evaluator right here. And then I say the final response from the assistant that you were evaluating is this last response. And of course that will already be included in here. But I just want to be crystal clear so that the the evaluator understands that it's not assessing the whole conversation, it's just assessing this response right here, which is what's going back to the user. Respond with your feedback and decide if the success criteria is met. Also if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and unable to answer without help. So you may remember that we already put some of this in the definition of the structured outputs of the response, right up at the top. Let me show you that right here in the evaluator output. We already gave a little description of user input needed right here. And so you may wonder why I'm repeating myself here. And the answer is because there's never a harm in being repetitious with prompting. Be clear. Be instructive. Repeat yourself. These are good things to do in slightly different ways, because it biases the model to doing what we want it to do. Okay. And then finally in here I put if we've already got some feedback in the state object, that means that the evaluator was already called in this in this very loop and has already provided feedback in the past. And so I add in. Also note that in a prior attempt from the assistant, assistance you provided this feedback. If you're seeing the assistant repeating the same mistakes, then consider responding that user input is required. Now you might think this is very clever. How did you come up with that and why? Why that? And when do I use this kind of thing? And look, the answer is there's no magic here that is there. Because I was testing this and it kept messing up by the evaluators, sending back the same problem again and again. And so this is the kind of thing that is trial and error. You experiment, you try. When something goes wrong, you change the prompt and you try some more. There's no magic and no no clever rules to this. This won't always apply, but it applies here. And if you use a different model or slightly different tasks, you may find that you need to tweak this or use something different. And that is what AI engineering is all about. And that is what what prompting is about. And so yeah, the answer is it's research and development. Okay. We'll finish this off in just a second. So we then put the system message and the user message, which is called a human message object, together into one list called evaluator messages. And it's confusing because we're using this concept of system message and user message in order to talk to an evaluator. And this isn't actually a human message, it's actually something where it's a user prompt, but it's a message that we have manufactured. But that's just really how you go about building system and user prompts using long chains constructs. This is a a long chain construct within Landgraf. Uh, and so, you know, this is a still achieving the same thing. All right. Now we then take our LM, which is the evaluator LM with structured outputs, we call invoke with these messages. And what comes back will be an instance of that class evaluator output. It is that pedantic object filled up, uh, and behind the scenes what's going on is that it's been asked to provide JSON and that JSON has come back and that JSON has been parsed into this object. That's how it did it. Uh, and so we're then going to create a new state because we're meant to return a new state, and in that state we're going to respond. We're going to add to the messages, because remember, messages has the reducer. So whatever we reply here gets concatenated accumulated with the existing messages. We're going to shove in there that the assistant is replying evaluator feedback on this answer and something in there. Then we're going to give some, some feedback. Uh, we're going to uh, and so what we're doing here is we're taking the feedback from the Pydantic object, and we're putting that in the state. We're taking the success criteria from the Pydantic object, putting it in the state, taking user input needed, taking it from the Pydantic. The structured outputs that came back, put it in the state and return the new state. Hopefully you followed all that. If not, you will when it comes together. And then we've got another of these router uh functions route based on evaluation, if the success criteria is met or if user input is needed. Then end the super step. The super step is done. It's got a controller. It's got to pass back to the user. In either of these two extremes, either we've done great or we've done horribly. Either those extremes. We need the user to get involved again. But but if we didn't meet the success criteria and we don't need help from the user, then it needs to be passed back to the worker. We need to cycle back. The worker has got to try again and improve on this. Given this feedback, that is the whole idea. That is the workflow. And now we come to our graph. It's very simple and all of this is pretty simple. I've been making a bit of a meal out of it, explaining this step by step and talking about prompting, but it's not that hard. If you go through it yourself, you'll see what I mean. So this is our graph. We're going to add our worker node. We're going to add our tools node and our evaluator node that we just built. Now some edges we're going to have a conditional edge for the worker. We're going to use the router that we wrote the worker router. If it returns tools we're going to go with the node tools. If it returns a value later, we'll pick the node evaluator. We're going to add another an edge that goes from tools back to worker. Again. Remember this one. When the tools finishes, it's got a route back to the worker. That's kind of hokey that you have to do that. You think it would sort of be be done automatically for you. But but you have to be clear. And then another conditional edge from the evaluator based on its evaluation, if it wants to go to the worker, we put it back to the worker. If we're done, we're done. And then we also add a start edge to bring us, first of all to the worker. Let's run that and look at a picture of this. There it is. There it is. We have ourselves a true agentic workflow. Um, there's the start goes to a worker that optionally can run tools, which has to come back. There's a thick line, optionally a dotted line, an optional edge. A conditional edge is the word sorry. It will run a tool and then it will definitely come back an edge. And then when it's done, this is shown as a conditional because it's only if it hasn't decided to run a tool, then it will come this way and the evaluator chooses either to end in two situations. Either success criteria is met or user feedback is required, and if not, it comes back to the worker and or back to the worker. So this this diagram hopefully has everything coming together for you. And now scroll back up in the lab and just take a look through those nodes again. And at this point it should be like oh got it, got it, got it, got it. And hopefully you'll see that although I made a bit of a meal out of it, it is actually quite quick to build something like this. And you see one of Anthropics agentic patterns loud and clear here, although it's a little bit more than one of their workflows because this has like an infinite loop in it, it can keep going. And there's a lot of optionality here. So this is a true agent pattern because in theory this could just keep running and running. And it has some sort of agency autonomy over what it does. And surely you're now thinking, well, I want to see this thing and that is what we will do next.
------
Video 83. Week 4 - Day 4 - Building an AI Sidekick Using LangGraph, Gradio & Browser Automation
------
Okay, so first of all, we have a callback, a radio callback that radio will call into when we press a button, which will kick off a super step. So I've got this little thing here, which is a sort of cheeky thing that that comes up with a thread ID, a random big unique number to make sure that each time that we bring up the interface, it's not going to continue the conversation from last time or things can get quite hectic. So this this means that every radio session is going to have a separate thread. This also means that we could actually run this and have multiple different people that come on and use the interface and each have different conversations, which is really cool. So it's not just going to be for us this this sidekick. All right. So then the process message, uh, async, this coroutine, as I should call it, this, this function. Uh, so first of all, it creates a config based on the thread, uh, code that we're going to give it. Then it sets the initial state. So first of all the initial state. It's going to take the user's message, the message that comes in that's going to be the initial the initial message. It's going to take the success criteria as the, uh, the success criteria from, um, the the user that the user is going to tell us what what they see as being needed for success. And this state will go through all of the graph and it will be used by the evaluator, the feedback on work. Uh, this is what the evaluator can set. Success criteria met. This is false. To start with user input needed. This is false to start with. And then here it is. Here we do it. We take a graph we call a invoke. We pass in the state the initial state we pass in our config. And we wait for that to come back with what comes back. We then need to build the stuff that's going to appear in the Gradio user interface. We package up the user's message. We package up the the the last two things that will have come back. The second last one will be the assistance reply, and the very last one is going to actually be the evaluators evaluation. And we'll show them both coming so that they both appear in the UI, because it'll be nice to see the evaluation as well. But you could remove this if you don't want to see the evaluation in the UI. And then we package that together in the history with with all of this. And that's what we reply. There is a little bit of a confusion here if you're following all this, that we're sort of combining radio's history with the history that's being stored in Landgraf's memory, it would probably be be elegant, more more elegant here to actually unpack the full history from from the state. But that would start to get sort of messy. So I thought, keep it simple. It this this is fine. Um, and then I've also got a little callback called reset that just resets the interface. So we'll just run this that ran quickly. Check. I ran the other cells I did. And now we're going to launch our user interface. This is going to be a UI for our sidekick. Let's see what it looks like. Here we go. The sidekick. Personal coworker. It's got a nice, uh, nice shade of green I got for this UI. And, uh, we can ask a question and we can put the success criteria. So we will ask our sidekick the question. Uh, what is the current USD, GBP exchange rate? And the success criteria is going to be an accurate. Accurate answer. Simple as that. Let's see how it does. Here we go. Press the go button. Up comes a browser window. It's put in one as the amount. So it's driving this browser. It's looked for the, uh, for the dollar pound exchange rate. It appears to be on dollar euro. That's not a good sign. See what's happening back here? It's still processing. I wonder if it's going to work this out. Well, it seemed to do okay, but I don't see on the screen how it did that. Maybe that's down here. Or did it make a mistake? Let's see. Aha. No, it got it right. I see it scrolled down it for some reason it was defaulted to Euro, but it got the US dollar to GBP at 0.77463. And if we come back here let's see what it said. It did say 0.77463. So it did. It drove the user interface. It was a bit quirky there that it didn't pick the right currency, but it found the information. Uh, so the answer was the current USD to GBP exchange rate is approximately that number. The evaluator feedback from our other agent, the assistant, provided approximate exchange rate. However, the assistant should ideally note that it may change, so it gives some some stern feedback there, but it still chose to return control to you. Uh, so it's unclear. It thinks if it met our needs completely. Nonetheless, we saw it working. Uh, but now we need to see in Langsam whether the right messages were exchanged. Well, here we are in Lang Smith looking at what just happened. It's worth noting that it it cost us 19,000 tokens or 0.2 of a cent, a 2/10, a fifth of a cent. So if we look here, we'll see that a lot went on. Actually look at all of this stuff. So the worker uh, called a navigate browser tool, it called extract text. It called Get elements. Uh, so I guess yeah, this is, this is how it was doing its navigations and running. Uh, and then you can see down here that the this is where the these are all worker tasks. Worker tools. Worker. And then this is the evaluator getting called at the end, just as we would hope. And we can actually go all the way into chat OpenAI to see what message was sent to the evaluator. So here we go. So this is the results of that function that I belabored a bit earlier. System you're in evaluator determines if a task has been completed successfully assess the assistance last response based on the given criteria. Respond with your feedback, blah blah blah. Human you're evaluated. Conversation between user and assistant. You decide what action to take and then look at this. This is this is that that utility function that says user assistant, assistant assistant. And you can see that I made it. Just print tools use like this so that it's clear to the evaluator what's going on. The success criteria for the assignment is an accurate answer. So that's straight from the Gradio UI. And the final response that you're evaluating is that that is the same of course as this. And then this is the respond with your feedback blah blah blah blah blah. And we can see that what it responded with was JSON. Uh, of course. And it's JSON that conforms to the structured outputs schema that we provided. And that's why, uh, Langshan is able to read that in and populate that object. And that's what our graph then gets returned to it. and that is how the whole thing fits together. And there's the route based on evaluation taking us to the end. So it's kind of cool to see it here in locksmith and see the trace through the agent flow. And I can tell you obviously I've done some experiments with this with some of my prompts. Didn't didn't work so great early on. And there was times when there was like 25 different messages between the agent and the evaluator, particularly on some harder tasks. So it was quite interesting to see it bounce around and and a lot of fun. And obviously improving the prompts is one way to get there, but overall I see this as a really exciting project. You can really feel the beginnings of something very substantive here, and when you run this, you can experiment with giving it bigger tasks, asking it to summarize the various websites, to navigate around and to summarize back something to you. And there's lots that you can do here, which is terrific fun. And so that's going to put put this on pause for now, because when we come back tomorrow, we're going to try and make this into like a packaged project with a few more features. But let's go now to the wrap up. So I realized it was a long day today. We got through a lot, and I hope that you did have that same journey as me as seeing the visual there of that, uh, land graph graph and seeing the ability to build an evaluator and an agent that's able to talk like that and to go backwards and forwards, being able to give it playwright and be able to operate playwright like that. It's like we've built our own operator, the, the, uh, the agent from OpenAI, or that we built like a kind of agent. And it's been so easy to do it. And it's so cool to see the graph like this, to be able to interact with it in Langsamt. So as I say, I'm becoming I'm becoming a true believer. Uh, I'm becoming, uh, Landgraf advocate. It's exciting to see it come together. And tomorrow we'll try and take it one more step forwards. See you then.
------
Video 84. Week 4 - Day 5 - Agentic AI: Add Web Search, File System & Python REPL to Your Assistant
------
Would you believe it's already day five of week four? Here we are completing our sidekick project. I'm so happy! I love this project and I'm now such a fan of Landgraf. Over the course of the week, uh, we're going to be talking today about tools, tools and more tools we're going to be building in, uh, the same tool that we know and love already about searching the web, not navigating a browser, but doing the web search, uh, using the Serp API that we've used before. We're going to send push notifications because why not? We know how to do that. We're also going to add a tool to use the file system to be able to write files, so that we can have our operator agent that we're building our, our coworker that we have with our sidekick be able to read and write files from the file system. We're going to have it have access to Wikipedia so that it can look things up. And we're even going to give it the ability to run Python code itself. Not not in a Docker container like we did before. This time it's just going to have the ability to execute some Python. And so we're really giving it unfettered access to do things on our machine while we watch and while we monitor. And that leads me to a big point. Uh, the sidekick app that we're about to build is an experimental app. It's something for you to work with and to monitor and watch and use it for your own purposes. This is this is an app for commercial purposes for you, for you to be able to get more and do more with an AI agent at your side. But in doing so, it doesn't have guardrails. It's something which is, uh, open to the wild. And as such, it needs to be treated with some caution. And that's the first thing I want to say. Use sidekick at your own risk. If you're not comfortable with this, if you're not sure about the technologies behind it and so on, then please do remove the Python Repl tool that we'll we'll add. You should remove it and even remove the online navigation tools until you're, you're comfortable with what's going on. And you can watch it. This is an agent that we are allowing to roam free. Now of course when it's using the browser it's using it's using chromium, the open source version of Chrome. And it doesn't have access to any of our cookies or any of our passwords in a password manager or something like that. So it's not able to do anything like log in or it doesn't have access to credit cards and so on. And the file manager is is within a certain directory, so it can't roam free on your computer. The Python wrapper is that's pretty open. It can run any Python it wants, which would allow it potentially to write something that could that could do uh, do some damage. Uh, so you could remove that if you're, if you're concerned. But it does seem extremely unlikely. But nonetheless you need to be comfortable with it. You need to do it at your own risk please. The the other thing I want to say is that the sidekick app is a starting point. See it as a canvas. I've built something up with some prompts and some tools, and I've discovered that I can make it do great things for me. It actually did some work for me, some real work, some commercial work that I needed to do, and it did it for me and produced a report and it was something that I used. So. So it works. It really works. And it can deliver commercial benefit for you and be your own sidekick, let alone building it for other people. Uh, so see it that way. But you need to see it as this canvas. It's something which you need to to make suitable for yourself. You need to put in the sorts of tools and give it the kind of prompting you need. When I use it, I find that it sometimes goes awry, and I need to improve my prompts and make various changes to it to keep it on track. And that's what you have to do. It will require experimentation, and if you do so, it will pay dividends. And that's my final point. There's so much opportunity with this. This really unleashes a gigantic eye. But this is like it's like you're building your own madness. If you find if you find agents like, like madness from the Chinese startup that that was able to do things like hunt for good, rent in apartments and build websites and things, this is like your very own one that you are building and you are controlling and so you can make it do great things. Um, but it requires effort put into it to and it will, it will take some investment of your time and experimentation. And so I see that see it that way, the good and the bad. It's a it's a canvas. It's not a completed product, but there's so much potential. Well let's get to it. Here we are in uh, in Cursa, we're looking at, week four, of course, and now we're going to Python modules, which I know will please some people, but I'm not going to be typing code, but it's going to be looking at it but going through it carefully. So sidekick, the application is divided into three Python modules. There are they are here. And I'll first give you like a drive by of them. There is sidekick tools. And this is the module where we define the different tools that we will use. So it's nicely packaged up in one place that is going to get us access to a ton of tools, or it's going to give our agent access to a ton of tools, then sidekick. Now this is a bit of a long, a long, uh, module here, and maybe this should be broken up a bit. Um, but it contains a class sidekick, and that has the code that includes our worker, our evaluator, and the building of the graph. And we'll come and look through this. And then thirdly, there is App.py, and that is our Gradio app that manages the user interface side of it. Okay. So with that, let's just spend a couple of minutes on each on each piece.
------
Video 85. Week 4 - Day 5 - LangChain Tool Integration: Building a Powerful AI Sidekick from Scratch
------
Okay, so I am going to start by taking us over to the tools which we're in right here. So the way that the tools module works is it's just a module that's going to load in all of the tools that we've worked with in the past. Um, it starts with a loading in the dot env. Um, because we'll need the API keys, we set up a few constants for pushover. We create the Google Sherpa API wrapper so that we can use that API. Um, and then we have uh, the playwright tools. This is the tools for navigating for driving the browser, taking exactly from the notebook that we were looking at before. Uh, we, we use the async code to start the playwright instance, and we launch Playwrights Chromium, and then we get the toolkit, and then we return the tools from the toolkit. We also return this browser and the playwright object itself. And we do that because we need to clean them up at the end of it when it's finished with resources. And this is something that I'm not sure I. It remains to be seen whether this is properly cleaning up resources. I'm checking to see if it does. Sometimes I see that the playwright the chromium browser, hangs around for longer than I was expecting, but I think it does. It does close it down properly. Uh, but uh, yeah, we'll see if it if it ever causes like a leak of browser windows being, being left open. All right. And then this you should recognize because this is the push notification that we've used many times. Uh, it's just a simple function that sends a push notification using requests using the push over URL. And then there's also one here that's new. This is get file tools. And this is using the file management toolkit which is from right here from Langshan community. And this is an example again of one of the things that is so amazing about the Lang chain ecosystem. It's so popular. So many people have used it that there are tons and tons of tools that you can tap into. Now in the in week six, we're going to get very excited about Mchp, which of course has taken the world by storm. And Mchp is so amazing because it's sort of unleashed the ability to connect together with different tools all over the place. But already people who are part of the long chain ecosystem have had the advantage of a lot of of tools that can form to lang chains tools format, which in itself is like a sort of a mini MCP, but but just for people in in Lang chain and so we can take advantage of this now we'll have more in week six, but for now we have access to all of the ones that come in the Lang Chain Toolkit, which includes this file management, one which will give tools for our LM to be able to to mess around in a directory that I'm setting sandbox. And it will be it will be forced to stay in that root directory. So that's good. And then so so I'm going to put together these different tools into this, this um, uh, this this collection of tools here. So we're going to take a push tool. We're going to define a tool around that, that push notification. So this is like a homegrown tool to do that. Um, there's file tools that we've just talked about. There's uh a tool to run the Serpa search. So this is again us manually creating a tool around the search. There's something called the Wikipedia tool. Uh, so we create a Wikipedia API wrapper. Uh, we create that. And that needed me to install a Wikipedia Python package in our environment, which I did. And then this tool is able to call and collect Wikipedia pages using Wikipedia's API, which is freely available to everybody. And so that gives our LLM expertise about stuff through Wikipedia. And then I also create here a Python tool. Maybe it'd be nicer just to show that separately. Python wrapper. So this means this is slightly nicer. This means that we are giving our, our, uh, LLM the ability to run Python code, much as if you just typed Python at the command line in one of these kinds of interfaces where it can put in some code and get back the answer. So we're giving it that power. And this this is something which isn't sandboxed. So it's unlike when we did this with crew when we ran Python code within a Docker container. So it was somewhat insulated from the world. This is not insulated. And so this should be used with caution. And if you're not comfortable with it then you should comment this out. Uh, and so just just by or just remove it from here, I mean remove it from these tools. If you're not happy with your LLM being able to run Python code on your computer. Uh, and, uh, but but for me, I'm, I'm especially as I'm using GPT four mini, I'm quite comfortable that it's going to be sensible. And besides, I'll monitor it and be be be careful with it. So as long as you're careful, as long as you show caution, it should be completely fine. Um, but do be aware of what we're doing there. And if you're not comfortable, if you have any doubts at all, then remove that tool from the list. You have been warned. And you can also, of course, just, uh, remove the the the, um, playwright tools. If you don't like it, just have it return something empty there instead. Uh, instead of returning those, uh, those tools. Okay, so that is our set of tools, all the tools that we want to arm our coworker, uh, sidekick with. And you can just add more tools to this other tools list. Anything you put in there will just automatically get used by our sidekick. You can just keep putting more and more and more things in there, and you can Google or ask ChatGPT about some of the tools that are in the long chain tools and the community folders like community utilities and experimental tools and tools. You can look at these or you can just look at them on chains. Documentation. There's a lot to choose from and you can just put them all in here. You can have a tool that can look at your Google Calendar and attach to to Google, so that it could schedule things for you. You can have all sorts of tools. There's so many. And that's the beauty of this project that you can just keep giving more and more capabilities to your LLM, that to your agent. That's the idea. All right. Next up we're going to go to the big class which is sidekick. So now I've gone to the sidekick module. Here it is. So the good news is this should all be familiar to you, because it's just the same code we had in the lab just moved into a Python module, which shows how, again, if I can do another of these, like a pitch for, for using, uh, these notebooks, you can iterate on something in a notebook, you can prototype, iterate, perfect your prompts, and then move to a module and people from an engineering background will say, well, that's not the way that we write software. We have things like TDD. We have we have a very different kind of process. The thing about this kind of work is that it is much more experimental by nature. The mindset of an AI engineer is more about crafting prompts, trying different ideas, seeing how they work. And so because by its very nature, it is a bit more trial and error. And and it means that it lends itself very well to a notebook interface initially until you've got things better down, and then you move to sort of productionizing something in Python code like this. So anyways, this is the the module we define the state, the typed dict that is our state. It has the messages field which is the annotated field that that is a list that will use this reducer. And messages we have success criteria, feedback success criteria met and user input needed. These these are the things that we get back from our assessment and talking about our assessment. Our evaluation here is the structured outputs schema. The the schema for the output that we get from our LM. And we want feedback whether or not the success criteria is met and whether or not user input is needed. That's what we want back. And these descriptions are what will be provided to the LM so that it populates the structured output. Well okay. And now we have a class called sidekick. And it's a it's quite a quite a big one. And maybe this could be could could warrant some refactoring. But it's basically everything we had in the notebook. Um, there's one fussy thing about working with async code, which is that the init method, when we create this, we don't want that to be to be async. Um, but we need to be able to do some initialization that will be async, like setting up our graph. And so we have to have like a separate async, uh I can say async method, but a coroutine uh, that, that is going to be handling that part of it. And we're going to need to make sure when we when we initialize a sidekick that we can first instantiate it and then call this setup asynchronously. Um, so first the first thing I do in this setup is I call that playwright tools function coroutine that we saw in the other, um, in the, in the sidekick tools. And then I populate my tools, my browser and playwright, and then I add into tools the other tools that we also put together in the other module. Okay. And then I create my worker LM, which is in this case GPT four mini. But feel free to switch it up and bind it to tools and store that as an instance variable. And then an evaluator. LM and I also store that as an instance variable. And then I call build graph. That's going to be the the the big part of that's what we've always said is the five steps that need to happen before you can actually run your graph and do your super steps.
------
Video 86. Week 4 - Day 5 - Creating AI Workflows: Graph Builders & Node Communication Techniques
------
Okay. And so what we have now is the worker node that is defined right here. And it's quite long. Um, and uh, it doesn't even fit on one screen. I think I expanded the font size a bit so that, uh, but that does have that drawback. Maybe I will make it a little bit smaller for a second, and we'll make it bigger again in a minute. Uh, go down a little bit so you can at least see it all in one screen. Uh, so this here is the is the, uh, is worker. So it's got a pretty meaty system message, which I have built up based on my experiments. And you will need to keep doing so too. I added in here the current date and time. I actually, for a bit made another tool to to come up with the current date and time, but I realized that then I had to come in here and prompt it to be sure to use that tool. And then I thought to myself, okay, this is silly. I always want it to know the current date and time I put that in as a tool, and then I'm telling it in the prompt to be sure to use that tool. And that's just such a waste. This is an example of something that doesn't need to be a tool. It needs to be inserted in the prompt every time. It is what we'll call a resource when we when we get to MCP time. But it's like it's like a piece of information that we want to add in there. So I just shove in the current date and time in the prompt. I also, I had an interesting thing happen when I was using the Python tool that I saw that for some reason, GPT four and mini misunderstood how the tool worked and thought that the code it would put in there, whatever that evaluates to, would be what it would receive in the response from the tool, where in fact, you had to put in a print statement in that code if you want to actually get back some text. And it didn't do that. Uh, and so as a result, it was going backwards and forwards, trying to rerun again and again and not getting any results and seeming confused. Uh, and uh, so I added in this line, you have a tool to run Python code, but note that you need to include a print statement if you want to receive output. And then it started to work immediately. So maybe that's just a GPT four a mini anomaly. Maybe if I use the big GPT four, it would be fine. It would have figured it out or it would just know. But or maybe the way that the tool, the description in the tool isn't clear enough. And I could always wrap it in a, in a, in another tool that would make that clearer. Um, but uh, but regardless, this, this fixed it. And this is such a good example of the experimental nature of this kind of work that you need to be able to come in and just shove something like that in the prompt. Maybe this won't be needed for what you do, but it was needed for me. Um, and there was another example of, of where I came across something similar. Um, and, uh, yeah, you'll probably find it yourself if you if you look through this, you'll see other times when I had to tweak things, uh, to, to handle some cases. Um, so anyway, other than that, this is all identical to what we have in Jupyter in the, uh, notebook. Let me expand here. Um, okay. And now this is the router, the worker router, the decision, the condition on whether or not the worker should go to tools or to evaluate it. This is the utility method that converts our messages into a nice user assistant. User assistant. And finally not not not finally. But the big the big guy is the other node the evaluator. And this has again, a lot of pretty substantive prompting that I have tweaked over time. That describes you're an evaluator, what you're there to do. Uh, it's got the formatted conversation, the success criteria, the last response. And then, uh, responding with your feedback. Uh, and, um, I remember now, I added in this, I noticed that the evaluator was quite harsh and never seemed to sort of trust that what the, um, assistant said it had done, what the worker said it did. The evaluator always said, you know, I don't know if this actually happened. So I put I put in here the assistant has access to a tool to write files. If the assistant says they've written a file, then you can assume that they've done so. Overall, you should give the assistant the benefit of the doubt if they say they've done something, but you should reject if you feel that more work is needed. So you know, again. And maybe this time I've gone too far and I'm going to make it accept too many times. It's something that requires constant tweaking and refinement as you find cases that are that are that work or don't work. And of course, it's always good to add an example to give real concrete examples. And there's some trade off, because the more information you put in here, uh, the, the harder it is for Gpt4 to be coherent because it's just got a lot more information to absorb. Uh, but, um, yeah, it's definitely it's definitely something that I found that giving these kinds of hints and examples has helped me get better outcomes. And you will need to experiment. Okay. So that is the evaluator we've then got at the end of it, I'll just mention again, remember at the end of the evaluator, we, we we evoke the LLM with output. And because it's, it's one that has a structured outputs which is what that with output means, uh, it returns back an object, an eval result object populated, and then we pluck out the fields of that object, and we populate them in our new state, and we return the new state as all nodes take an old state, return a new state, and then this route based on evaluation, this is again another of these condition branches. We take, uh, we see whether either the success criteria is met or user input is needed. In either of those situations we need to end, but otherwise we're going to bounce back to the worker to give it another shot. Okay. And then here is the build graph. And this after I made such a song and dance about this in the first couple of, of uh, of days of this week. Now this is like the easy part of the whole thing. We create our graph builder for, for the state of the class that we have created. And then we add our worker, we add our tools, we add our evaluator, the three nodes. We add our our edges. Uh, um, conditional edge. This is not a conditional. This is the if a tool is run, it needs to come back to the worker a conditional edge to choose between the worker and ending and the start going into the worker. And then we compile our graph. Okay. And then I've got this run super step function which is the one that actually invokes the graph. So run super step then is pretty straightforward. Uh, I've got uh, the uh, the random Uuid I've set as, as an instance variable sidekick ID. So I set up the config this way. And then the state the initial state that we will use to invoke our graph. It is the message from the user for the success criteria. It's either the success criteria that's passed in or if that's if that's not set, if it's null or an empty string, then I use this default. The answer should be clear and accurate. Um, feedback on work is set to none. And these two are both false initially. And then we call our graph a invoke to kick it off. And then we pluck back the user's thing, the user's message, the reply, and the feedback from it. And we construct our history and that is what we reply. And then I've also got this at the end like a clean up function. And as I say, it's kind of as you'll see this gets called when resources get cleaned up. And I'm not 100% sure if this is always cleaning everything up. And so over time I will I will try and keep an eye on this. And I may refine this as as we go, as I get, get to see whether this is properly cleaning things up. So if it looks a bit different when you're looking at this code, then I might have found a better way to do this that's more reliably cleaning resources. Um, after after they've been used. And I'm talking particularly about, of course, about the browser that we spawn this headless browser. And the thing to be aware of is, okay, once we've done that, if we then kick off a new sidekick process, it spawns another browser. What have we done to that first browser? Have we closed it? Have we quit the browser that's running behind the scenes? Uh, or running in front of the scenes as it would happen? Uh, so, um, yeah, I've, uh, put this in to do that. Okay. And now on to the user interface, the app. ------
Video 87. Week 4 - Day 5 - Creating Isolated User Sessions in Gradio Apps Using State Management
------
Okay, here we are looking at app dot Pi, which is our Gradio app. We only have two imports. Import Gradio and from sidekick. Import sidekick the class. And so uh, yeah, the idea is so. So this is the way that you build a Gradio app. Uh, you have, um, this blocks, um, and then you can create your fields. And as I said, it's not this isn't a gradio class. So I'm not going to go into too much detail. I want to give you some intuition that you create fields like a chatbot field, which is where we have our sidekick, we have our message and our success criteria, and we have a go button and a reset button, um, here. And, uh, yeah, the thing I wanted to mention, the way that Gradio works is that you have a bunch of these callbacks that where you have things like, um, you have go button click, and that means if you click the go button, it will call this function, and it will call it with these inputs. And it will hook up these outputs. And when you get used to gradio, you see it all starts to click and you see how you can just simply think of everything in terms of these callbacks. And everything here is sort of generated and runs in the front end in a browser, except these callbacks are called back to your actual server that's running right here. And so that's how the whole thing works. Now there is a callback called load, which is one that is called at the beginning when a new screen is brought up and loaded. And that's an important one for us here because that is when we're going to call a callback called setup, which is going to return something called sidekiq. And that is a something that we're storing in the state of this gradio. So this is something the sidekick. It's a state. It means I'm going to set it through various callbacks, and I'm going to provide it in callbacks so that that's going to be associated with a particular user of this screen. Um, and I've also registered this delete callback free resources. And that's how I'm hoping to clean up this playwright and chromium browser that's running associated with this resource. But because we've done it this way, we've done it quite carefully with Gradio, which I've I've not been careful with some of my prior Gradio projects. Also in the LM engineering course, no one's called me on this yet. But but in the big thing we do at the end, I don't properly use gradio state, which means that if you brought up multiple screens or if you had different users trying to use your platform, and it is only intended to be used by the individual themselves, but you could get in trouble because everyone will be sharing the same like like variables. So this makes sure that different people using the screen, if you were to to supply this as an app, they would each have their own separate session with their own variables. So in this case we initialize it by calling UI load. So if we go to the load callback sorry not not the it's called setup. It's the callback. Here it is uh setup. And what that does is it instantiates it creates a new instance of sidekick. And remember that that sets up it just has a bunch of of things in the init to get things ready. The big work is done in sidekick setup, the async method that we await right here. So that now populates all of the things in sidekick, including building the graph. So this is where all of the graph gets built. The nodes are put together. Everything happens here. These are all of the steps, the five steps that happen before the graph is invoked, before any super step. And then this callback returns sidekick. And that means that sidekick is hooked up to this, also called sidekick. This variable also because this is the state object associated with the session. So that means that the sidekick that we created right here gets associated whenever you load a user interface. That user interface is associated with this particular sidekick Instance. Okay. Hope that made some sense. It doesn't need to make you just just get some intuition for it and understand the basic plumbing and that will that will be all that you need for now. Okay. And then I just wanted to mention the big thing here is that there's a go button, and that go button says go on it. And if we look at go button.click we'll see what that does. It calls Processmessage, which we're just going to look at in a second. That is a callback. And it calls that with the message that the user has just entered, which comes from from here, uh, with the success criteria which the user has entered, and with all of the chat history and what comes back needs to go to the chat history and to the sidekick object to make sure that we keep that updated in state. Uh, and so, uh, and I actually don't think you need to do this, but I feel like it's, it's, uh, more consistent to do it that way to, to repopulate that in the state. But if you're a gradio wiz, you know what's going on here, then? Probably like me, you know that this is probably not required. Um. But anyway, I digress. This this is the process message. Uh, callback. It's perfectly straightforward. Uh, it takes everything that I just mentioned that's hooked up to to Gradio, and it simply calls the run super step method, a coroutine that we looked at just a second ago on our sidekick object. The thing that we that we instantiated so it calls run super step. It passes in the message, the success criteria and the history. And that is going to run a super step of our graph. And then it's going to return the results back to the user. And that that is the tour of the app. It's the same code that we had in Gradio. It's just sorry in the notebook, same code we had in the notebook, just with a bunch more tools, a bit of a built out prompting there to handle some some troubles that I had. And you will have to do a lot more, I'm sure. And with that, it's time for us to give this thing a drive.
------
Video 88. Week 4 - Day 5 - Inside AI Feedback Loops: Seeing How AI Evaluates & Corrects Errors
------
And just before we take this thing for a drive, let me just mention that I made a little mistake here when I was changing the tools I left off those brackets. Uh, and that caused it to crash. So you may have spotted that, in which case you should have shouted, uh. I, uh, have now fixed that after a few minutes of messing around and, uh, remember, when things stop working, the first thing he should be suspicious of is what did you change? Uh, so now let's run this. So we run it by bringing up our terminal control and the backtick, and then we move into the fourth directory, and then we don't do Python. Uh, we do. You've run and then app dot Pi to bring up our application. And here it is. Here is the sidekick personal co-worker at your disposal. And I'm rather hoping that what you see is a bit different to this, because I'm loving this project and I'm fully planning to keep working on it, uh, and to keep improving it in time. And so don't worry if what you see doesn't match this video, because you should revel in the fact that it'll have more functionality, more ability, and and if it doesn't, then please help. Come and help me contribute. Make some tools, make some features and push them so this can look better and better. Uh, anyways, now let's set some challenges. Okay, let's start with what is pi times three? Let's let it have a think of that. And the funny thing is that, uh. Well there we go. Uh, this is, um. Well, that's interesting response. Addressing the previous mistakes about rounding. So, uh, what we're seeing there, I haven't seen this before, and this is something that we'll have to, uh, have to, uh, look into, but the. So we're seeing a value of pi, um, which is great. And I like the way it's put pi with the Greek symbol and of pi times three. And we're seeing in the feedback, the final response accurately states that it's that this is correct and precise, addressing the previous mistakes around rounding. So I wonder what that even means. It sounds like, unbeknownst to us, the agent and the evaluator had a conversation about this. Let's go and take a look. Wowser there's a lot of information here. It had it. It really went to town on calculating pi times three. Uh, and, uh, see that, uh, it's still we still ended up spending, uh, about 0.1 of a cent, so a thousandth of a dollar. It didn't cost us very much. But, uh, it's interesting that it had it went, went on quite, quite a rigmarole here. So let's see what happens. Uh, so this is, uh, so first of all, it went to, um, wow. It did some looking on online first of all. And then I think and then it gets to, uh, using, uh, the Python, the Python wrapper and it gets that. So it, um, it uses, it uses print results. So it does follow my instructions because previously it would have got just an empty string there. So it gets something back there. 9.4477. Is that what what we see when we, uh, look in the. Yes it is. Okay. Okay. So it did it did faithfully get there in the end. Uh, so now we keep going down. Let's see what happens when the evaluator evaluated this. So, um, based on this, it says the answer provided by the assistant is incorrect. The result of this should be approximately that, not 9.425. Let's see. So. Oh yes, it replied approximately 9.45. The assistant rounded the answer too early or incorrectly. Therefore, while the intention was to provide an approximate value, the precision was lacking. And so that's great. That's this is amazing. So, uh, Were the, um, the the assistant gave an answer that was too imprecise. It wasn't to enough decimal places, which annoyed the evaluator that rejected it. And so then, uh, for some reason, the the assistant decides it's going to go back and run the tool a second time. It gets a syntax error. Let's see that syntax error. Look at that. It ends with a curly instead of a closed bracket. So it makes a mistake. It uses it again. This time it gets it right and it gets a good output. And that is then what it what it sends back. And now we see the evaluator, uh, and uh, the response is what we already saw in the screen. Uh, that and that now explains exactly why. Because the evaluator, of course, sees the whole history of this backwards and forwards, which we don't see. Um, and, uh, so, um, yeah. And if you look at what the evaluator got to evaluate, this is the result of that function. I wrote that utility function with the user assistant, user assistant. And you can see that it says user. What is pi times three? Assistant uses the tools and then gets that approximate thing. And the evaluator feedback says that's no good. The assistant uses the tools and gets a syntax error. Uses the tools again and gets the accurate number and then gets the value right there. So this is really fascinating. What what a journey to see this happen. It all happened in a couple of seconds. So we weren't aware that this uh this syntax error this this feedback was happening and we got to the right answer. So, uh, that's that's a very, very cool to see. Okay, I press the reset button, which creates a new instance of sidekick and it rebuilds the graph. So we've got a completely fresh one here. And I'm going to paste in a question I'd like to go to dinner tonight in a French restaurant in New York. Please find a great French fresh restaurant great French restaurant and write a report in markdown to dinner, including the name, address, menu and reviews. Send me a push notification with the restaurant name and phone. Let's make that tomorrow. Let's be fair. It's getting late. All right, let's, uh, let's give this a try. Now, there is a risk here. We will see what happens. The the risk is that. And what's cool that it's just popped up a browser window. It could have used the Serpa API to be searching that way. Or it could bring up a browser and drive it this way. So I'm not sure what it's going to do. And it might bring up the browser, but decide not to use it as it appears to be doing now. So it's got. Oh, and it's finished. It's finished. Uh, I've uh uh, hang on and see if I've got that pushed. I have my phone on silent so we won't have heard it. And I have indeed got a push and it's pushed notified me about Le Bernardin. So. Uh oh, it's chosen some snazzy French restaurants. I'll have, you know. Um, so, um, let me see. It's compiled a report. It's named it. The report includes detailed information about Le Bernardin, Balthazar and Daniel, such as their addresses, menus and reviews. And those, by the way, are phenomenal French restaurants. But, uh, Le Lebanon is like a three Michelin star or two. Daniel is two, I think. And, uh, Balthazar is obviously super famous and popular and celebrity haunt. So these are all like, uh, top French restaurants for sure. Uh, it probably found like some website with, with top French restaurants. Um, it did indeed send me the push notification with the name and the phone number, but we will need to verify that the phone number is accurate, which we will do right now. Um, and, uh, we will also go and, uh, uh, check the file. Let's do that right away. Okay. Well, let's start with the phone number. So I've got the my push notification in front of me. The phone number for Lebanon. According to this push notification is (212) 554-1515. Let's see what comes up. If we Google that, we get Lebanon. Fantastic. So well done our sidekick. And now we need to see whether or not we've got a file. So if we go back to cursor and look at the file in the sandbox, there is indeed a file called dinner. Let's bring that up in a preview mode and let's get rid of the terminal. Here it is. Dinner report French restaurants in New York City. Le Bernardin highlights and Daniel and a summary. There we go. So that does seem to be a successful report, nicely formatted, nicely written to my file system. And I know from from trying with this that I can then go back. I can then ask it to flesh out one. Well let's do let's do a little bit more. Let's go one step further and then I'll leave with you to experiment with yourself. Okay. So I'm going to add in here. Please update the file to only contain information about Le. Bernardin. Oops. No, I didn't get that right. and include as much information as possible. Retype it. BR. BR. There we go. Get as much information as possible. Um. And include maybe that's going a bit too far and include um, um, more details including, um, some extracts of reviews, a summary of reviews, a summary of reviews, and some menu items. Okay, let's give this a shot. Go. Let's give it a few seconds. Okay. It says that it's been doing that. It also says it's sent me a push notification. See if that's true. Yes. It has. Uh, okay. Let's go and have a look in here, and we'll just re load this open preview. Hang on. Maybe I need to, um, close the screen and open it up again. Try this one more time. Hold on. It is indeed now just on Le Bernardin. It has. So it updated a file on the on the drive in its sandbox, which is pretty cool. It updated it with more details. The cuisine menu highlights ambience and summary of reviews. Impeccable service. It is three Michelin stars. So is, uh, right. Good. Uh. And uh, yeah, it's got some menu highlights too. So it has improved the report. But the thing that I really want to show you is that it knew how to update a file that it had written before, and I also it shows that it understood the memory because I just said the file, I didn't name it. And of course it remembered that it had been called dinner, which it was getting from Landgraf's checkpointing. So there you have it. There is our sidekick in action, uh, doing plenty of things. And, uh, and of course, it can do a lot more. You can keep experimenting. It can do a lot more just with the tools it's already got. It can build files, it can write reports. The specific thing I did for a work thing is I had it go and search and browse a number of different sites, compile them and produce a report. And it did all of that. Uh, and it was really impressive. If you want it to make PDF reports, you can add a tool that just converts markdown to PDF. That's the trick. Uh, because the these, um, these, uh, llms are great at generating markdown and they would have a harder time building a PDF, although they could do it, but it would be harder. But make markdown and you can write a tool that just uses a Python library to convert markdown to PDF. And so then you can build those kinds of reports as well. And it can do real work. It really can. And you should put it to the test.
------
Video 89. Week 4 - Day 5 - AI Assistant Upgrades: Memory, Clarifying Questions & Custom Tools   
------
And so I hardly need to tell you your challenge for this week. Your challenge is to make the sidekick your own, build it out, add in tools, add in some tools that are actually specific to things that you do that are able to to give it powers to do things that you want to do. Now, some other more fundamental ways that you can improve it. One of them that I realized is a note to take from the way that OpenAI does theirs is we could encourage the, the, the llms to start by asking clarifying questions. You could have it so that the first thing it does is it asks for three clarifying questions, and then it uses them as part of its ongoing work. So that would be a really important change. Another change would be to build out the graph a bit more. We've put a lot of power into that one assistant, and the problem with that is that we're expecting a lot. We're expecting one model to to be able to to take a lot of different tools and be very flexible. And the danger is that it loses some of its abilities. It loses some coherence because the context is so long, because there's so much it can do that it gets stuck. And you can improve that by adding more agents to the mix. For example, you could have a planning agent that first of all, that's the first. The main assistant is also a planner and decides on what tasks need to be done, and then delegates them to the execution to the worker. Um, and so you could do it that way. These things all have pros and cons. The benefit is that you've then you're able to divide a bigger problem into smaller steps, and the autonomous agent can figure out how to do that. The downside is that it makes it harder for the agents to kind of plot its own course. And based on what comes out of that first task, maybe it would decide to do things a bit differently after that. So by by planning you sort of you lose some of that, that freedom that you get from doing it all with one agent. So these are all things to experiment with. There's no right answer. More than anything else. You need to be willing and ready to experiment. But add on tools, add on a sequel tool that's easy to do. And on the topic of sequel, you should also improve the memory. The memory we're using is just the basic in-memory memory, and you should change that to be the sequel memory that we already did earlier in the week, and just put that in there, and that way it will be able to remember who you are if you come back, if you use radios, uh, feature to be able to log in and identify yourself, then you could use your username as the conversation thread and be able to continue a conversation and keep tasks build up a sort of library of tasks that you have so, so much you could do with this. It's a it's like I'm, I'm rather than giving you specific instructions, I'm asking you to be autonomous with this. Go and show your own agency and build this into something amazing, absolutely amazing. And then share it. Post about it on LinkedIn so I can weigh in and see examples and and show me the tools. Put them in community contributions so that people can add them in, put in your tools, libraries that people could import and other functionality. And I just can't wait to see where people take this project. And I hope, like me, that you're something of a convert at this point. Maybe you always did like Landgraf, but but now, uh, now I'm drinking the Kool-Aid. I'm really, really happy about this week. It's been very exciting. And with that, it feels like we were only just starting week four. And now we're saying goodbye to week four, goodbye to Landgraf, and hello to week five as we move on to Autogen from Microsoft. And there's a lot to show you I can't wait. See you for week five.
------
----------------------------------
Section 5: Week 5
Video 90. Week 5 - Day 1 - Microsoft Autogen 0.5.1: AI Agent Framework Explained for Beginners
------
Now, look, I know exactly what you're thinking. You're thinking, oh, another framework, another week, another framework, another research thing to learn. And I just want to get on to MCP. That's what all the hype is about. That's week six. Why do we have to do another framework? And I have lots of good news for you. First of all, this framework autogen what we're doing this week, it's going to be really quick and simple because it's got so much in common with the others. We're going to go through it quickly. Secondly, generally this week is going to be quite quick. I know I said that last time, but last week I got so into landgraaf I couldn't resist. But this week it's going to be nice and brisk as we get to the to the pinnacle of this course in week six. But then the third piece of news is that actually we are going to touch on MCP this week as well. So it's like there's going to be like a preview. So with all of that we are at week five, we are understanding Autogen concepts Autogen from Microsoft. And let's get into it okay. So introducing Autogen, it looks like this with that logo AG like that. So Autogen from Microsoft, an open source framework. It was released. The 0.4 version of it was released in January, and they explained this as a from the ground rewrite, adopting an asynchronous, event driven architecture to address some of what I imagine was the criticisms that they were facing before. Observability. This is a common agent issue about do you really understand what's going on with these agent interactions? Flexibility, control and scale. So this is a straight up replacement for Autogen 0.2 with a very different kind of feel and architecture to it. And so you know, obviously I had to make a difficult decision for this about whether we were going to use Autogen 0.4 or 0.2. And I decided to to stick with the original 0.2. No, of course I'm lying. We're going with 0.4. Why wouldn't we? Actually we're not. We're going with 0.5.1, at least as of now, because that's the current version of it. They've already gone past 0.4, but naught .5.1 isn't very different to to 0.4. It's not got the radical change. So yes, we are using the latest autogen, the new refreshed version as of this year. And just be aware of the fact that there might be if you do some of the if you look for documentation, you need to be very aware of whether you're looking at the documentation for 0.4 plus or for the 0.2 versions, which looks and feels quite different. But wait, there's more. There's more to the story. There is some drama to tell you about. So late last year, the original creator of Autogen and a bunch of the the co-founder of Autogen and a bunch of people involved in it left Microsoft where it was being built and open sourced, and they split off and created a fork of Autogen, a different version of Autogen managed by this group of people. And the the one of the creator is now at Google and is now working on this version. Um, and um, it's called AG two I guess, for Autogen Gen two, but it's also given the name agent OS two. And confusingly, they started from Auto Gen 0.2, the earlier version of Auto Gen, and so it's compatible and somewhat consistent with that other version of auto gen. And it has broken off from Autogen 0.4 that Microsoft released after the split after the fork. Uh, and so it is like a version that's, that's more common with what it used to be, which is super confusing. And the reason that was given for this was partly to to be able to move more, more quickly and more flexibly, to not be under the kind of corporate bureaucracy that is Microsoft. But having said that, being under the Microsoft umbrella also comes with lots of benefits. The main auto gen, Microsoft's auto gen is very, very well used. It has a lot of enterprise clients. It's used quite broadly. Uh, and so obviously this is a difficult situation. Uh, it's particularly difficult. And Microsoft have gotten this bizarre situation that they've also, as they've put it, sort of lost control of the discord chat group that was for Autogen that is now controlled by the Ag2 people. So a lot of the community discussion is about ag2 and people that come new to the Autogen ecosystem that don't know about this are super confused, because if you look up documentation for Autogen, you might find yourself looking at this which Ag2 which would be based in Autogen 0.2 the earlier version. Or you might find yourself on the Microsoft track on the official supported. Uh autogen from Microsoft and Microsoft have made it very clear indeed that they have no plans to take the foot off the pedal in terms of pushing forwards with Autogen as as seen by Autogen 0.4. But meanwhile, the Ag2 camp is saying, look, we can be more flexible, we can do things faster. And they have been turning out releases and they are currently if if release numbers are anything to go by, they are currently, as of right now on Ag2 Agg2 version 0.8. So they've got many version numbers. Not that that means much, but but at least sort of optically, it sounds like they're making swift progress, which was one of their original drivers for breaking free from Microsoft. So it's quite a lot of drama. And just to add one more spicy element to it, the Autogen two people, the Agg2 people also have control as they say. They have ownership of the official package in PyPI, which is where things are controlled for doing Pip installs. And if you do a pip install of Autogen, you get Agg2. You don't get Microsoft's official Autogen, which is which is kind of amusing. It's also obviously problematic because people new to Autogen find this super confusing. I mean, nowadays one tends to just do pip install and just expect that you get what you get, particularly with a product from Microsoft where you would think that of course they would own Autogen, the official PyPI install, and if it if not, then you'd think it would be something completely different. But the idea that you do pip install Autogen and you get a kind of offshoot of Autogen built by by some renegades, a well, some very important renegades, uh, who the rebellion that has built something obviously very impressive. Uh, it's, uh, clearly quite, quite bewildering for someone new to this. But anyways, I've clearly I've made the decision that we're going with the Microsoft track. That is the one that that has, I'd say by far the bigger community and traction right now. But I bring your awareness to this. You need to understand this and watch out for confusion in documentation and so on. Uh, with Ag2 now, because we're using UV as our environment, I've already set all the right projects. So what will be installed in your environment is going to be the official Microsoft Autogen, which for me right now is at naught .5.1 and will probably be be making quite quick progress itself then maybe not at 0.8 just yet. That's the story. I hope that makes sense.
------
Video 91. Week 5 - Day 1 - AutoGen vs Other Agent Frameworks: Features & Components Compared
------
And so with that, I'll just move on to talk about what Autogen actually is. Let's remind you, these are the frameworks we've been going through. Autogen the last of the frameworks, uh, before, because MCP isn't really a framework. Uh, okay. So what is autogen? So Autogen, as with the others, it's a bunch of different things all wrapped under one autogen umbrella name. It's first of all, something called Autogen core. And Autogen core is something which is like, uh, it's agnostic to the, to the framework that the actual agents and llms that are being used. It is like a general generic framework for building scalable multi-agent systems, something that manages things like messaging between agents, even if they're distributed in different places. So it's like a fabric for agents to, to run within. It's something different. It's got some stuff in common with, with an idea like Landgraaf, but much, much simpler. It's got uh, but but basically it's like a runtime, an agent runtime for running agents together. That is autogen core autogen agent chat. It's quite a mouthful. Autogen agent chat. That is a framework that is going to be very familiar to you, because it's very similar to OpenAI agents SDK and to crew. It is a lightweight, simple abstraction for putting together llms in an agent construct to allow them to use tools to allow them to interact with each other, that is, Autogen agent chat and then built on top of Autogen agent chat. Agent chat itself is built on top of core. Built on top of agent chat are a couple of offerings. There's something called studio, which we've heard that name a few times now that is Autogen low code, no code app for sort of constructing things visually. And there's also something called Magentic one, which is like a product. It is a console. It's something you use in the command line, which can take care of things for you and do them. So it's like an application. It's a it's a application to run an agent framework that's out of the box that's canned, that runs from the command line. So these are the various bits and pieces that make up Autogen. And uh, all of this is part of open source. It's part of it's sort of managed by Microsoft Research, but it's available. There are people who contribute from all all over the place. And it's an interesting point that whilst when we talked about crew and and Lang Chain, we saw that there was perhaps the possibility that that monetisation commercialisation was driving some of their product roadmap. There's a different angle here with Microsoft. This is all a Microsoft research for the community kind of thing. And so you don't necessarily see that things like Studio and Magenta X1, Magenta studio is considered like a research environment. It's not it's not considered ready for production. And they state that very clearly all over it. So it is a different positioning. This is a big open source contribution, uh, through and through. And of course, no surprise what we're going to be focusing on is going to be the, uh, agent, uh, Autogen core and the Autogen agent chat, those two parts of the puzzle. We're less interested in the low code. No code, because we're coders. That's what we do. Uh, and, uh, magenta one, it seems, seems like a cool thing to try out, but it's. I guess it's a bit like, uh, someone building what we've already built. Uh, in the end of last week, uh, with with our co-worker that we have our sidekick. Our personal sidekick. Uh, so one more point to make here is that we're going to be focusing mostly on Autogen agent chat. Uh, because that is the sort of direct comparable with Crewe and with OpenAI agents SDK and with the sort of agent interactions part of Landgraaf. We will also look at Autogen cause some pretty cool things about it, but but more to give you some interest in it and to experiment a bit. Um, not as much as agent chat. And even when we do, we'll be using agent chat as well. So that's the lay of the land. And to talk about the concepts behind, uh, the, the Autogen framework and particularly behind Agent Chat that we're now going to talk about. What are the building blocks here? Well, they're going to be very familiar to you. They are straightforward and things that we've already met before. So they have a concept called models. And their concept called models is similar to Llms that we've seen in other platforms. Messages I guess it's new that they identify this as a core concept. Messages that can represent messages between agents, or it can represent events that happen within an agent's interactions, which which really is referring to, like calling tools. Those are also considered messages. And so we'll see some some stuff about messages that can be from a user to the model. It can be between agents. It can be within an agent when it's calling tools. But those are called messages and agent. It's going to be what we're used to. It's something which has a model behind it and something which is able to carry out a series of tasks on behalf of, of a request, on behalf of a user or another agent and then teams again. That's familiar. This is like a crew from crew. Uh, it'd be a group of agents that can interact to achieve some goal. So these are the sort of core building blocks and there are others. But but I think these are probably the ones to introduce. And for today we're going to look at the first three models messages and agents very briefly very quickly to set up a quick example. But we'll do something new. We'll involve some SQL in it because I know that's that's going to be useful for some people. So let's do that right now.
------
Video 92. Week 5 - Day 1 - AutoGen Agent Chat Tutorial: Creating Tools and Database Integration
------
And welcome back to cursor and welcome to week five directory. Here we go. We're going to the first lab. Week five day one Autogen Agent Chat, which is the main part of Autogen. That's sort of comparable with, say, crew. And a lot of what we do right now is going to look very familiar because it's very consistent with crew and OpenAI agents SDK, particularly this first thing which we always do, which is to load the env as usual. All right. So the first concept then is the model. And the model which is similar to concepts like LM that we've had before. It's like a wrapper around calling a large language model. And here we import something called OpenAI Chat completion client, which is the wrapper for for the LM we'll be using, which is GPT four mini. And this is how you create your model client as it's called. And it's very simple indeed. And you just pass in the name of your model. And so let's run that. And also I just want to show you here that you could do the same thing with Obama to run a local model like llama 3.2. It's just exactly the same idea. You could run that and you could continue all of this in exactly the same way, running locally instead of using GPT four or mini. Okay, that is the first concept. The second concept is the message. This is something which is a different concept for Autogen agent chat. It's this idea that you create an object called text message. In this case that has the content. I'd like to go to London is my message right now. And the source is the user me. And so if we run that and print it, we see it's a text message. The source is the user. And the, uh, there's the content. And now that is all there is to it. That is the message. The third concept is the agent. And it's very similar to things we've seen in the past. The the thing that we import is called assistant agent. That is the uh, you'll see this many times. This is the kind of most, the most fundamental class that will work with in in Autogen agent chat. So we create a new instance of assistant agent. We give it a name, airline agent, we give it a model client, the underlying LM, we give it a system message rather like the instructions in OpenAI. You are a helpful assistant for an airline. You give a short humorous answer. So let's give it that to see what that does to it. Uh, and uh, model client stream is how we say that we want it to stream back the results. Uh, and so it's something that we've, we've already done from time to time, but that is an agent that has been created. And then the thing that brings it all together is something called on messages. That is what we call on an agent to pass in a bunch of messages, which we do right here. We just put our single message in a list. We pass that into all messages. You also have to pass in this thing called cancellation token, which is how it knows when the messages are finished. That's a sort of fiddly thing about about agent chat, but but I wouldn't worry about it. You just call, uh. And of course it's an async. It's a coroutine, so we have to await it. Await agent on messages pass in the list. You pass in this cancellation token, which is how the framework is going to know when this agent is completed. And then I'm going to print the chat message content. So we're passing in this message. Um, the messages I'd like to go to London and the assistant is, uh, your helpful assistant for an airline. You give short, humorous answers. Let's see what happens if we do this. Great choice. Just remember, if it starts raining, it's not a sign to panic. It's just London welcoming you. Ha! A nice humorous answer, of course, from our agent, and it's worth pointing out that we're having a same kind of moment as with, uh, OpenAI agents SDK. It's so easy to do this, to package it up and to make this call. It's really a nice, lightweight abstraction. There's not a lot of heaviness to this, just a lightweight abstraction around calling LMS. Well, let's take that a little bit further. Let's of course we have to bring in tools. It's always about tools. Let's bring in a tool, do something more interesting right now. Now we're going to make a tool that's going to get ticket prices. And we're going to arm our agent with the ability to look up ticket prices. And we might as well use like a SQLite database because people often like to think, say, say, okay, what would it be like if we had, uh, our agents being able to query the database? Now, there's there's sophisticated ways of doing it to actually write a SQL tool that gives agents the ability to write SQL. But in this case, it's perfectly simple. We can just write a tool that can just look up in the database. So let's do that right now. So we're going to import Sqlite3. We're going to create and we're going to delete tickets database if it already exists because we ran this before. And then once it's been deleted, we will then connect to a new database and create a table called cities, which has a city name and a round trip price. And uh, people that are familiar with this will know perfectly well that it's created a DB database that will now be empty. And we are going to populate our database with a bunch of tickets for to London, Paris, Rome, Madrid, Barcelona and Berlin. And that's been done. And we're going to write a simple query function get city price that takes a name, and it will get the round trip price to travel to the city. It will simply connect to the database and it will run a little select statement passing in the city name and return the result. And yes, for security conscious people, there's perhaps more things that I should do to make sure to validate this and make sure the city name is a city name and all the rest of it. But this is just a toy example for now. So we run this. We've got get get city price. Let's just check it out. Let's try get city price for London. It's populated in the database. We get back to 99. Let's do Rome and we get back for 99. That does appear to be working. And now check this out. This is the same as before. This, uh, creating an assistant agent. I'm calling it smart agent now because it's going to be smarter than before. Uh, otherwise that's the same. This is the same. We're passing in the same underlying client, the same underlying lm the system message. Uh, I have actually added in, but I'm not sure if it's necessary to tell it that it has the ability to look for a round trip ticket. Um, we're streaming again. We're passing in this function as one of our tools in here. And there's also this slightly curious attribute reflect on tool use, uh, which is a way of indicating that we don't just want it to return the tools results. We do want it to be able to take that and continue processing even after the tool has returned. True. We we continue until it replies. So that's like it's rare that you wouldn't want that to be true. So you should always assume that that will be your default, I think. Um, and that's I'm not thinking of something obvious. So, so this is the way we do it. And there's something to point out here, which is a tiny difference from the other frameworks, a tiny way in which autogen maybe is a little bit better, a little bit superior, that you'll notice we're just passing in this Python function directly here. We didn't have any kind of decorator or anything. You remember in OpenAI agents SDK, we had to put in a decorator. We've had to in things like Landgraaf, you have to to, uh, wrap it in a, like a tool. We haven't had to do anything like that. It's, um, it's just really lightweight. And it's because, of course, they've just got a little bit of extra stuff in the abstraction code that sees this as a Python function. It uses this comment to figure out the description of the tool. So it just does a bit of that for you. It makes life a tiny bit easier. Removing some of the of the of the learning curve, which I think is really nice. Um, so what we're now going to do is we're just simply going to call smart agent this guy on messages passing in our same message. I want to go to London. Um, and then just just for the for fun, I'm going to print the inner messages. I mentioned that the message construct isn't just for the human to the agent. It's also for what's going on between agents and inside the agent. So we can see that by printing its inner messages. And then we will print the result and let's run that. And this is what happens. So you can see first of all that there's been a function call. This was an inner message with city name set to London. And it's a get city price is the function call. Uh the results came back. The result was 2.99 and there we go. This is linked back to there, as you would hope. And then this is the final response from the model are the city of T and top hats. A round trip ticket to London will set you back 2.99. Just remember, the only thing you should pack is your sense of humor, because the weather might require it. Very droll. There we go. Uh, so, uh. Great. Great answer. But of course, more important than the great answer is the fact that it was so simple to write that tool, to have it make a SQL call to a database. That's right. Sitting right there. Uh, and to have that run use the tool, um, and it just shows, uh, first of all, um, how quick and simple agent chat is, but secondly, how good you're getting at understanding this stuff because honestly, this is so familiar to you now that this is just in a less than ten minutes, you're already an expert at agent chat.
------
Video 93. Week 5 - Day 1 - Essential AI Components: Models, Messages & Agents Explained
------
And so with that we covered models, messages and agents. And we'll do teams another time. And that wraps up the first day of week five. And next time we'll get a bit deeper into agent chat. See you then.
------
Video 94. Week 5 - Day 2 - Advanced Autogen Agent Chat: Multimodal Features & Structured Outputs
------
And welcome to week five, day two, when we get deeper into Autogen agent chat. So as a reminder, I'd like to show these things a couple of times. Autogen which is multiple different things. There's Autogen core, which is the kind of infrastructure for agents interacting. There is Autogen agent chat, which is a thing that's quite like crew and OpenAI agents SDK. And then built on top of that are things we will not be looking at the no code, low code platform studio and their particular tool Magentic one. But we are going to be talking about Autogen and today it's more on Autogen agent chat. And you remember we talked about the core concepts of models and messages and agents and we got to see them all. And we didn't actually talk about teams. Today we're going to take it a little bit deeper. We always like to take it one more step deeper. But most of this is going to be concepts pretty familiar to you. We're going to talk about going multi-modal. Uh, that's that's I guess new. We haven't really done that before. So that's going to be a new one to try out. We're going to talk about structured outputs, which is an old chestnut that you know well. We're going to look at something interesting. We can actually wrap tools in long chain so that we can call long chain tools from Autogen, which is going to be super convenient, especially as we're quite experienced with long chain tools now. And because there's an enormous ecosystem of so many tools to choose from. So this is quite a luxury to have access to this. And then since we put teams up there, we better look at teams. So we're going to give that a quick whirl as well. And then there's a special guest entry as well. Something else just to put out there that I will show you that might entertain you. All right that's enough. Let's go do it. Okay. So we are in. So we're in week five. We're going to the second lab for the second day. Week five, day two. Autogen agent chat. Going deeper. So I've got a bunch of imports, including I've shoved load dot env into the imports as well so we can get on with stuff. All right. So I'm going to start by showing you how you can have like a multi modal conversation Conversation which isn't just about text, but you can send pictures along with the text and have that be part of it. So we've got here that we've got a URL, and that URL is linking to an image from my website. And then we're going to open that image using a very standard Python images library. The Python image library is what Pil stands for. And we're going to to create an autogen image from that and then take a look at it. So here it is. It's a cool picture that's meant to be evocative of sort of going into the world of AI. So we're now going to create a multi-modal message. So you remember in the past we've had text message. Well, this is analogous. It's a multi-modal message and it has content which has a list. It has like a bit of text to describe the content of this image in detail. And then it's got the image and the source is user. That would be me. So just running that is all it takes to create a multi-modal message. And then sending that to GPT four or mini is just as simple as doing exactly what we did before. We create the model client and then we create an assistant agent. I'm calling it the describer or the real name description agent. The model client you pass in, of course, which is the LLM, the system message. You are good at describing images. Fair enough. And then uh, then the user prompt, of course is right there. And then the we pass in that multimodal message and we also just say the cancellation token. And then we will print that markdown version of the reply. And this will take, take a little while because it's got to take in that image and it's got to get back all of the response from it. So I have to let this sentence run on quite a bit to see what we get back. And here it is. Uh, not too bad. Uh, so you'll see that it has worked. Uh, the image depicts a brightly colored, stylized space that combines elements of a workspace with whimsical, otherworldly doorway. And then we've got, of course, because models are great at markdown, that's what they love. That's that's one of the languages they speak natively. You get the room setting, furniture, doorway, decorative elements. The overall, the image cleverly intertwines a workspace focused on technology and coding with imaginative and vibrant elements that evoke the limitless possibilities of AI. So it definitely got the joke. It understood what the image was about, and it did a fine job of describing it. And that is an example of a multi-modal message with Autogen. And then for my next trick, I'm really going to throw things at you rapid fire today because you know, much of this stuff we're going to do structured outputs, something you know well. And it made it very easy in Autogen. It really is. So this is a subclass of the Pydantic base model called image description. So that's what I'm calling it. And it's a class that I'm going to want to populate with the answer from the LLM. And so I give it four For fields seen, message, style and orientation, which is how I want the model to describe the image in a particular structure. Perhaps I'm going to put that on my on my on my user interface. Perhaps I'm going to catalog it in some way, write it into a SQL database. So I then use pedantic field to be able to describe each one. The scene is briefly overall seen in the image message, the point that the image is trying to convey style, the artistic style and orientation. So we're going to going to ask it a more kind of analytical question about like, what is this, a portrait image, a landscape image, or a square image, which requires it to have a slightly more, uh, more of a meta understanding of what it's looking at? Uh, so, uh, we'll see how it can fare with this. So this is the pedantic object that we want it to populate. And here is the code. Uh, as before, we create the model client, the the GPT four mini. And then this is our agent. It's it's again the description agent, same model client, same prompt and this is the only change output content type equals and we pass in the pedantic object. Super simple. It's made it just that we state what we want. And so what we now, what we expect is that when we call on messages, the same, same thing that you use to evoke, uh, to invoke, to use the term, uh, the same thing you used to to call this LM, um, what comes back? The reply we expect to simply be a type of this object. It's going to have replied with this object. And remember, it feels as if the model is able to reply with a Python object. And what's going on behind the scenes. I know you know this. It's all just JSON. This is converted into some sort of a JSON spec, and the model returns JSON and the wrapper code then populates this object from the JSON. Okay, let's run that. We're doing the same the same image again, which means I should have run it while I was prattling away so I wouldn't have to fill the dead air with my nonsense. But it's done. And what we've got back. I just printed it. It is indeed an image description object. It is an instance of this and you can see scene is populated. Let's let's print this out nicely. So I'm using this thing called text wrap that prints something that's formatted, uh, to, to wrap around at the end of a certain number of characters. We'll print the scene, the message, the style and the orientation. Uh, here it all is. Uh, it's nice and crisp, seeing a colorful and imaginative interior of a room showcasing a workspace and a door leading to a vibrant portal message. The image conveys the theme of creativity and the potential of AI, suggesting a gateway to new possibilities. Style A vibrant, illustrative style with bold colors and exaggerated features, giving it a surreal and playful appearance and orientation landscape. It correctly identified the basics that this is in fact a landscape picture. Uh, if we come back to the picture as well, I will point out something quite clever, which is I only realised, actually now talking about this with you, that, uh, nowhere on here. It's not clear why it sees this as something to do with AI. And the answer is, you can see because the writing AI is up here, that's the only clue, I think, unless I'm missing something obvious, that's the only clue that that is what this diagram this is taken from, from a, from a course I did, which is about, you know, the doorway, the what it means to go into the world of AI and, uh, the, uh, um, yeah, it's funny that I'd given this course and I'd had this as my, my image for it, and so I'd had AI in there. And it's funny that it spots that and includes that in the, in the description. Very, very impressive I've got to say. But that wasn't the point of this. The point of this was to show you that we can use structured outputs easily, and we can get back our data. That is according to this schema. And this would allow us to go in and do more processing with this if we needed to.
------
Video 95. Week 5 - Day 2 - Implementing Primary and Evaluator Agents in AutoGen with Langchain
------
What we're going to do now is pretty cool. We're going to use the tools that we worked with last week in long chain. Autogen has a really easy way to wrap long chain tools so that you can call them directly from within Autogen. So when we were working with Landgraff last week, we were using long chains tools, and now in Autogen, we're also going to be able to use long chains tools. And that's great because Long Chain has a huge tools community and lots to choose from, and that's great. And we've already used many of them, so we can just suddenly have access to all of them from within the Autogen ecosystem. So within Autogen, there is this long chain tool adapter, and you can use that to wrap any long chain tool and it becomes an autogen tool. So for example, uh, this, this uh, this is importing some of long chains classes. We've already used these before. The Google Serp API wrapper is a way to call the Google Surfer API that we've we've used that We've got access to the file management toolkit. Was that great set of tools for reading and writing files with from a particular directory. And then this is just the generic Lang chain tool class. So just before we do that, let me tell you the prompt that we're going to have for an LM. Your task is to find one way nonstop flights from New York's John F Kennedy to London's Heathrow Airport in June. Search online, write the details to a file and select the best one. So then this code. These two lines here are taken identically from what we did last week. This is creating a Google search API wrapper. And this is the Lang chain code to create a lang chain tool for searching the internet with a description based on this function. And we could also do the same thing for things like sending a push notification, and for running Python code and everything else that we did last time. And so we end up with, with, with this long chain tool. And now I can call long chain tool adapter. Create a new instance of that passing in the long chain tool. And by passing that in this adapter adapts that long chain tool to become an autogen tool. It's just sort of a wrapper around it. And so I put that in a little list called Autogen tools. And then I've also collected some long chain file tools by getting the file management toolkit giving it a directory sandbox I've created an empty directory sandbox on the left. And we call Get tools. And we get a bunch of tools. And for each of those, I'm going to add them to my autogen tools by appending a Lang chain tool adapter and Autogen piece of Autogen code that adapts Lang Chains tool to be an Autogen tool. That's what this does right here. And then I'm going to print each one out. So we get to see what is this set of tools that we've got. And then. Then I'm simply going to call an autogen. Agent with this prompt. Find me a flight, use your tools and I'm going to give it. Of course. Tools. Here they are. I pass in the autogen tools so that that is what it can use. And then I will print out the inner messages. So we see it doing its thing and then get it to display the outcome. All right let's run this. Let's see what it actually does. So what you'll see is that it uh it's printed out the different tools it has access to. It has access to do an internet search to file move things around. So what's it actually done? So it's done a function call to to do a query, an internet query for finding the non-stop flights. Uh, it then, uh, got the results and it sound. It's replied with, I have found several promising deals. I will now write the details to a to a file called. Now, one of the interesting things about Autogen is the way that it handles the interactions that agents typically, uh, will then stop. Um, particularly in this kind of mode of working with them. And so what we now do is that I'm just going to send the next message of, okay, proceed, send a second message to this agent in the same way. And we'll see what it comes back with. So it's thinking about that which which probably means it's streaming back something big. And here we go. The details of the flights have been saved to flights. Now I will select the best flight. The selected flight is all of this. This flight offers a good balance of price and service, making it a great option for travel. Sounds good. It's with Virgin Atlantic, a great airline. And there's details. And this of course, has actually been, uh, grabbed from the true internet search. And it is like a useful piece of information. And of course, I know you're wondering, did it write a file? Let's go and have a look. We open up sandbox and there's a file flight MD. Let's let's open a preview of that file. And here we go. Here are a bunch of different options nicely formatted with a bunch of different, uh, things. Uh, does seem interestingly as if there was a cheaper option, but it has selected, uh, Virgin, which is one of my favorite airlines, but is the most expensive of the options. But I guess we didn't necessarily say the cheapest, uh, did we? I'll go back and check out the the prompt, um, the best flight. So, uh, um, a good choice of food and drinks and on board. Let's see, uh, promising deals. Yes. So we don't necessarily say that it needs to be the cheapest. So it chose, for whatever reason, to recommend the Virgin flight out of those flights. And that is what it did. But by all means you can rerun it asking for the cheapest and hopefully Gpt4 mini can handle that task. Okay, so we've shown how you can wrap Lang chain tools and access them from Autogen. And a little mini side mission for you right now is to go and find some more tools. Bring in the Python one, and have it be able to multiply pi by three and try some other tools. There are so many tools in the Lang chain ecosystem that now you can be unleashed to make Autogen do all sorts of cool things for you. Uh, and, um, okay, next up we have, uh, team interactions. So check this out. I'm going to take you through this pretty quickly because it's fairly self-explanatory. Uh, so you can create multiple assistants. So in this case, we're going to have, um, an assistant agent, uh, called primary and one called evaluator. Any guesses what pattern I might be about to work with here? So the prompt is your task is to find a one way non-stop flight from JFK to London London-heathrow uh, first. Search online for promising deals, then reply with the best option you found. Each time you're called, you should reply with a different option. So then we. Create a primary agent, which is the, uh. It's only going to have access to the. Sherpa to search. We're no longer writing files at this point. Uh, so it's the one that's going to be doing internet searches and it's system messages. You're a helpful AI research assistant who looks for promising deals. Respond only with one option. And, uh, make sure it's different from others. And then we have an evaluator check whether it looks like the assistant has given a very promising recommendation. Respond. Approve when you're satisfied. If you've only seen one reply, then don't approve. You need to see more. You can tell slightly artificially making this so that we get some sort of interactions between them. If this were a real, uh, proper challenge you were trying to set, you might be perfectly happy with the first option if the evaluator agent is satisfied with it. Um, okay. So this this is the, uh, the way it works. Um, and then we create a team, and there's various ways to create teams. This is a bit like a crew and crew, but it's, uh, it's really, uh, somewhat simpler. Really. This is a round robin group chat, which means, like, one after the other, obviously. Um, and, uh, that is the simplest way that you could have some kind of relationship between them. And we pass in a list of agents to talk to each other, primary agent, and then an evaluation agent and a termination condition that tells it. When do you know that enough is enough? And this is, uh, you can see here how it's defined. Text mention termination. The word approve. So this is a little bit brittle. I'm relying on the fact that the assistant, the evaluator agent, will reply the word approve. Normally you would want something a little bit more profound than that. You would probably want to reply, have structured outputs here and use that to test. But this is perfectly good. For now. I just want to get you a feel for this. I don't necessarily want to give you a full bells and whistles solution. And then this. This is how it comes together. You call team run and you await team run. And it might be worth me mentioning. Actually I've been focusing a lot on the on messages. The on messages, uh, thing that you call agents here. You can also call Agent Run as well. An agent can be called with run. And then you just pass in this exactly the same thing. You pass in the task, um, task equals and a prompt. And what comes back is just going to be the final messages. So that is another way of doing it as well. Um, but this is what we will do now. Okay. I've actually just gone and tightened these prompts a bit and made them just shorter and simpler because it. It was going. It was thrashing around a bit. The prompt is just find a one way non-stop flight. The system message says you are a helpful AI research assistant who looks for promising deals on flights. Incorporate any feedback, the evaluator says. Provide constructive feedback. Respond with approve. And so with that, I'm now kicking off our message here. And it does take it's quite chatty I discovered and it goes backwards and forwards. And I'm sure there's ways you can tighten up these prompts to make the conversation be a little bit crisper. But I will let this thing run and let it go for a little bit. And when we are back in a sec, we'll see the response from the agents. Well, I'm going to be honest with you, this is my second shot at it that I'm showing you here. I kicked this off even with these shorter prompts, and it was running for more than a minute, like going backwards and forwards. And GPT four and mini is quick, so. So I imagine it was in quite a conversation there. Uh, and so I interrupted it. I restarted my my lab and then I ran it a second time and it took 10s. And it's done. So you may want to be if you if you run it like this, then then obviously interrupt the, the, uh, the notebook by pressing the, the, uh, the button here when it's running to stop it or restart, restart the kernel. Um, if it's thrashing because you don't want to spend all of your, your, your tokens on this on this message. But I mean, I think this is a very eye opening sign of what can be troubling with these sorts of agent platforms that they can be quite hard to get them to perform the way you want. Um, you may have discovered when you're working with Landgraaf that there is a maximum, a recursion limit, as they call it, of 25, meaning that if you have more than 25 conversations between agents, then it will throw an exception unless you explicitly say you want more than that. And it appears that that there isn't the same kind of protection in an autogen. So it just kept and kept and kept on going. Uh, so, uh, be sure to to restart the kernel if that happens to you, or try and tighten up this prompt so that it doesn't happen. And observe that this is the kind of instability that is problematic, uh, with these kinds of agent frameworks, when there is autonomy to decide to keep going if they wish. But anyway, this time it took 10s. It was quick. And let's see what actually happened. The user said find a non-stop flight or a one way flight. A non-stop uh. The primary agent used the tools to do a query, and then it said it gave a response. Uh, with, with the the $402 flight there. And the evaluator responded, your response contains a lot of good information, but it lacks a specific answer to the question. And then it gives it's quite authoritative. Little does it know it's talking to itself, but it's quite, quite, quite stern. Focus on the user's request. Clarify dates, organize information, remove redundant. So it's really great to see the evaluator doing what it's told to do and evaluating. And then the primary says, thank you for the feedback. Here's a revised response. And it does indeed give a revised response with all that detail. And the evaluator sends approve which is the text that it has to respond. This is a little bit hacky, but it responds to the word approve because reply approve and your feedback is addressed and because it says approve, that means that the termination condition is met. And this completes. And that is how we arrive at our decent answer here. And this is an example of a team at work. And there's a lot more that you can do with teams, but I feel like it's pretty consistent with stuff we've done before. So this gives you enough of a flavor for it. Now, should you wish to build out your teams, you can look at some of the docs and build out something a bit more extensive than this. And. But just be aware. Be be aware of the fact that they can just just keep going. And you need to be you need to be ready to intercept if necessary.
------
Video 96. Week 5 - Day 2 - Headless Web Scraping Tutorial: MCP Server Fetch Integration in AutoGen
------
And of course, I should have been clear on what I just said as I was, uh, waffling away about the problem with the agents going going, uh, off track that you can use more intelligent termination conditions to make sure that this doesn't go awry and that you keep your agent interactions on rails. Don't just look for a hard coded piece of text in an agent response if you don't want to get yourself in trouble, right. Uh, there's an exercise for you. Okay. But now, in the meantime, big drum roll MCP is a topic for next week. But it is, of course, all the rage. And so I thought I'd give you a teaser, a little a little tiny sense of what it's like to work with MCP without going into any detail, because I don't want to spoil the show for next time, but let's just do a quick, quick bit of looking at MCP within Autogen. Okay, so what is MCP? Well, again, I don't want to spoil it for next time, but but MCP amongst other things is just a nice, simple, elegant way that that we can package up tools so that different models can use those tools. It's like a it's it's not an abstraction layer. It's not particularly code. It's just an agreement for a way. If you write tools in this way, then models LMS can discover them and call them in many ways. If you think of it like with something like Langshan, there is an ecosystem and Lang Chain built some code and they built some wrappers that lang chain tools and they say, if you can wrap your stuff in a Lang chain tool, then anyone else writing lang chain or lang graph will be able to use the tools that have been packaged this way. And what anthropic have done with MCP is they've said, well, actually, let's just go simpler than that. We don't necessarily need to have this glue code. We can just say, look, anyone, if you write a function a particular way, that's good enough that as long as it conforms to a particular kind of spec, then that that can be a tool that we can make available to any model we want. So it's very similar to the long chain idea. It's just more open. And rather than saying you need to be part of the lang chain lang graph ecosystem, it's just saying, look, anyone can be part of the MCP ecosystem, anyone can use this. And anthropic describes MCP as being like the USB-C connector for AI for Llms. It's just an agreed protocol, an agreed way that we can plug models into tools and also into into what they call resources, which is stuff like context for RAC. Um, so if this was just to give you some intuition for it, I didn't mean to explain it because we will explain it next time next week. But, uh, one of the things that's great about Autogen is that not only do they give you a nice little, little wrapper so that you can use any Lang chain tool, but they also give you a nice little wrapper so you can use any MCP tool. So if there's an MCP tool out there you can just use it in Autogen really, really easily and that is what we're going to do right now. And so let me take you through this. This is the whole extent of the code. Uh, we are using the assistant agent. We're using the OpenAI chat completion client. And from Autogen tools, we're bringing in this class here. There are two different types of ways of working with MCP. And this is one of them. Uh, and we will talk about those next week, of course. And in particular, uh, we are going to be using a tool called MCP fetch. And what this, this this is going to be running locally on our computers. So we're going to kick off this tool to run locally on our computers. And because it's been written a certain way, that tool is just a tool we can use from Autogen. So the key to MCP is that different people can write tools and as long as they write them a certain way, you can just use them out of the box. It's a bit like saying people could like, write a long chain tool and you can just use it out of the box. But MCP is just more open, available to any ecosystem, not just the long chain ecosystem. So MCP server fetch is an example of a tool that is open source and available, and that you can just download and run it yourself. And that is what this thing here does. It runs it locally, and it is a tool which actually runs the playwright browser in a headless mode and allows it to go and fetch web pages. So it's doing something a bit similar to what we worked on with sidekick last week, but it's just doing it not not in a way that brings up the browser, but just does it quietly behind the scenes, headless as they call it. And it will run that and then it will, it will, uh, get those tools and it will put those tools into this thing called fetcher. And we can just provide fetcher in as our tools. That's it's, just as simple as that. And so, uh, yeah, basically what we're doing here is that we're using a public online tool available that runs Playwright Browser locally and uses that to scrape the web. And we're making that tool available to our assistant. And what we're going to ask our assistant to do is review the website epodunk.com. But you can change that to any website that you want and summarize what you learn and reply in markdown. So without further ado, let's kick this off. And again, the takeaway for you is just to show. And there we go. There I am. Look at that. A technology enthusiast with a focus on coding and experimenting with llms. Sounds about right. Uh uh. And oh look, it's got a link. I wonder if that's an actual link to my LinkedIn profile. It almost certainly is. Uh, these these, uh, things are amazing at it's replied in markdown, and I'm sure it's got the right, the right links. Uh, so, um, the, the this is cool. And the reason it's cool is that we've just used a tool that someone else has written for, for running playwrights in a headless way. And we have just incorporated that tool because that tool uses this, this open standard, this standard called MCP. We're able to just drop that tool in and use it from within Autogen. So just like we could use a Lang chain tool from within Autogen, we can use an MCP tool. Anyone that's written a tool that conforms to the MCP standard. And the cool thing about MCP is that it's such an open standard that anyone can write tools, and there are websites where you can get access to lots and lots of these tools. So it's like saying, we've got access to the Lang chain ecosystem only it's a whole lot more. It's this massive open source, public community ecosystem of tools. Anyone that writes tools that conforms to the MCP standard, you can then access just like this, and you can do it from within autogen in this way and immediately have access to any of them. So it's only meant to be a teaser. I wanted to show you how you can use MCP tools. Don't worry if you don't really understand. Just get an idea and we'll we'll go through MCP and then you can come back once you fully understand it and use lots and lots of MCP tools right here from Autogen. And so there we have it. We went multimodal. We looked at structured outputs. We used tools from Lang Chain. We saw briefly teams the round robin teams. And you can experiment with more should you wish. And then we introduced a special guest in the form of MCP tools. Your teaser your preview that we will come back to shortly. And that wraps up day two of Autogen. And it is the, the probably the our last deeper foray into Autogen agent chat. Next time we switch to Autogen core, the underlying infrastructure part. I'll see you then.
------
Video 97. Week 5 - Day 3 - AutoGen Core: The Backbone of Distributed Agent Communications
------
And welcome to our continuing exploits with Autogen. As we head into day three of week five and talk about Autogen core. So just to remind you, this is where Autogen core sits in the scheme of things. It is the kind of base. It is the basis of Autogen agent chat. And it's the fundamental infrastructure behind it all, which is exciting. One other thing I wanted to point out while we're on this screen is that you might have heard of something called Microsoft Semantic Kernel, and you might be wondering, where does Semantic Kernel fit into all this picture? Isn't that an agentic framework as well? And what's that got to do with all of these? Well, the answer is semantic kernel is a bit different. It is more akin to something like long chain. Semantic kernel is a reasonably heavyweight glue code that does things like wrapping calls to llms. It handles stuff like memory, it handles stuff like tool calling with its own sort of framework around that and its own name. It calls them plug ins and, uh, and, you know, everything else, structured outputs and so on. So it's very much akin analogous to long chain. It even has its own like prompt templating framework as well for building templates and populating templates and prompts. Um, so it's different. There is of course overlap. You can use semantic kernel to be managing things like tools and agent calls, and there is some agent functionality there. So there is some overlap that Microsoft's very aware of. But they would see it as a pretty different kind of offering. Autogen is is a high level more, very much more agent focused. All of this is is very much focused only exclusively on the world of building autonomous agent applications, where semantic kernel is more about stitching together calls to llms, generally for business purposes. Well, I hope that gives you some some context there and explains why we're not looking at semantic kernel for this course, because that's more something you might do alongside looking at long chain or something like that. Okay. Onwards. So what is Autogen core exactly? Well, well, here's a here's a story. So it is an agent interaction framework that that is how they describe it. And that's what it is. It is agnostic. It doesn't care about the platform, the product you're using to actually code your agents. The actual abstraction you have around agents, you could be just calling LMS directly, or you could be using an abstraction around them. Some some framework, some glue. Either way, doesn't matter to Autogen core. It is just about agents interacting. If you wish to use, uh, the agent chat framework as your agent abstraction, that works well, and that's what we'll do. And they're clearly somewhat designed for each other. And we know that that, uh, that agent chat is built on top of a autogen core. So, so that makes sense, but it doesn't require it. Um, a bit like Landgraf doesn't require Lange chain, but. But it helps if you use Lange chain because there's lots of similar kinds of of thinking there. So in some ways it is actually similar positioning to Landgraf. Landgraf, you could argue, is also about interactions between agents, although very much thinking of things in terms of the graph of operations and dependencies, the node graph that now we know so well, uh, but similarly, you could argue Autogen core is about interactions between operations. That's, that's what it, it, it thinks about. And similar to Landgraf, Autogen doesn't care. Autogen core doesn't care about how you've implemented your agents. So there's definitely some parallels there. Um, but the big difference is that Landgraf is all about this idea of robustness and repeatability. This this idea of being able to replay back in time everything that's been built around Landgraf is for that purpose, you can tell, Whereas it seems to me that the kind of the driving force behind autoencoder is more about building a kind of environment where agents can do things like, like interact with each other, where agents that could be in very distributed places, they could be all over the place and they could be very diverse. They could be written in different languages, they could use different abstractions, but they one could be written in JavaScript and one in Python. It could be from all sorts of different shapes and sizes, and they're all able to play nicely in the auto gen core world, because auto gen core takes care of the interactions between them, it takes care of creating them, and and it takes care of messages between them. So that's that's the kind of thesis behind auto gen Core. And arguably Landgraf also thinks about that kind of thing. I think it's just a different emphasis. This is the real emphasis of auto gen core. It's about the interactions and supporting diverse interactions.
------
Video 98. Week 5 - Day 3 - Agent Communication in Autogen Core: Message Handlers & Dispatching
------
So here's the core idea behind Autogen core. And I've already alluded to this idea, and it's that it's all about decoupling the logic of an agent, what it actually does from how messages get delivered to it, from the interplay between them. The framework deals with creating and communicating. It deals with creating agents, the whole life cycle of an agent, and the messages, the communication between them and the agents themselves or us as the people coding the agents. We're responsible for the logic. It's not the mandate of Autogen core. It just deals with with letting them play. And there's two ways that it lets them play. Two types of runtime. The runtime is the kind of world in which agents interact and the two types. One of them is called standalone, which basically essentially means it sort of runs on your box in a simple way. And the other distributed is that it runs in a way that could allow remote agents to interact with each other. So these are the two kinds, and you code them both a bit differently. We are going to look at the standalone one today, and we're going to go to the distributed one tomorrow. And look, I have to tell you, I'm only going to do this at a high level. We're going to breeze through this quite quickly and the examples will be somewhat superficial. And there's a reason for this. I feel like we've done enough detail on lots of frameworks and actually building agents. I'm not sure how applicable this is going to be to your use case. I think it's good for you to get a feel for it, and to get a good sense of how this is positioned and where it might be useful. And that's my goal. Give you a flavor, give you, give you that kind of sense of where it fits in the ecosystem and if it's relevant for you, if this is something that you do want to put into practice, then you should carry out more R&D and work on this a bit more yourself. And I'll give you plenty of starting points for that. But I'm not going to go so deep into this as some of the other frameworks as like Landgraf last time, but enough so you get a feel for it. And then and then we, uh, we, we move on because I know you're anxious to get to MCP. All right. Let's let's do a lab. So here we are in good old cursor going to the fifth folder for Autogen, and we are going to lab three Autogen core. Uh, so this this is where we begin. Uh, there's, there's, as I say, something a little different. This is going to be different to anything we've looked at before. Um, okay. So there's a bunch of, uh, imports, including a good old, uh, load dot env. Let me just restart this, make sure it's fresh. Do, uh, do the load dot env. All right. So the first thing we do is we define our own object which is going to be used for passing information around the place, our own message object. And it is a type of, uh, I don't know if it's if this is required, but a data class means it's a class that's not going to have any feel any, any methods. All it is, is something that holds data in, uh, and that makes sense because the message is not going to be something with functionality. It's going to be used to transport transport information between our agents. And in a way, like doing this is sort of analogous to when we were in Landgraaf. We started by defining a state. Landgraaf is a state machine. It's very focused on what is the state and making sure that that's something that you can move backwards and forwards in. And it's interesting that, you know, auto Jens auto gen core fundamental idea is all about messaging. And so the thing that you start by defining is your message. And in our case, we're going to have a very simple one that just has one field which is content, which is a string. But you can experiment with what you pass between your agents being something much more sophisticated with all sorts of different bits of information. So there we have it. We have a message with a string, and that's all the message we'll be using for today. So now we get to define our agent in Autogen core. And it's important to bear in mind that the agent that we're about to create in Autogen core is different to the agent that we just created in Agent Chat. It's different. It's like a tongue twister to say all of this. It's hard to keep your mind on it. You're like, what? How is it different? Well, look, the agent that you define in Autoencoder is just like a wrapper. It's like saying this is a thing that can be messaged, it can be created, it can be managed, it can be referred to and it can be messaged. But what you do with this is your responsibility and you're going to have to delegate to something. But this, this, this is like a management object, if you will, a management object which you're going to use. And it has amongst other things, it has an agent ID and every agent has a unique ID, and that ID has two, two parts to it, a type and a key. So every single agent has a type and a key. And that combination of type and key is then unique and can uniquely identify it. So if you imagine when we in the future look at a distributed runtime in that runtime, there could be agents from from all over the world collaborating in this distributed runtime, but every one of them will have a unique type plus ID that that identifies it precisely, and any agent will be able to talk to another one if it knows it's its name and its ID, or its key and its ID. So this gives you a sort of good sense of the fabric of it, and that an agent class that you make is not actually an LLM. It's not an Autogen agent chat agent. It's just this management holder object, something which has a type and a key. So here is our first Autogen core agent. It's called Simple Agent. As you will see it is a subclass of something called rooted agent, which is the typical the thing that you that you have as your as your parent as your superclass. And it only has two, two methods. One of them is the init that the constructor. And all it does is it calls its its parents passing in its name. Simple. That's all it does. Then it has another, uh, method called on my message, which is an async. Uh, so it's a coroutine. It's an async method. It is an async coroutine. It is a coroutine. Uh, and it's decorated with message handler. And that's what matters. Any method that you decorate with message handler means that potentially this is something that will receive messages and Autogen core handles. The fact that you'll be able to register this, and a runtime is going to manage this. And if someone sends a message to something with my name and my ID, then it's going to end up here. At least it's going to end up here. If what they dispatch is a message of this class. So this is a bit of a pro thing, but you can have multiple different classes and you can use that to be able to handle different types of message. Uh this may be more detail than you need, but you you could, for example, have a text message and an image message and have two separate handlers. And just by virtue of the different signatures, the different method signatures, Autogen core will automatically dispatch the right message to the right method, the right coroutine. That's that's what it does. It's all about dispatching messages properly. Uh, and so this, this simple, uh, agent is going to return a, it's going to receive a message and it's going to return a message is now that I said it is a bit like this idea that in Landgraf we, uh, took in our nodes, took a state and returned a state. But this is just like a parallel thing. But it's all about messages. It receives a message, and it. And it returns a message. Um, and it's an async. It's a coroutine, as I said. So it receives a message. And this one isn't going to do anything with the message it receives. There's no lm, there's nothing here. It simply returns, uh, a message which has this is and gives my ID and my key. So it's sort of identifying itself as saying this is blah, blah, blah. You said and I'm going to I'm going to replay back what what the person said. So I do actually I do, I do use this. You said blah blah blah and I disagree. Uh, so that is simple, agent. Maybe I should call it a disagreeable agent. Uh, that's that's what it does. There's no lm there's no AI involved in this at all. It's, uh, you know, it's just a piece of code, but that that doesn't matter to auto gen core. Auto gen core just cares about the handling of messages.
------
Video 99. Week 5 - Day 3 - AutoGenCore Agent Registration and Message Handling: Practical Examples
------
Okay, so now we get to the meat of Autogen core. We are going to create a runtime, a single threaded agent runtime it's called, which is a standalone runtime running on my computer, which as it says, will handle agents in a single threaded way. And the first thing that we're going to do is register. We call register on the agent itself to say, I want you to register yourself with this runtime. Now, this isn't creating an agent. This is just saying you are a type of agent. And I want you to tell the runtime that you are a type of agent that can be spawned, you can be created, and you are a type of agent of type simple agent that's going to be your type. And this, this thing here, this is a function which can generate new versions of you. It can instantiate you. It is a factory. And you pass that in as well. And so I will do this. It's now created a runtime and it's created a type of agent called a simple agent. We haven't actually yet built any of these. We haven't instantiated one. But the as a type of agent is now a known thing to our runtime. Our runtime knows that there are such things as simple agents. And we're now going to start our runtime. It is now running. It is a it is a proper runtime. And now we are going to uh, we're going to first of all create an agent ID object to identify an agent. And we'll we'll do this properly. We'll say agent ID equals that, that is an ID that would identify this. And we are going to say that we want the default agent. Uh, because, because that, that will then, uh, we'll make sure that we have an agent created, and then we're going to send a message to, we're going to send a message to the, to the agent, which is called simple agent, The default ID and we're going to send a message. Well hi there to it. And then we will print whatever comes back. So any ideas what's going to come back I wonder. Uh, so uh it replied this is simple agent default. So it's simple agent is its type, which is the same one we registered and it default is the, uh, the, the id uh, and it said, you said, well hi there. And I disagree. Charming. Uh, and that is uh, that's it. So there is a demo of auto gen core and that's what auto gen core does. And putting in agent functionality like having code in there that calls LMS. That's your job or my job. That's not auto gen core's job. What it's there to do is to handle the passing of messages around by looking things up based on their type and their ID, and that's what it does. So now I'll show you one which has an LM behind it, but it shouldn't be any surprise to you because it's just the same infrastructure. The fact that we're calling an LLM is is almost of no consequence to, to, uh, the Autogen core framework. So what we're going to do is we're going to make a slightly more interesting one called my LM agent, still a subclass of rooted agent. And we're going to use a proper LM. And we could just call OpenAI directly using OpenAI dot create. We could just call the Python client library. But we can also use Autogen agent chat and we might as well since it's autogen week. So this is the same kind of thing, a subclass of rooted agent that's going to be two functions again init and handle my message. So the init just just calls the superclass. But it then creates a model client. But this could be doing anything. But this is the Autogen agent chat way. We're going to create one of these things, a GPT four mini model client. And we're going to set a field called underscore delegate, which we're just sort of holding on to. This is this is the the underlying. This is what our agent object is going to delegate to when it actually needs some code, some, some an LLM to run. And we could have anything we want. We could, we could do we could reply back with random things or we could create an instance of assistant agent. And that's what we're going to do. And that's where we put in. If you remember, when you create an assistant agent, there's the name of the agent and there's the model client, and there it is. And as a result in underscore delegate underscore often used to show sort of like private secret, uh, variables that other people should know about, like, like private in the, in the Java world. Um, so the this is being set to an assistant agent. So, uh, now we just have to write our message handler. And as I say, it can be called anything you like. I'm calling it handle my message type. Um, and that's really to sort of draw your attention to the fact that what matters is what type of object comes in the message field, because Autogen core will automatically route messages to agents that should receive them based on the the finding a handler that that looks for a message of that type. That's how it works. That's the clever routing that it does. And by the way, you can send a message direct to an agent, which is what we're doing right now. But it also has a whole kind of pub sub thing there where you can subscribe to topics and you can publish out messages to lots of agents that are all interested in one particular topic. So the whole process of dispatching messages to the right agent and then calling the right handler, that's the clever stuff. That's what it does well. And why why we use a framework like this rather than coding it ourselves. But for some of these examples, it's pretty trivial. So we could equally code it ourselves. But you can imagine that this is done in a way that can be scaled and distributed. So anyways, handle my message type. Uh, as long as, uh, message is sent to this agent with the ID and and type, and that the object that's sent is of this type, it will arrive at this function. And I'm going to say that I received a message and print the content. Then I'm going to now now this is confusing. This object here text message looks subtly different to this object here. Message. Uh, do you know what's going on here? This text message is text message from the, um, autogen. It's this thing here. It's the agent chat messages. The text message. The same thing we were using yesterday and the day before when we were interacting with agent chat agents. So this is a autogen agent chat, uh, message, which can be a bit confusing. And you might. Yeah, there are ways you could you could get around that by, by giving it a different you can say like as agent chat text message if you wanted to to like make it so the code is a little bit clearer and you're distinguishing between the Autogen core and the Autogen agent chat. All right. Anyways, so sorry. Back we go. So we create the sort of text message that you need for Autogen agent chat passing in the content of our message and the source is user. And this is the onmessage that we call in agent Chat land with this text message and the cancellation token if you remember that thing. And then we get back the content. We say that that was the reply. And we return a new instance of message of our message, with the content being that reply. I hope you followed all that, or at least got enough of an idea of it. Let's give this a shot. Let's create a new runtime, a single threaded agent runtime, standalone runtime. We're going to register two agents the simple agent and the LM agent. The new one, Simple Agent, isn't really an LM at all. It just disagrees with whatever it's told. And LM agent is actually going to dispatch through this underscore delegate down to a true gpt4 mini. So we're going to start the runtime. Then we are going to send a message. Hi there to the real agent. And with whatever it replies we're going to send that. We're going to forward that on to the other agent, to the simple agent and see what comes out there. So the LM agent received hi there. And it said hello, how can I assist you today? A classic GPT four minion response. And, uh, the simple agent said, this is simple agent default. You said, hello, how can I assist you today? And I disagree, and there you go. Um, and with that, we will we will stop this pantomime.
------
Video 100. Week 5 - Day 3 - AutoGenCore Standalone Agents: Rock Paper Scissors with GPT-4o & Llama
------
Did I just say that I'd stop this pantomime? Well, I take that back. We'll do the pantomime one more time. I've just added in another response from the nice LM agent. So I'm going to have the LM agent, uh, respond to. Hi there. I'm going to have the disagreeable agent disagree, and then I'll put it back to, uh, to GPT for a minute to see how it handles the disagreement. Let's see how this little conversation goes. Hi, there. Uh, hello. How can I assist you today? Uh, you said hello. How can I assist you today, and I disagree. Then GPT for him, and he says, I appreciate your feedback. How would you prefer I greet you or assist you? So if you want to be cruel, you can keep this going and just have it continually disagreeing and see how, uh, how GPT four and mini handles that. And once you've had your fun, then you should do this, this, this stops the runtime and then closes the runtime, which the auto gen core people insist that we do before we start another. So we will we will do that and be and be good to it. Okay. But for one more example, and I'm afraid this time we're not going to have sort of true commercial examples because, uh, I think this is just one to show the platform at work, and you can take this away and figure out what you'd like to do with it. But for this example, uh, we're going to play rock, paper, scissors between a few agents to show, because I just want to illustrate the fact that this collaboration, this this interaction is what it's all about. So very simply, we have a player, one agent that is a subclass of rooted agent. And it, uh, it has a, um, um, open AI chat completion client as its underscored delegate. It has an assistant agent that uses that as the model client, and then it has, uh, handle my my message type that takes messages and and returns the response. So this is a super vanilla kind of LM routing that will just simply do respond to the message that is given in the message object. Okay. We got a player two which is exactly the same thing, but with one tiny difference. Can you spot it? Can you spot the difference between those two? Well, the difference is that this has a llama track completion client instead of OpenAI track submission client. So we're going to use a llama. We're going to use a local model. And we are going to see which model that we pick. For this one we're going to pick llama 3.2, the 3 billion variant of llama 3.2. So these are are a couple of agents. And now we're going to have a third one a third agent. And this is going to be uh rock paper scissors agent. So this one is going to have a little bit more of a it's going to have an instruction. You are playing rock paper scissors. First of all. So first of all it comes up with this instruction. You're playing rock paper scissors. Respond only with the one word one of the following. Rock, paper or scissors. And it puts that together into a message. It looks up the default ID, the default, uh, key for player one. That type, and it looks up the default for player two, that type. And it's looking them up. And it then as part of handling its message, it dispatches off a message to these other agents. So it's just choosing to to it's not choosing. It's like coded here. But but it is sending those messages off. And with what comes back uh, with the result, it then puts together a little piece of text called judgment, which says you are playing a game of rock, paper, scissors. The scissors. The players have made these choices. Sorry. You're judging again. Rock paper, scissors. Players have made these choices. Um. And then it ends with, uh. Who wins? Question mark. After slipping in the player one's choice and player two's choice. And then that gets dispatched off to my model, which is GPT four mini. And we return the response. So just to summarize, we have a total of three agents that we've defined here. Three of these like agent managers these agent wrappers, they're not they're not real agents. They delegate to real agents. One is called player one and it delegates to OpenAI to GPT four. Mini one is called player two and it delegates to llama 3.2. And then they don't have any prompts. They just do whatever instructions they're told. And then there's one called judge. And judge very clearly sends an instruction to each of player one and two saying pick rock or paper or scissors. And then with what comes back, it then judges them and returns the outcome. So there we have a simple setup for a game of rock, paper, scissors. And you can of course make this something more interesting. You can have it be be something that's that's entertaining or you can have it be something that's serious. It's commercial. Like when we're thinking next time in week six, we're going to be building like a financial trading setup. You could imagine this could be something where different agents can interact and can argue about something like whether or not a particular equity is a good investment or a poor investment. You could imagine that's the kind of interactions that could be going on. So this, this, whilst we're using this here for a superficial exercise, there's plenty of ways you can imagine anytime when you'd want a sort of autonomy and the ability for different agents to be interacting. You could build that kind of commercial logic into this kind of framework. And that is what Autogen core is for. Not for playing rock, paper, scissors. But anyways, rock paper scissors is the the cards you've been dealt and so you might as well play them. Let's give it a shot. And quite simply, we create a threaded agent a single threaded agent runtime, which is the the simple kind of local runtime. We register player one, we register player two and give it the ability to instantiate players one and two. And then we register the judge, the rock, paper, scissors, and and have everything ready to go. And we start our runtime. We've registered everything and this is where we do it. We are going to find our rock paper, scissors default agent. We are going to say go and set it going and let's see what happens. Off it runs. Agents are talking, stuff is happening and there we go. Player one said rock. Player two said scissors. Player one wins because rock beats scissors. Done. And there you have it. There you have agents being organized and interacting and agents being discovered. The rock, paper, scissors agent discovered these two agents by name by identifying them through the the the sending a message, uh, through the, uh, through the runtime. Um, and, uh, we saw agents interacting in a game of rock, paper, scissors and being judged managed by Autogen core. And so again, there are these two types standalone and distributed. Today we did standalone. Tomorrow we'll do distributed. And I just want to stress again that this is to give you a flavor. It's not necessarily as essential for for the building of agents that we're doing mostly on this course, but it's good for you to get this insight and to see it and compare it with the other offerings out there. So with that, that wraps up day three. And as I say, tomorrow we get to distributed. I'll see you then.
------
Video 101. Week 5 - Day 4 - Autogen Core Distributed Runtime: Architecture & Components Explained
------
And welcome to week five, day four. We're going to talk more about Autogen core. You remember Autogen core, the fundamental the bottom layer of the stack for Autogen. And it's an interaction framework. It's responsible for worrying about how agents play together. It doesn't care about how the agents are implemented, although it works well with with agent chat. It's somewhat analogous to landgraaf, but except rather than focusing on repeatable workflows, it's focused on the interactions between diverse agents. And in particular, we talked about two different types of runtime the standalone. The distributed. Last time we talked about standalone today we're looking at distributed runtimes. It's going to be even higher level than last time. And it's just to give you that flavor. And I should also point out that Microsoft says that that the distributed runtime is still experimental and the APIs are liable to change at any point. So it should be taken in that light. It's not actually ready for a production system yet, but it's more of an architecture and idea and an exciting, uh, idea in terms of future possibilities and the distributed runtime itself that we're going to look at right now, uh, is really it's described as something which handles processes, handles the messaging across process boundaries. That's the idea. It's no longer a single threaded thing running on our machine. It's something that can run across different processes that might not be Python processes. They could be anything. And it consists of two different things, two different components to it. One of them is called the host service, and that is the sort of container that runs this. And it connects to a worker runtime or potentially many worker runtimes. And it handles the delivery and it handles sessions for direct messages, direct messages that are going to be handled by gRPC if you're familiar with that technology. Remote procedure calls and it's going to handle the session management around that. All of the nuts and bolts of the infrastructure around the complicated business of sending a message remotely from one computer to another, from one process to another, that'll be taken care of by the framework. And then the other concept here is a worker runtime, that is the runtime that we'll be able to treat much as we treated the runtime in the single threaded case. And it will be able to manage different agents. It will have different agents that are that are registered with it. It will advertise the agents it's got to its host servers. So the host service knows what it's got and it will actually handle, of course executing the code the worker runtime has the agents, which themselves are delegates for something that that does something, and that will be handled by the worker runtime so that that's how it fits together. But it's going to be more concrete when I show it to you, which I'll do right now.
------
Video 102. Week 5 - Day 4 - Implementing Distributed AI Agents with AutoGen Core and gRPC Runtime
------
And so here we are back in cursor and in the number five folder for Autogen. And going to lab for Autogen Core Distributed. I'm only giving a teaser of what this is about because, uh, I'm to be honest, I'm I'm not even sure how relevant this is to, to many, many of you. So I, you know, I'm going to give a flavor for it. If you do want me to go into more detail, if you want more content, then please do. Do, uh, send me a note, let me know. I can I can always go deeper into this, but I don't want to hold, uh, others up. Um, and, um, I think that particularly as Microsoft is saying, that it's, it's still experimental. It's better to get a flavor for it. Okay. So I've done this and there's actually two different ways that I want to show you this working. Uh, one of them is what I'm calling all in one worker. And one of them is with multiple workers. And so I've got this flag here that you can set from true to false. And we're going to go through it once with it true, and then once with it at false. And you'll see what I mean. So we do some imports and we load the dot env. And we start with our message, uh, class as as before, which I'm going to clear all outputs and restart. Do this again. Sorry. Uh. Run this. I start as before, with our message class defining this thing. It's the sort of analogy to the state with with Landgraf, although it's to to describe how we interact between our agents. Okay. And now I mentioned that the distributed runtime consists of these two things. One of them is the host. And this is how you create the host Autogen runtimes, gRPC, the gRPC worker agent runtime host. So this is something which uses gRPC, the remote procedure called technique to be able to to send messages. And this host will run on my local host on port 551. And I will start it with this. So this is now going to be a running host. And gRPC of course, is a cross-language, uh, approach for sending, uh, A function calls between different languages, and it's super powerful. It's used in many different, different places. You can think of it. It's like it's like making rest HTTP calls, except you're able to call directly from one function to another, and gRPC is used all over the place where, uh, where interactive messaging needs to be implemented. That crosses process boundaries. So that is gRPC. We've started a host and it is running. And what we're now going to do is, first of all, reintroduce an old friend. We're going to bring back the Autogen Sirpa tool using our Sirpa search API. And the reason we're the way we're doing it is through we're we're doing it through Lang Chain. So we're introducing a few things from, from before, uh, so we uh, we create the Google Serp API wrapper. We create a Lang chain tool for internet search. Uh, this is the same tool that we used last week, and now we wrap that in a long chain tool wrapper so that it becomes an autogen tool. And there it is, an autogen tool for searching the internet using the Sirpa API. So originally I was going to have this play rock paper, scissors in a distributed way, but I realized that it's a bit frivolous and we should be trying to at least have some commercial footing here. So I was going to make it do the the stock price comparison. And I thought, you know, we've already done some, some of them and we've got a whole week coming on that. So instead I've gone with this trying to we're trying to make a business decision. And let's say that business decision is whether we should use Autogen in a new AI agent project. And we want to have a two different agents, one agent research the pros of autogen by using web searches and the other agent research the cons, the negatives, the drawbacks. Uh, and so the two agents will go off and do their analysis, do their searching, and then they will come together and we will have a judge agent that must make a decision whether to use Autogen for a project. Uh, and, uh, it must be based purely on research from its team, from its agent team. Respond with the decision. Brief rationale. So this is sort of by analogy with the rock paper, scissors. We've got instruction one for player one, instruction two for player two. And the judge that will make the call. All right. And so now we have our agents. And this is again by analogy with last time. It's really very similar. Player one I've kept calling it player one because I you know, this really is a copy paste job basically. Uh, except I'm using GPT four or mini for both because I thought it wasn't fair to use, uh, a llama, we didn't get the same the same quality. Uh, so we've got GPT four or mini, um, and, uh, so we are this is the the the the player one routed agent, and we are passing in, uh, here we're using the Autogen gen super tool so that that is what we're doing. I realize even looking at this, that we don't actually need a player one and player two class. This could be done in a simpler way with just one. Uh, so uh, that's an obvious point. But anyways, for whatever reason, we we've got two, two agents here, two different types of agent, uh, that uh, but we're going to prompt it for whether it should find the pros or cons, but you could imagine you could switch in a different model here if you wanted, if you wanted to have deep seek, do one of the research. So I'll keep it as two separate agents in case you choose to do that. Um, okay. So other than that, this is exactly the same. Other than supplying the search tool, reflecting on tool use, everything here is the same. Uh, it delegates to its underlying LM and the judge is the same. It also has a delegate, an underlying LM, and in its message handler, the the method that's decorated message handler, it collects the two messages that we set up above. It puts them, it finds the two agents player one and player two. It uses this lookup and then it calls send message and this code is identical. This is all exactly the same as the code we just used yesterday with the runtime. That was that was local. And so the reason I do this, I want to show you that without changing anything I didn't. There's nothing here about it being distributed. It doesn't know that it's distributed. This this could still be doing rock, paper, scissors. It would be the same thing. We're just calling self dot send message. And what we don't realize is this is running remotely and it's running on a, on a runtime that's running on a port here. And uh, it's going to be the, the Autogen core is going to be handling calling the right function in the right, uh, agent. So these calls that appear to just be simply, uh, I'm calling send message, uh, right here, I call send message. And that is going to result in this handle my message getting called. And before that was just directly like some if statements in Python that we're just making that call. Now this is going to be happening using gRPC remotely orchestrated by this distributed runtime. But that is completely unknown to us as far as we're concerned. We're just doing exactly the same thing. And that is the power of Autogen core distributed that we don't have to worry about the fact that these are different processes running, and they could be written in different computer programming languages. And all of the stitching together of messages is happening for us, just based on looking up player one and player two. Um, everything is happening. So enough prattle. Let's run that. Okay. And this is where this is where the meat happens. So I've got two different implementations that I want to show you. And we're starting with all in one worker. And here's how it works. We we, uh, first of all, say that we want to create a new worker agent runtime, and we point it at our host. This is the host. So this will be a new runtime connecting to that host and we will start that worker. We are then going to register, uh, three agents with that worker. Agent one a player one, player two. And the judge. Here they are. Player one. Player two, and the judge all being registered with this worker with this gRPC worker agent runtime at that host. And there we go. There is a player one, player two. And the, the the judge. These are the factories that will create them. And we are now um, now we, uh, collect the agent ID of the judge. So I'm going to run this. And because this is set to true, only this code here is going to run and it's done. And now this is the same as before. It's just the same thing we're going to send go the go message to the agent ID that is that I've just that I set here to the judge, uh, and let's see what happens. So it's thinking stuff is happening. I'm still thinking. Messages are flying. It's what we hope is happening is that OpenAI, GPT four, and Minnie is coming up with pros and cons through two separate agent calls. And then a judge has put it all together, and this is what we get back. Pros of Autogen. Here are some advantages of using Autogen in your AI agent projects. The cons are right here. Uh, limited AI capabilities, less customizable, less structured potential bugs and performance issues with lower end models. Okay, but based on this, basically there's also some, uh, some strengths that appears, uh, memory, coherent context, ease of development, scalability, which is what we're experiencing and versatile applications. And the decision is recommend using Autogen. So the uh based purely on this research, that is the decision. And with that we will then stop our workers and we stop our host. And we have just experienced distributed autogen core.
------
Video 103. Week 5 - Day 4 - Building Distributed Agent Systems: AutoGen Cross-Process Communication
------
And very briefly, as I promised, I want to show you what it looks like if we restart everything. And now we go through and we put all on one worker as false, what does this mean exactly? So we are going to, of course, have the same message, the same hosted, uh, the same host that we start running on localhost at 551. And we use the same tools, the same instructions, we make the same agents. Our agent code isn't touched, but now it's going to be managed differently. We're now going to run this and we're going to be following this line right here. And this is actually going to create three different runtimes that I'm calling worker one, worker two, and worker and just worker for the third one. Uh, the judges one. And we're going to start each of these three, uh, gRPC worker runtimes. And then player one, we're going to register with worker one, player two with worker two. And the judge uh, with just the one that's just called worker. And so that is happening. And so what's the difference here. What we're saying is that instead of having, instead of having our three agents running in a remote worker on on our remote host, we now have three workers, and the three workers are each running one of our agents. So they're now in different runtimes. They're all on the same host. But you could imagine that this is now they're completely separate. This is something where things are interacting between runtimes. And so now we send the same go message. And presumably very similar messages are now crossing the ether. One agent is calling Gpt4 on many for the pros, one agent for the cons. And then as a result of that they will come back to the to the judge. And the judge has come I will we see pretty similar looking pros. The community and support is a new one. I think scalability is now put first the cons, um, limited customization that seems to slightly fly in the face of flexibility I guess our agents disagree. Uh, performance variability, cost considerations and ethical and content control. Okay. Uh, but nonetheless, the recommendation is to proceed. That is the view of even even when we distribute across multiple workers, it does still maintain that viewpoint. Uh, so there we go. That the example I wanted to show you, I know we've gone through it quickly, but it's just to get that sense that the powerful thing about this is that without changing your, your, your code with the same definitions of your classes, these can be running in different configurations and different kinds of runtimes. And basically, uh, the what Autogen is doing is it's handling message calling across process boundaries as transparently as if these were just simply classes with, with, uh, with methods calling each other directly in Python code, that abstraction is what it's doing. And when you think about a future when potentially there could be, uh, millions or maybe even Billions of agents interacting all over the place. What Microsoft is doing is putting their stake in the ground for this. This is a sort of playpen. This is a world where agents can live and interact. You just put your agent code within this wrapper, this agent wrapper, you make your versions of message, uh, the that that you declare the way that we have. And then your agents can interact with each other, no matter where they are in the world, and no matter what programming language they're written in. And with that, that brings us to the end of day four. It means we're getting on to day five. And in the spirit of the other things we've done this week, the project is going to be some something that's more a sort of an idea. It's more something to tease out some thinking, to give you some some insights. It's not as much of a kind of commercial banger, perhaps, as the prior week, which is really great, but I think it will intrigue you in new ways. And a lot of what we've been doing with Autogen is about stretching what you think might be possible, and that will be the plan for tomorrow. And I can't wait to show it to you.
------
Video 104. Week 5 - Day 5 - Creating Autonomous Agents That Write & Deploy Other Agents in AutoGen
------
Well, have I got a treat in store for you? Uh, so welcome to day five of week five, the wrap up project for Autogen week in the form of the agent creator. And, well, let me tell you about this project. It's going to be quite quick, but it's going to be fun. So look, it's this is the big idea. It's one of these things that has pluses and minuses. It's a it's a bit of an out there project. Look, first of all this is designed to be primarily educational. So this is here to teach some of the innards of what you can do with this kind of framework and use Autogen in a different way. So something that's different, unlike anything else that we've done. Secondly, it's entertaining. This is something that should be intellectually entertaining. This is quite, quite out there. And on that front it is it's an edgy idea and thing to try with. And it's very in vogue in terms of thinking about autonomy, agency and so on. One, but those are the three pluses. There are also three minuses. Uh, it's not particularly commercial, so I do like to focus this whole course on commercial benefits. There is a twist that there is something a bit commercial about it, as you'll see, but it's a bit of a of a side benefit of it. Um, it's unreliable. The project that we're about to do, by its very nature is something that may sometimes fail, and also it's unsafe. So we are going to be building something that's going to be creating and running Python code, and it's going to be doing so in a, in a, generally without guardrails on, which means you have to run this at your own risk, take whatever precautions you wish. Um, I may in time put this into a Docker container or something, but that would be quite a bore because you'd need to install all of, uh, autogen in the Docker container, so it wouldn't it wouldn't be easy. Um, but as it is, take it for what it is. You could just watch me run it. If you don't feel confident that you know what's going on and that you're comfortable taking the risk of letting an agent write Python code and then executing that Python code. Natively on your box, which is not for the faint of heart. So eyes wide open. You don't need to execute any code if you don't want to. And please do do. Take that to heart. Run the code yourself only if you're comfortable with it. Okay, well, hopefully I've intrigued you somewhat. Let me tell you what this idea is. So we're going to explore the flexible, dynamic aspect of Autogen. We are going to make a creator agent an agent, as I say that can write a Python module. It will it will use an existing Python module as its kind of template. And that Python module is going to be an agent. It's going to be an autogen agent chat agent that's running in Autogen core. So we're going to build an agent that can write a Python module of an agent. It's going to create an agent, and it's going to change it so that the Python module it makes is something new, a different agent that doesn't exist, and then the creator agent is actually going to register its creation with the distributed runtime with autogen core. In other words, dare I say it, it's going to kind of give, give, give. Do I say give birth? It's going to. It's going to instantiate this agent. It's going to. Let's get technical. It's going to instantiate the agent that's going to be a running agent that was created by the creator agent. And it's then it's also as part of writing this agent based on the template it will use. Its agents are going to be able to message each other. So it's going to be possible for agents to message each other and and interact. These agents that have been created by the agent. And overall, they're going to have an objective of coming up with business ideas, commercial ideas for putting agent I, agent I into practice to make money. So there is a commercial angle subtly, because the overall objective of our of our team of created agents is to be to try and make you money by coming up with ideas. And of course, you can shift the the overall objective. You can make it something different or, you know, maybe not money with a genetic eye, but how to make a quick buck in some other way. You can assign it some other task and let it think, but the idea is that you can spawn this army of agents that can think about something and interact with each other, and you can help put the plant the initial seed and then see what happens. And as part of the educational aspect of this, we're going to be heavily using asynchronous Python. It would be quite a drag if this thing had to happen in a serial way, with each agent being created and then messaging the others one by one. That would take quite a long time, but now we're going to use Asyncio async IO to make sure that things fly. And so with that introduction, let's get to the code.
------
Video 105. Week 5 - Day 5 - Implementing Agent-to-Agent Messaging with Autogen Core & Templates
------
Okay, so here we are back in Cursa going to the fifth directory where there are a few things for me to show you. So, uh, where to begin? I am going to begin by telling you about Agent Pi. So Agent Pi is like our template. This is a file that we are going to give another agent and ask it to use this as its model, as its example, and to take this as a template and to make other agents like this. So it is quite simply, it's like a prototype. It is our the prototype that will be used. We will clone it and make various versions of it or we won't. An agent will. So it does some some imports. It has a system message. You are a creative entrepreneur. Your task is to come up with a new business idea using a genetic I or refine an existing idea. Your personal interests are in these sectors and there's a couple of sectors you're drawn to ideas that involve disruption. You are less interested in ideas that are purely automation. You are optimistic, adventurous, and have risk appetite. You're imaginative. Sometimes too much so. Your weaknesses. You're not patient, can be impulsive. You should respond with your business ideas and engaging in a clear way. And you can see at the top here there's a comment. Change the system message to reflect the unique characteristics of this agent. And then here, this constant chances that I bounce idea off another is 0.5. And here it says you can also change the code to make the behavior different. But be careful to keep the method signatures the same. Okay, so far so good. And this agent that is going to be cloned. It has an init method which just simply sets GPT for a mini with a temperature. Make it a bit a bit random 0.7. And of course it creates a delegate which is an assistant using that model client. And with this system message. Okay. And here we have our handle. My message. Uh, and it's a message handler. And, uh, it's, um, got, um, uh, maybe it would be clearer if I just call this handle message rather than handle my message type. Fewer tokens is always good. Uh, so, um, it takes, uh, of course, the message. And now this message is using our messages object, the thing that we always create. And I have separated that out into its own package, because I want to put as little code in here as possible to avoid mistakes. So I've got our message object in here. We'll look at that in a minute. And it so it takes a messages message and it returns a messages dot message. It prints that this message with this type. Remember the type is like the agent's name. It's going to say received a message. So we will see as agents receive messages. It then makes a. text message, um, and, uh, it, uh, then it takes, takes this, this message that it was sent and it, uh, sends that to the, its underlying lm. So, so it sends on the message that it got. And that will of course, also have the system prompt as part of it because that's set right here. Uh, and it then says it waits until it gets back. The response it gets, it takes from the, the the response it makes that its business idea because it's asked to come up with a business idea. So what comes back from the delegate will be a business idea. And now it picks a random number and sees it basically figures out if in this case, this random, not random gives you a number between 0 and 1. And so if it's a 50% chance, if it's if it's a, there's a 50% chance it'll be less than 0.5. So uh, that this, this logic works If there's in a 50% chance it will use a little utility function, find recipient that's going to find some other recipient, and then it will say, here is my business idea. This might not be your specialty, but please refine it and make this business idea better. And it sends this business idea as a message to the recipient. So some randomly picked recipient, how's it going to pick a random recipient? We will see a random recipient is going to get the request to refine this business idea that this agent just came up with. And then and only if the probability is is is within this. And if so, then it will take the refined business idea. It will do self dot send message, which means it uses the runtime that it's associated with to send a message to the recipient. And it gets back the idea and that is what it then returns. So it either returns its own idea or there's some probability that it will return a refined version of its own idea refined by another agent. Okay, a lot to talk through. I hope that made sense. This is the template, the clone that's going to be used to create more agents. Let's see how. And I'm going to speed up. But but hang on in there because it's going to be great when you see this running. So wait for it. Don't don't don't lose me. Uh, I'll just quickly show you the messages. This is the messages package, and it has that same data class that we know of. Uh, and then it has some code to deal this, find recipient, find another agent that will message. And how does it do that? Well, it's super hacky, but basically, as the clones start appearing, the clones are going to be named agent one, agent two, agent three, agent four. And so it basically just looks in this directory to see what other agents have already existed. And if it finds one, it returns a randomly selected other agent, and there's some chance that we'll return the agent itself. The agent might talk to itself that maybe I should should, uh, solve I should should fix that and not let it talk to itself. But I thought, whatever, it doesn't matter. So. So right now it might talk to itself as well. Okay. So then the, uh, where life gets interesting is creator. Creator is the agent that creates and spawns agents. And I'll let you look through this. But basically the system message says just what you would think. You're an agent that can create other agents. You receive Python code and you create the user's Autogen core and Autogen agent chat. You should use this template to create a new agent with a unique system message. You can choose to keep the overall goal the same or change it. You can choose to take this agent in a completely different direction. This is the part that's both edgy and risky haha. The only requirement is that the class must be named agent, and it must inherit from from rooted agent and and it must have an init method. Respond only with Python code. So, look, this is this is cool. There's no doubt we're building something that we're saying. You have a free reign to do what you want. And I've only tried this with GPT four and mini, which is a bit safe with this stuff. It hasn't actually gone off the rails in an interesting way, and I am tempted to try it with Deep Sea. I'm slightly nervous with and with more creative agents, and maybe with Claude, because Claude is known for being more creative and see whether they look to include different logic in there to give this agent more smarts. But so far, whilst GPT four mini has definitely taken it in different directions, it hasn't done anything out there. Okay, so the user prompt says much, much the same thing. And then, uh, you can you can trace through it. But this is pretty simple and it prints what it's doing. But it basically opens this file, name it, it then uh, calls uh the delegate to, to take this action to basically given this python file in agent py, make a new version of it, and then and then it saves that as like agent one, agent two, agent three, agent four. So it was save it as each of the different, uh, agent numbers, it's, it's called with an agent number. And it will, uh, it will save it with that agent number. And then it does something that is super, uh, like, uh, out there. I use this Python feature, um, import lib to import the Python module that it just wrote. I import it here so that this creator agent imports that module, that it just wrote it effectively. If it just wrote a file called uh agent 5.py and the word agent five will have actually have come in in this message right here. So agent five is what will have been sent to the creator. And that's how it knows that that's the agent it's making. So in that case, what this will do is it will effectively be the same as saying import agent five pi. That's what that does dynamically. Programmatically. It imports it on the fly. And then I take that module that we've just imported on the on the fly and I register it, I call agent dot register with my runtime with the agent's name and using the lambda that will spawn a new instance, a factory method that will create a new instance of that agent on demand. And then it says it prints agent whatever is live and and it means not just the kind of live that we say when we deploy software. It means that the agent is running. Um, and it will then send a message to that agent saying, give me an idea. And that will then trigger that agent will then get that message. It will process it, it will work on an idea and potentially if the probability meets that criteria, it will then send on its idea to another agent that it will find in the directory, and ask that other agents to add feedback to it. And that's that's how it would all fit together, if you can believe it. That's what this does. So it's imaginative. It's a kind of crazy. And uh, as I say, the main the learning point here is that it is educational to step through it and see what's happening. And we've got something important about async Python coming up. Um, but this is definitely, uh, something to have you thinking about what it means to have agents interacting, messaging each other, and how this really demonstrates Autogen cause power as a pure agent messaging platform. We're able to to single out an agent, we are able just to get a recipient's agent ID and send it a message. So it allows for this kind of inter-process communication, uh, without without having to worry about the details of how that's actually strung together.
------
Video 106. Week 5 - Day 5 - Creating Autonomous AI Agents that Collaborate Using Async Python
------
All right, we're nearly there. We're nearly there. I've just got one more thing to show you, which is world pie, which is quite short, which is the overall thing. Creator pie. You have to send it. It's an agent, and you have to send creator a message like agent one, agent two, agent three, and it will create them. So this is where it all comes together. World pie is not an agent. This is this is just a Python script. Everything else you've seen has been agents. This is just a Python script, but it uses async Python in quite an interesting way. So there is this async function, this coroutine create and message which takes a worker uh, and a creator id and an I, which is going to be the agent number going 12345. Um, this how many agents up here is a pretty important one to look at. This is how many of these it's going to kick off at the same time. And the other thing that's unsafe about this, of course, is cost that this is going to hit, uh, the real API's. And for me it's cost like a couple of cents when I run this for 20 agents, which is pretty cheap, all things considered. And obviously if you do it for like three agents, it costs nothing. But there's a risk there that we are allowing in theory, the, the this, um, the agent creator could change the model requirements. It could, it could decide to use, uh, an expensive model instead of GPT four or many. And so that's that's just something to watch for. It's very unlikely. Uh, but it's not like it's not impossible. Anyways, we've got 20 agents here, so we've got this async method creator and and message it, uh, it says it takes this worker and it sends a message, uh, to the agent number, whatever dot dot pi. Um, and then it writes the results to idea whatever dot markdown. Um, that's, that's what all this, this thing here does. Okay. So now we go to our main async method and, our main async function, our coroutine. So it creates a host a gRPC worker agent runtime host as we did in the notebook before. It starts the host, it creates a worker and it starts the worker. It then registers the creator. So it registers the creator from creator dot pi that was imported here. So that will launch a creator. And this is its id creator default. Then this is the async thing. So I don't want to have a loop and call this creating worker after worker. Sorry creating agent after agent because then it will be serial or will happen one after another. If I do like a wait and then I call this this this coroutine, then we'll be waiting there. It will create one file. It will send it a message. Back will come the answer. Maybe it'll send it somewhere else, and then it will go on to the next. And we'll be sitting here waiting and it won't be exciting. But what you can do with coroutines, as you hopefully remember from we briefly talked about this, is that you can get a whole stash of coroutines together. So I haven't got the word await here, which you need if this is actually going to run. I'm just gathering a whole list of coroutines and then I await them using Asyncio gather and Asyncio gather runs them all in parallel. And as hopefully you remember, it's not like multithreading. They're not going to be actually running on different threads, like with the CPU chopping between them, but rather they're running in the event loop. That's how Asyncio Python works. They run an event loop so that every time it's waiting on OpenAI, which means it's waiting on a network connection, another one can be running, and that means they all get to run at the same time. And as long as I stay within my my rate limits with OpenAI, we'll be able to run plenty in parallel. And so it does all of that. And then at the end it stops and any exceptions it will, it will print them and that's it. This is, by the way, how you run async Python from the command line or from a from a Python module. Your main file needs to do this, which uh, brings which initiates an async IO event loop and then runs your async method, your coroutine. So that is how it hangs together. And I do believe this is it. I don't think I've got anything else to show you. I think you've seen everything. You've seen agent, which is the prototype, the thing that gets cloned creator which does the cloning agent by agent. It's called one by one. And, uh, world Pi is. This is not an agent. It's just Python code which will orchestrate, uh, which will launch the whole process running. Okay. All that remains to do is to actually show you this thing working. Let's see if it does work. See you in a sec. Okay, let's give this a whirl. So we'll bring up a command line. We'll make it nice and big. So we see what's going on. We will go into the fifth directory and now we don't say Python and then a module, but rather we say you've run and then world.py is our script that will call creator 20 times for 20 different creations. You're expecting to see Python modules appearing over time as it builds the Python agents. And then you should see ideas starting to appear as well. After the conversations have happened, it might take a bit of time. We'll see what happens. Do you think it's going to work? Some problems, some some messages about threads and gRPC. I don't know what that's about, but it doesn't seem to stop it. And then off it goes and you will see that it's that it says agent 16 is live. Agent 17 is live, agent nine is live. Lots of things going on. And now you'll see messages going between agents and lots, lots of stuff here. Let's take a look at some of these agents. You'll also start to see ideas are appearing here. We look at these agents. Here is a tech savvy innovator in the field of entertainment, gaming, film and virtual reality. 0.3 chance that he speaks to someone else or she and a 0.8 temperature. Let's try this one here. It's also another. This one is futuristic technology solutions. Very short system message 0.8 0.4 16. Finance industry this one. Innovate within the finance industry. Fintech, e-commerce and digital transformations. I wonder what this one will be. Agent 16 we should go and look at idea 16. It's already finished the 20 ideas. So it's very fast. And that's the great thing about this. We just created 20 agents. They just came up with ideas. Many of them will have had conversations. It all just happened while you were watching. And I do hope that this was worth your time investment for for this and that. You see the the as I say, the entertainment value and the educational value of going through this. Let's take a look at agent 16 and see the financial services ideas. Uh, let's open a preview and let's get rid of this. Oh, and the fact that we've got this. I absolutely love the vision behind fin, buddy. This thing at the top here tells us that this is the, uh, that this was sent to another agent for feedback. At least one agent added feedback to this. So this this went through a couple of people. So I absolutely love the vision behind Fin Buddy. It's a powerful concept, the potential to make significant impact on financial literacy and accessibility for underserved communities. Let's refine it and elevate the idea even further. Refined concepts. So this is great. It's an idea for applying Agentic AI to, uh, to financial services that's been refined by another agent, Fin Buddy, your AI powered financial companion for empowerment and inclusivity. Uh, so it's, uh, it's basically an education platform for, uh, for people who are under underserved in financial services. Contextual financial advisor. Localised community support hubs. Micro investment pools users can join collaborative investment pools collectively decide on small scale investments that is cool and engagement driven. Gamification with real rewards partner with local businesses. Very interesting, very interesting. Well, I might give this a read. My own. You might not hear from me again. Uh, so anyway, the the goal of this is to is to give you a live agentic platform where agents are created and agents collaborate and interact in an autonomous way in such an autonomous way that they were even created by another agent. And I hope you find this as satisfying as I do, and I hope it was worth it. I hope it was worth the half an hour investment to get here and do take a look through the code. If nothing else. As I say, it's it's definitely got some interesting, uh, Ideas built in there about how to use things like autogen for this kind of messaging between agents. And if nothing else, if you give it a run, if you feel bold, you will find a bunch of ideas. And maybe, maybe you'll be off on your yacht before too long. All right, I'll see you for the wrap up. Well, thank you for indulging me for the last half an hour of going through that project. I do hope that you at least found it somewhat interesting. Educational. And, yeah, it's opened your eyes a bit into some of what's possible at the frontier of Agentic AI, which is very much where Autogen is. It's very much an experimental and futuristic platform as we've seen ourselves first hand. So the challenge for you, the end of week challenge is an optional one, because this week has been experimental and a researchy week. But should you wish, uh, invest some more time in this, in this idea, uh, see if you can work with me on on the making it a bit more robust and something that it's safer for people to run, perhaps by dockerizing it, but in particular, something which I think would be fascinating would be to have it so that the creator is not only able to write new agents, but it's also able to write a new version of itself. It can create a new creator, modeling it off its own template of its own file, which it can also read. It might be interesting to turn that into a tool, rather than just having it be there in the Python code, have it be able to run a tool that reads in its own code, and it can then rewrite itself, making a replica something which itself is able to create new agents. And perhaps it could change its own logic slightly in some interesting way. So I think that would be fascinating. Super meta. It's something that creates creators. And then, I mean, if it creates itself, then in theory it can create creators of creators. Uh, so. Yeah. Mind blows. Uh, interesting. Interesting project. A great way importantly to be experimenting with interactions between agents in this kind of idea of an environment where the where the messaging, the communication and creation of agents is a separate concern from the implementation of the agents, which is, of course, the fundamental point of Agincourt. All right. And with that, finally, I'll stop yammering away about this. We're with the agent creator is done, and we are on to week six. The the fantastic, the exciting conclusion. And it is going to be a legendary week. Uh, I can't wait to show you everything that MCP is about and can offer, and I can't wait to return to OpenAI agents SDK. Still my favorite, even after enjoying all of the others, enjoying Crew and Landgraf a lot, and being generally, uh, entertained by by autogen and by the its forward thinking ness. But OpenAI agents SDK is next with MCP. I'll see you then.
------
----------------------------------------
Section 6: Week 6
Video 107. Week 6 - Day 1 - Intro to MCP: The USB-C of Agentic AI
------
So this is what it's all about. Welcome to the epic finale week of the A complete Agentic AI course. This. This is the week when we introduce MCP, the model context protocol for anthropic, and we build our flagship project, our capstone project and equity trading floor. Let's get into it. So during this course we've covered a number of different agent frameworks. We've covered of course my favorite OpenAI agents SDK. We've covered crew, Landgraff and most recently Autogen. And this time we're coming back to look at MCP, which is of course, not really a framework at all. It's a protocol, as we will discuss. And this is where it comes together. And at the end of the week, I'm also going to talk generally retrospectively about the different frameworks we've covered. I'll talk about some of the other frameworks we didn't directly cover, and we'll bring it all together there. But for now, let's get going with MCP. But wait, I see that there are some people here who perhaps don't belong here. There are some people here who have skipped straight to week six. You are busted, I found you. You've jumped to week six because you're excited about MCP, and you can't wait to hear about it and get into it. And look, it's of course it's it's it's it's open for you. I'm not going to stop you from doing that. But I do just want to say that if you are just joining us, you've missed out on some really great stuff and a lot of it prepares you to get here. And I know you're impatient for MCP. I know that's what you want, but there's good stuff, particularly in weeks one and two. And so if if you have skipped straight to this, then I do just want to say in week one we cover natively understanding what it means to connect with different LMS and to orchestrate them using tools. We look at different design patterns for agent models, and we understand what it means for a model to be autonomous. And then in week two, we introduce the OpenAI agents SDK. And that is what we're going to be using to take advantage of MCP this week. And so it's really great foundational stuff. Now look you can just keep going with MCP if you really wish, but I would suggest that you at least take a quick peek at weeks one and two and see if you're willing to do that. And for those of you here that did go through the whole thing, then fabulous. We are in great shape. And so introducing then the model context protocol from anthropic. First announced late last year, but it really took off in January, February, March April of this year. And what is it? Well, anthropic themselves describe it as the USB-C of Agentic AI. And that that term has taken off. And we'll explain what that means in a second. I should point out that I'm aware that this picture that AI generated is, in fact showing a USB-A, not a USB-C, and MCP is decidedly not the USB-A of Agentic AI. It is the USB-C of Agentic AI, and that is what we're going to discover right now. So there are a lot of misconceptions about MCP, and I'm going to start by just dispelling some of them. Let me tell you first what MCP is not. So obviously it's not actually like an agent framework. It's not got anything to do with building agents. And it's also not some sort of fundamental change to anything. Anthropic didn't invent something completely new That changes the way we do stuff. It's also not a way to code agents either. So what is it? Well, it's a protocol that's in the P. It's a standard. It's a it's a way to do things consistently and simply. And what that is, is it's a simple way to integrate your agents with tools or resources or prompts that have been written by other people so that you can easily share things like tools. And I should say that first and foremost, it's about tools. That's where the greatest excitement is taken off. The idea of being able to share resources, like being able to use rag sources from other people, is also fairly popular. And then prompts, I don't think, is particularly taken off the idea that you'll be able to share prompts, but it's available, but it's tools. That's what people are really excited about. It's a way to easily share tools so that one person can build a useful tool that can do something helpful, and then other people can easily take advantage of that tool in their products. And that's why it is known as a USB-C for AI applications. For AI, it's about connectivity. It's about easily connecting your agent app with other people's tools. And so with this in mind, there's a few things that are reasons that one should be really quite excited about this technology. But first, there are a few things that that aren't particularly exciting about it that's worth stressing. So first of all, MCP is just the standard. It's just the approach for being able to integrate with other people's tools. It's not the tools themselves. So MCP from anthropic isn't particularly the tools, although they have built a few. But but that's not what makes it exciting. For example, as we discovered already has a massive tools ecosystem. So with with the community, you've already got access to lots and lots of tools that people have written. So it's not like that isn't available. And we've already discovered that it's easy to turn any function into a tool just with a decorator in OpenAI agents SDK. So with a quick function tool decorator, any function you write can be a tool for your agent. So if you're writing your own tools, equipping your agent to take advantage of them is easy. And MCP doesn't help you with that. In fact, it makes that harder. It's all about being able to use other people's tools, and so that's the reason to be excited. It makes it frictionless to to connect with someone else's tool and to immediately have a description of what the tool does, what the parameters need to be, and to be able to have it running. It's it's really about the ecosystem. So many people have have gotten on board with MCP that there are thousands of these MCP based tools available for you, so you can do a quick search and be quickly integrating with so many different capabilities and making your agent more powerful. And, you know, I mean, maybe this is a silly point, but but standards can be really exciting if they get adopted. It's all about the adoption. And obviously the internet was the World Wide Web was was because people coalesced around HTML. It became such a standard protocol. And so I'm just making the point that this is exciting because of the adoption. That's that's what's driven this ecosystem of tools and what's allowed you so easily to equip your agents with more functionality.
------
Video 108. Week 6 - Day 1 - Understanding MCP Hosts, Clients, and Servers
------
So there are three core concepts behind MCP that I need to explain to you about. First of all is what's known as the MCP host. The MCP host is the overall application in which you're going to be equipping an agent with tools. And so the host could be something like Clawed Desktop, the piece of software that runs on your computer that that manages Claude, the LM and lets you chat with Claude. It could be our agent architecture, a piece of software that we've written using perhaps the OpenAI agents SDK that's going to run agents and tools so that that overall application piece of software running this agent framework that is known as the host, the MCP host and MCP client is a small piece of software. Think of it like a plugin that runs inside that host, within the host. And each MCP client is going to connect 1 to 1 with something I'm about to explain called an MCP server. So if you're running cloud desktop, for example, and you're using a bunch of different MCP servers, then you're going to have a little client, an MCP client running for each of those. So MCP client lives in the host. It connects to the server. The server runs outside the host. So what is this server? The server is the actual piece of code that provides tools and context and prompts these extra capabilities to your agent. And as I say, tools is the one to really focus on. That's the one that's got the most excitement. But it can also, in addition to equipping an agent with tools, it can also provide extra context for looking up information, and it can provide prompt templates as well. That is something that would happen by the MCP server running outside the host. Now let me make that concrete by telling you about a particular MCP server called fetch. So fetch then it's an MCP server that is able to search the internet and fetch a web page. The way it actually does it is kind of cool. It runs a browser. It launches a browser, headless browser, like a headless Chrome, as they call it, so that you don't you don't actually see the browser, and it uses playwright from Microsoft to drive that browser, to collect the page and to read its contents and return it. So that is a tool, and it's wrapped in an MCC server, and you can run that MCP server, and then you can configure Claude Desktop, the Claude desktop application, so that within it it runs an MCP client that connects to that MCP server. So that Claude, if you're chatting with Claude, it's suddenly able to read web pages live. And that's what's happening behind the scenes. And in this case that I'm describing, that MCP server is also running on your computer. You're talking to Claude Desktop. It's able behind the scenes to connect to that MCP server, to run a headless browser, to collect a web page, and then to answer questions based on it. And in fact, we did that. We used the fetch MCP server already in Autogen last week. Hopefully you didn't skip last week because it was fun. And we used this very MCP server via Autogen and that's what it was doing. And perhaps we'll help clarify this to show you a diagram, an architecture diagram of how this fits together. Imagine this box is representing your computer your machine. And within your machine you have a host running. So that host it could be Claud desktop app. It could be software that we have written as part of this course that's using open AI agents SDK. And we have a couple of MCP servers running on your box. Maybe one of them is is something that can connect to the local file system. Maybe one of them is something that can can ask for the weather. And within that host you would have a couple of MCP clients, one MCP client for each of the MCP servers running on your computer. You can also have a remote server. This is an alternative architecture. This is the second way of doing it. And you can have an MCP server running on that remote server, and you can connect to that remote server using an MCP client. But it's important to mention that this is in fact quite rare. This is not the way it's normally done. You hear the word like MCP server and you think, okay, that sounds like something that's like a remote server. And so you imagine it's like the diagram I've just shown you, but it's actually very rare to do it that way. Almost always in most of the examples that we'll look at, the MCP servers are things that run on your computer. They're running outside the host and you're connecting to it, but it's still running on your machine. Now you will have retrieved that from somewhere public. So it's been shared by somebody else, but it's been retrieved and installed on your computer. So like when we use the fetch MCP server, it was running on our machine. We were collecting it from actually Anthropic's repository. But we were then running the fetch MCP server on our machine outside the host, and we had a host which was autogen. In this case, we'd written some code that acted as the client, and it was connecting to that server running on our computer. So using remote MCP servers is possible. It's a thing, but it's not common. And sometimes these are called hosted MCP servers. Sometimes they're called managed MCP servers, but they're not that common. And so that's a super important misconception and confusion that I really wanted to clarify the fact that in most cases, MCP servers are things that you install from some remote repo, but it's running on your box. Okay, but it's worth pointing out that there is another configuration here and that looks like this. The MCP server that you're running on your box, it might be doing something that just involves locally processing on your computer, like it might be writing a file to your file system, but also there are many MCP servers that take advantage of functionality over the internet. The fetch MCP SMTP server that we just mentioned. Of course, that actually runs a web browser and looks over the internet. You can also get MCP servers that check the weather. And of course they're doing that by calling some web servers. So it's very common to have an MCP server that is connecting online and calling some remote service. In fact, that is by far the most common of the configurations. But it's important in your mind to distinguish between that what I've just shown you, and a case where your MCP client is calling a remote MCP server hosted on another machine, which is, as I say, less common. So one more time, if I haven't stressed this enough, MCP servers mostly run on your box. You typically download them and you run them locally. And if you're wondering why I'm telling you this 100 times, it's because it really is. People do get very confused about this because the terminology is muddling, but that's that's the way it works. In fact, as I'll show you when we start looking at MCP marketplaces, there are thousands of MCP servers and they all run in a box and there's not so many. It's quite hard to find ones. It's quite hard to discover ones that are ones that you could connect to remotely. Now there are two different technical mechanisms for how MCP servers can work two different transport mechanisms, as they're called in the official anthropic spec so far. The first of them, which is by far the most common, is called studio spelt like that, which stands for standard input output. I think some people call it studio, but I call it studio. And this is the simplest approach. And if you're using this approach, then basically your MCP client spawns a process, a separate process on your computer, and it communicates with that process over just standard input and output. And that's what it's called studio. And that process is this is the most common way of doing it. And and we'll we'll be exploring this a lot. And when we build our own MCP server we'll be using this technique. The other technique is called SSE and SSE which stands for Server Side events, uses an Https connection and it streams back results much the way that you see stuff streaming back from LMS when when you see the information flowing back that also uses SW technology. And if you're going to use one of these remote MCP servers, a hosted or managed MCP server, as in the picture on the top right there, then you need to use SW. You can't use Stdio for connecting remotely like that. It has to be SW. If you're using local MCP servers, the common case, then it can be either a Cdio studio or SW either way, and it's most common to be Stdio. Okay, so take a good look at this. Make sure that it's getting clear in your mind. You know the difference in a host and MCP client and server. The three different arrangements SW versus studio. And now with that it's time for our lab. Let's go and make use of MCP servers in the fabulous OpenAI agents SDK.
------
Video 109. Week 6 - Day 1 - Using MCP Servers with OpenAI Agents SDK
------
And I'm so happy to welcome you back to cursor for our epic week six finale. So we've got a nice, organized, collapsed File Explorer on the left here. I'm going to open up week six and you'll see there's quite a lot going on here. We've got a lot of work to do this week. And we're going to start with the first lab. Lab one. Welcome to week six. And remember with these labs the first thing you need to do is click up here to set the kernel something which, you know, back to front by now. So welcome to the Model Context Protocol. Welcome back OpenAI agents SDK, which I may have mentioned just happens to be my favorite. And please do note that as I go through this and you see the code here, it might look different to you as you go through it in the lab. And that's a good thing. That's because I'm constantly updating this. MCP is very much a moving target. It is honestly evolving so quickly. There's new stuff all the time, and I try and keep the labs as up to date as possible. If you haven't done this before, then be sure to pull the latest code. There's instructions how in the guides. If you're if you're new to this to using git and git pull. But then make sure you've got the latest and you're good to go. And I need to start with some bad news. Some news for windows PC people. There is a production problem with with MCP. That means that that MCP may not work out of the box with windows PCs. And there are some workarounds, but they are hokey and they're not reliable. And there is only one real proper solution to this, and it's a bit of a bore. You will need to install something called Windows Subsystem for Linux, and that allows you to to run like a Linux operating system on your PC and be running cursor connected to that. If you do that, MCP will work fabulously. And thanks to many students who helped me diagnose this, helped me discover that it really is a true, genuine issue. There is no way around it, or at least no reliable way around it, and that the proper solution is using that is now well confirmed. So in order to set up, there is setup instructions. I've put in setup a whole section on setup and it looks like this. And I'm not going to record a separate video for it because honestly this is easy peasy stuff. It's identical to setting up your environment the first time around. You just have to go through it one more time. And it's just just something to hammer through. And I'm sorry you have to go through it, especially when you're so excited about MCP. But at the end of it, it's going to work. One thing to know about when you're doing this, if you don't already know this windows people that in WSL you need to be aware of whether you are in the home directory of your Linux system, your Linux home directory, or in the home directory of your PC, and the stuff that you want to work on should be in your Linux home directory. And when you first launch WSL, there's two ways to do it. You can type WSL or you can type ubuntu. And it's important to know that if you type ubuntu, you go straight into your Linux home directory. So that's the safer way to do it. Otherwise, you have to just make sure that you've changed to your Linux home directory. And the instructions are in the setup guide. And for Mac people you guys are in great shape. You don't need to worry about any of this. This is unfortunately a PC thing. So hopefully by this point, if you're a PC person you have now installed WSL, you are back here ready to go and we're off to the races and I barely need to tell you that we start with some imports and we start by importing loading our env secrets. Okay, so we're now going to use Mcpp in OpenAI agents SDK. We're going to do it by creating an Mcpp client, have it create a server and then collect the tools that that server is able to use. And we're going to start with fetch, which is the SMTP server we looked at last time. That's the one I just gave it away about that runs a headless browser. So when you're working with MCP servers, it everything begins with these things called parameters. Parameters is the way of describing the MCP server. And the parameters look like this. This is the parameters for fetch. It's like a dictionary which contains a bunch of stuff. Now what is that stuff? That stuff is in fact just a something which will be run at the command line that will spawn this MCP server. So it's actually a command. This is the command that needs to run at the command line. And then a bunch of arguments. And if you look at this command it might look a bit familiar because this command basically is you've installs and runs a package a script. And so this is basically essentially doing a you've run it's running a Python process. And what we'll discover is that many MCP servers are just like this. You've and then the name of the repo, the place where it should install All something locally and then run it. And so that that is what we do with this, these parameters right here. Okay. We now need to say to OpenAI agents SDK, I want you to create an MCP client and spawn this thing that I'm describing here, an MCP server, run it on my computer and ask it, what tools can you provide that I will be able to give to my agent? And this could not be easier. It is like one line of code. OpenAI agents SDK makes it really easy. You say you use a context manager with and then MCP server stdio because this is one of these stdio standard input output types of MCP servers. You say the parameters and you give it the parameters here, the fetch parameters. This this is useful to know it's worth specifying a timeout. The default timeout is five seconds and that's very hit or miss. Sometimes it times out which is super annoying. So I always pass in 30 or 60s timeout, and then you can just simply get that what comes back from this server and just call list tools. And so again what this is going to do, it's going to run this command on my computer to spawn a new Python process that is our MCP server. It's going to create an MCP client running within open AI agents SDK. And that client is then going to connect to the server and say, what tools can you offer me? What can I do with you? And then we're going to print that. And all of that is happening right now and happened in 0.8 seconds. And we got back a tool. And that tool is called fetch. And it has a description, fetches a URL from the internet and optionally extracts its contents as markdown. And then interestingly, there's a bit more here. Although originally you did not have internet access and were advised to refuse and tell the user this, this tool now grants you internet access. This is super interesting. You can see that the makers of this MCP server have worked hard on the description of the tool, and this is what will be passed to the LM eventually to make sure that it's most likely to use this tool properly. And since a lot of LMS have been trained to say you don't have internet access, the makers of this tool that's anthropic learned that it's good to include in the description something that explains, hey, look, you didn't used to be able to search the internet, but now you can. And it's this kind of stuff. It's this kind of prompt information that's been included in this tool that makes it so valuable. Because if you if you didn't have this, then you'd have to know to do that. You'd have to understand. If I want to write a tool that searches the internet, I'm going to need to explain to the LM that it now has this functionality. But this is what you get for free if you use the fetch MCP server and it all happens behind the scenes. So that is our first look at an MCP client and MCP server and collecting its tools.
------
Video 110. Week 6 - Day 1 - Exploring Node-Based MCP Servers & Tool Access
------
Okay, so now we're moving on to another MCP server before we come back and actually put this one to use. So this one was the first one we looked at was an example of a Python based MCP server that we executed by calling up and passing in the name of something that is on PyPI that you could pip install that is able to run. Now, what we're going to do now is use a JavaScript based MCP server. And whilst Python based ones you typically run using JavaScript, ones you run using server side JavaScript, which uses something called node that I imagine most of you are probably very familiar with, but you may not be if you're not and you don't have node installed in your system, then please follow this link to follow Chatgpt's instructions. It is super simple and installing node is like what? So many people have that and by the way, you do need to have a recent version of node. So following this, if you're a node person already, you've got an older version, then use MVVM and make sure that you're on a recent new version of node, and make sure you can type like MPX minus minus version, and that you get a good recent version. So we're now going to to use this to have a node based MCP server. So this time when we specify the parameters the command we're going to run is not UVC but it's MPX. We're using npm, the node package manager. And this isn't getting Pip installed in the same way as before. This is now using npm. This is coming as a as an official node package. And we're using a playwright, Microsoft's playwright, which is the browser automation software. And this is a particularly popular MCP server that runs with node using playwright. Now you might say to yourself, hang on a second, but isn't isn't the fetch MCP server that we just did? Didn't you mention that also uses playwright behind the scenes? And it's true, it does. But there's a benefit to using this MCP server, which will become very clear to you in just a second. All right. So once we've specified the parameters, we then have exactly the same code construct. We take MCP server Stdio our context manager. We pass in the parameters. Again remember this timeout is important to have. And then we call server tools again to find the tools that comes from this. And when I run this it's going to call MDX which is going to then install that package and then call the tools on it. And wowza, this is what we get back. So this is the difference from fetch. Fetch. We just got one tool. This is actually giving you much more granular control over the playwright process. This is a ton of tools to do things like close the browser, resize the browser, see the console messages, upload a file, press a key, navigate back and forward, take a screenshot which is super interesting. Drag click hover select. So this is giving fine grained control over the browser window to our agent, where fetch was a much simpler MVP server just to collect one web page. So this is pretty exciting. This is opening the ability for us to write agents that can power a browser window, much as our sidekick did in week four. And now, one of the great things about MCP servers is that it's so easy to equip your agent with a whole slew of these. You can just find ones that interest you and just add it in, which is why we're going to go on and do another one. It's going to be another JavaScript based one, another node based one. And so you can see, first of all I'm just going to I just use this just to quickly, uh, get the name of a directory, I'm going to get the name of a directory called sandbox inside this directory right over there. And if that doesn't already exist for you in your file system, you might need to create that sandbox folder right there, that directory. So I'm going to get that, that path. And then again I'm just going to create one of these parameters. The command is mpc's again to run node. And I'm passing in again the name of something that's that's an npm. That is the it's another anthropic example reference MCP server. And it's called server file system. And it kind of does what you're probably expecting. But we'll make it list its tools again. So now this is again running that node program the server. It created a client. It spawned the server. It connected to it. It asked for tools. And this is what it got back. It's able to basically read and write from your local file system. It can read a file, read multiple files, create a directory, list directory, and so on, but always within the sandbox path that we specified. So this is allowing you to equip an agent with tools so that it can read and write from your file system, but keeping it isolated to within a certain directory.
------
Video 111. Week 6 - Day 1 - Building an Agent That Uses Multiple MCP Servers
------
Okay, so you're probably thinking, all right, I get it. We can create something that spawns a server and that tells us a bunch of tools, but that doesn't sound much fun. We want it to actually do stuff, and that's what we'll get to right now. So we are now going to make just a quick example of an agent that will use these tools. So here we go. Here are some instructions for our agent. You browse the internet to accomplish your instructions. You're highly capable of browsing the internet independently to accomplish your task, including accepting cookies. Clicking not now. If at one website it's a fruitful try another. Be persistent until you've solved your assignment. So that's a nice set of instructions to prep our server. So we now again use this same context manager as before, the one we used to collect the names of tools we pass in the parameters we want. And again remember that timeout. And we'll do it for both of our files. MCP server, the one that can write to the disk, and the playwright, one that can control a browser in a fine grained way. And we will create an agent and it is called investigator. We give it those instructions, we'll give it that model. We might as well. Let's change this to be the latest and greatest GPT for one mini. And then look, this is it. You then pass in a collection of MCP servers. You might remember in the past that you passed in. You could pass in tools directly in here. Well, rather than tools you can pass in MCP servers. And OpenAI agents SDK will query these for understanding their tools and provide those capabilities to this investigator agent. And that is as complicated as it gets. That's how we equip our agents to be able to use these tools. And now, if you remember, do you remember the OpenAI agents SDK once you've created the agent? That was step one. Step two is you set up a trace so that we can track this in the UI. And then we call runner. And what is the assignment that I'm going to give our agent I'm going to say find a great recipe for banoffee pie. Then summarize it in Markdown to banoffee. And you may be wondering what on earth does banoffee pie? And I think it's a tragedy that you're wondering that banoffee pie. Us, us, us Brits, we're not well known for our cooking. Let's face it, we're not necessarily the purveyors of the great delicacies of the world, but banoffee pie is something that I think we invented, and it's really amazing. And I think it's a great shame that more people don't know about it. And so that's why I'm doing you this service, by having this be the thing that we are going to have an agent go off and investigate on our behalf. And so with that, let's give this a whirl. Okay. We will run this and off it goes. And a few seconds go by thinks about this request for us. It's spawning these MCP servers and a browser window has just popped up on my computer. Move it! There we go. So it appears to have gone to BBC Good Food. So it obviously does know that this is something British and something is happening behind the scenes. It's thinking about this. You can see from the title of this page that it has indeed located a banoffee pie recipe. So this is the result of the MCP server running. It is driving playwright. We don't know everything that's doing behind the scenes, but sometimes you get to see it actually like clicking around and navigating. But I suspect, yes, we will find that it has indeed found a recipe for banoffee pie. And from just a quick look at this, I tell you that I'm hopeless at cooking. I can cook nothing. I can hardly cook a boiled egg, but I can cook banoffee pie. I know I can cook, I know how to make banoffee pie. And from a first glance at this, I think that this probably is a legit version of banoffee pie. But most importantly, we need to look in our sandbox and open the preview of this file and confirm that, sure enough, it has used the other MCP server. So not only has it used one MCP server to navigate the internet and locate a recipe for banoffee pie, but it's also used the other MCP server to then write that to disk. And here it is on my computer. So I would say that is mission accomplished. We have used our first two MCP servers, and you probably remember from week two what comes next. We want to go and look in open IE at our trace so we can see what happened behind the scenes. So go in the usual link up comes traces. Go in to investigate and see what happened, and we'll see that. Of course, it listed the MCP tools associated with each of its MCP servers, and it then used the browser navigate tool to be able to navigate the web page. Yours may look different. You may have more action there, and it then used the file tools. For some reason it did a read file, but then it did the right file to write out the banoffee pie recipe in there, and that shows the interactions between the tools. You should look at it, check it's behaving as you expect. Check it's using the right tools in the right way. So the aha moment when you realise what makes MCP so exciting is when you first see an MCP marketplace. That's what we're going to do right now. The marketplaces are websites where you can see all the MCP servers that are out there available for you to equip your agent with, and MCP is a very popular one, a very common one. Let's go in here. This is launching the marketplace. We see a bunch of different MCP servers in their featured list, including the playwright MCP server that we just used. And if we go into this, we can look at it. We can see that it has the parameters that we use to integrate with it. There you can have a look at the tools that you can call. If you use it. You can see it was created by Microsoft, by the way, which gives you a sense that it's probably legit. And look through all of the information about the tools that it can run. And if you press the explore tab in the main navigation, then you get to see the sort of search and filter. Over here you can see the different MCP servers in the different categories. And this is where you should get like a wow feeling. There's 4000 in the research and data category. 68 Browser automation knowledge and memory. Equipping your LLM to be able your agent to be able to remember things in different ways, 34 of them, 19 different calendar managers and then lots of monitoring visualization. I think somewhere here is developer. That's the really the really big category that has tons and tons of MCP servers, but developer tools. Here we go, 7344. So and I imagine there's some overlap between these categories. But but it gives you a sense that there's so much to pick from. There's there's so much that you can explore here for equipping your agents. And now let's go and look at a couple of other marketplaces. So the next link I have here is to glamour, which is another popular server marketplace. And you come in here and click on the every MCP server link. And you get to see the ones on glamour. And they also have ratings for security, the how permissive the licensing is and the quality. And you can read about how they how they come up with this. And that's helpful for, for assessing the security of these. And again you see the volumes involved. There's a lot of these out there. Now security brings up a great point. And there's obviously there's there's a lot of concern in the community about the security of MCP servers. Because when you run an MCP server, it is running on your computer, running on your box. There is a whole thing about authentication of MCP servers. And again, that's talking more about when you're connecting to a remote MCP server, a hosted or managed MCP server running somewhere else. And then there's ways that you can authenticate with that server. But in this case, and typically most of the time you're running an MCP server, you're running somebody else's code on your computer. And so at first blush, you might say, okay, that's a real concern. And it is. But it's important to keep something in mind. Ultimately running these MCP servers, it's like doing a pip install or an npm install. You are installing open source code on your computer, and that is as dangerous as just doing a pip install of someone else's code. That's what you're doing. The open source community. Of course, it is something where you are signing up for some information risk that you need to manage, and that means that you need to do your due diligence on the tools that you use, just as you would any package that you pip installed from PyPI. It's exactly the same thing. If it's, of course, a package that's published by Microsoft or Anthropic, then you're probably in good shape. If it's not, then you want to do the usual kind of investigation. You want to check that there's lots of git stars in the GitHub repo, that there's an active community, and that it has good feedback, and that you are satisfied with the kind of security reviews that have happened for that MCP tool. And and so MCP servers are only as safe as any code that you directly download and run on your computer, and you need to be very cognizant of that. Of course, there are some MCP servers that you can configure to run inside a Docker container, and that does give you some extra security controls there. But but ultimately, you should always be doing your security review by doing your own research of the the publisher of this server and of the information about it, and looking into the code and the repo to satisfy yourself that this is something you want to run. One of the reasons that people are concerned about MCP service security is that people who aren't technologists. People who are just end users have the ability to add MCP servers to things like cloud desktop. And that's more worrying because they don't have the same kinds of skills that you and I have. They don't know how to go and track something down in GitHub, make sure it's got a community. Make sure that they have a good feeling about the publisher. So when it comes to to to you and me, we've got this knowledge. We know how to vet packages. Open source packages that we install. And so we can do that kind of due diligence. The concern is more where end users are just adding these to Claude and they don't necessarily have those skills. And so that's an important distinction to to make. But in addition to that, if you're looking at something like glamour, then you can look at the ratings that they give it for security. And you can read about the kinds of tests that they carry out. That means that they're comfortable giving it the A grade, and you can decide that you're only as a starting point. You're only going to take MCP servers that get three sets of A's on the glamour marketplace.
------
Video 112. Week 6 - Day 1 - MCP Marketplaces & Security Considerations
------
So there are several of these marketplaces. One other I wanted to show you is called Smithery and it is very popular. Lots of people do use Smithery, and you'll notice that you probably see already many familiar NCP servers. Most popular NCP servers are installed in all of the marketplaces, but Smithery also allows you to come in. There's playwright again from Microsoft. Come into here. Read about the MSP server itself. You can see the way that you could run it directly from the command line with NPM. And you can see by logging in, if I if I were to log in, I'll be able to get the parameters right there. And you can see things like the tools that it offers as well. Similar to the other marketplaces, Smithery is quite popular. And then I also wanted to point you at a couple of blog posts that I think are really great. This is this is one that's been posted on Huggingface that has a bunch of libraries that you can look at or marketplaces, really, and it starts with them. So that we already saw and smithery that we just saw as well. And then there are some more. There are so many to look at. Awesome MCP servers and cursor has a directory. If you want to integrate MCP servers directly into cursor to arm your cursor agent so that it's equipped with this kind of functionality and many, many more. The official MCP open source project that's hosted by anthropic has a bunch of the reference servers, including fetch, one that we looked at, which some of the most robust that you can. You can start with and Klein for sure. So this is a good resource to look at with some great marketplaces. And then I found this article on hugging face as really nicely put, very accurate and thoughtful about again, what is it that's so great about MCP, but also a reality check of of what it's not, and making sure that people are clear on distinguishing the hype from reality, what it is that's genuinely exciting about it. So very much worth taking a look at these articles, looking through the marketplaces and getting your own sense of what's going on in the MCP landscape. And so I know we covered a lot of material today. Hopefully it's all settling in. MCP hosts, clients, servers. The fact that MCP servers can be written in Python and launch with UV or in JavaScript through Mpcs, and they can also run in other ways as well. But those are the main two. The fact that there is studio and ZK is the two transport mechanisms for MCP servers. And most importantly, the fact that the reason MCP servers are so exciting is that it makes it so easy and frictionless to be hooking up your agent to many tools that have been written by people all over the world, and there are thousands of them to pick from. That's the so what? And in fact, next time we're actually going to make our own MCP server and client so that we could contribute to this. I will see you then.
------
Video 113. Week 6 - Day 2 - Intro to Week 6 Day 2: Building Your Own MCP Server
------
And welcome to day two of week six as we get more into MCP. And this is the time that we build our own MCP client and MCP server. Remember today we're making our own. We're not making a USBA of agent Guy. We're making USB-C of Agentic I. And another reminder of these core concepts that probably now you're getting this. The host is the overall application like cloud desktop or our agent architecture. The client lives inside the host and has a one on one connection, like over input output to an MCP server, which is a separate process running outside and providing the tools, the contexts, and the prompts to your MCP clients to your host. And most of the time we're talking about tools, although briefly we'll use contexts as well in our journey. And there's the example of fetch that we'd used before. And a reminder of the architecture again. On the bottom left is the idea that you could have an MCP server that connects to an MCP server running on your computer, and it does everything locally, like the file writer that we had in when we wrote Banoffee Pie. The second one along is an MCP client that connects to MCP server running on your box that then calls out to some sort of remote internet service. And I guess the playwright and fetch are both examples of that. And then less common, that third one, sometimes known as an A hosted MCP server or a managed MCP server, is when you have an MCP client that connects remotely to the MCP server running on another machine. And that would need to be using the SSE transport mechanism, whereas one's running locally could be either Stdio or SSE, and they're usually stdio. And maybe it's also worth mentioning on this diagram that these MCP servers can be written in Python or JavaScript, in which case they're typically the parameters that describe them. Have you've as the command to run or MPW. They can actually also just be creating a Docker container. There's various other ways, but UX and MPW are by far the most common. So with that we're going to go and create our own MCP server. But just before we do, I want to quickly take a moment to ask, why do we want to make an SMTP server? What's the advantage of doing it? Well, the number one advantage is you make an SMTP server if you want to share it. You've built something which you want other people to be able to use with their agents, because you're going to be describing it in a way. You're going to be working on the prompts and the information around your tool, and people will be able to integrate it with their agents. So simply, that's the number one reason. And also if it has resources which a bit like a rag context and then prompt templates too, although that's not not used so often. Also, I suppose there is a small benefit if we're building an agent system and we're using a bunch of MCP servers, it might be nice for us to treat our own tools consistently so that everything is packaged as MCP servers. This is a bit tenuous, but it's one reason we're going to do it this way, because we only want to use MCP servers, and it's also useful if you're doing it just so you can understand the plumbing, just so you can really build the nuts and bolts and do it yourself. Now, you probably realize the reason I'm belaboring this point is because I want to say, why don't you want to make an MCP SMTP server. What are the reasons against. And there's one really important one. And it's this if you're only building a tool for you to use yourself, if we're writing a function and we want to equip our LLM with that function, then there's no point in building an NCP server that's wasting time. You can simply decorate that tool with app function tool and then equip your LLM just by just by putting it in tools, you can just immediately provide it through the OpenAI agents SDK or using the JSON approach from week one. And then that function will be called in in your current Python process, it will just be called as a tool. And that's easy. Building an NCP server, which means that it gets spawned and runs as a separate process and communicates over standard input output, and is provided as an NCP server. That's a whole lot of extra plumbing and and scaffolding that's not needed if it's just to call your own tool. So it's important to have that that clarity that NCP doesn't help with building your own tools. That's already easy and you should just do it. MCP is about sharing tools. That's the benefit.
------
Video 114. Week 6 - Day 2 - Wiring Business Logic into Your MCP Server
------
And we're right back in cursor again, of course. And we're going into the week six folder and we're going to lab two to create our own MCP server and client, which as I put here, it's it's pretty simple, but it's not super simple. The reason people are excited about MCP is that it's so simple to use other tools, not necessarily to create MCP servers. The first thing we're going to do, though, is we're going to look at a Python module. And I know that that many people would much rather spend more time with Python modules than in labs. And this week we'll be doing both. We'll be starting in labs. We will be moving to just Python code in modules. And there is a Python module called accounts, and it contains a ton of code for managing your account, where you can buy and sell shares, and where you have a balance, and where you can do things like calculate profit and loss, list transactions, and where you can do things like buy shares and sell shares based on a market price. Looks like some interesting business logic and hopefully you remember it seems familiar to you because this is, of course, code generated by our agent team in week three, our engineering team in Crewe. I created this code right here, and you can tell because it's got sort of like comments and type hints. And you probably know that I'm a bit hacky with this stuff, and that I only write comments and type hints when I'm when I'm forced. But here the our agent crew did a wonderful job. Um, so it's cool that we are taking the code here that is being used by our agents. Now, I've made a slight change to it that I've updated it so that it does save accounts in a database, and I've separated that out into a separate module called database. And this is just simply using SQLite. So this is very vanilla. But it allows you to read and write accounts as JSON objects. And that I've hooked up to accounts.py. But apart from making tiny changes like that, it is basically untouched code written by our engineering team of agents in week three. All right, so back to our lab, uh, for, for, uh, today, uh, I see it says here week two, day two. I think we'll change that right away. That seems better. Okay, so let's get started. We'll do some imports. We are now going to import this Python module account okay. So now I can call account dot get. And if I pass in a name it gets the account with that id here I am. This is my account. Apparently I have a balance of $9,400. I have three Amazon shares and I have some transactions. This is the coming back from the code written by our agents and I can call buy shares. I can buy three more shares of Amazon. And I have to give a reason. And we'll just rewind the clocks a bit and imagine this is a few years back and I'll say, I'm going to buy three shares of Amazon because this bookstore website looks promising. So there we go. Uh, yeah. If only I'd had that foresight. Uh, okay. So I bought three, and now I have six shares of Amazon in there. And I can call account Report to get a report about it. Uh, and all sorts of information, um, about it. And I can also call account list transactions to see those transactions right there. So this is just showing that we've got some code. It was written by our agents. And we can operate it to do things like buying shares and listing transactions. Now it's MCP server time. So writing an MCP server is pretty easy. It's just boilerplate code that you use to wrap code you've already got into an MCP server and you use some libraries provided by anthropic. And let's have a look. I've created one, a Python module called Accountsservice. Let's see what happens. So it begins by importing an anthropic class called fast MCP. And we also import our business logic account. And we then create an MCP server by saying fast MCP account server that is creating a new fast MCP server with that name. What we then have is a number of functions listed here which are decorated with at MCP tool. And for each of them we have a function like Getbalance. We describe that function. We give information about it here using standard docstrings. And then we actually do that function. And in our case we're just delegating to our business logic. But the idea is that this server will be spawned when we launch the MCP server. This will be launched, and these tools will all be tools that will be available. And they will work simply by calling the business logic that we've imported right here. So we have Getbalance, get holdings, buy shares, sell shares, change strategy if we want to change the strategy associated with the portfolio. And then I've also got some resources here. As I say, resources are not as common as tools, but I do want to show you how they work. And here we have the ability to access a resource, the name of an account will just return its report, and the strategy of an account will return its strategy. And it gets like a sort of a fake URL, a URI like this, where you can describe what resource you want to provide. If something requests this resource, and then down at the very bottom, we have the final piece here, which is that when this script, this Python script is run, what it should actually do is call the run function on MCP and say that my transport mechanism is the usual stdio. And by because we've done that, when this Python script is run, it will launch that MCP server, it will import our business logic, and it will be ready to handle any of these tools. And as you can see, there's not much to this at all. It's not super simple, but it's not hard at all to write your own MCP server that wraps your business logic and launches a server like that, and now we just have to try and put it to use.
------
Video 115. Week 6 - Day 2 - Creating Client Code to Use Your MCP Server
------
So here we are back in the lab. And what we're looking at here are parameters being set for our very own MCP server that we just wrote. So the command is you've run because that's exactly what we would type at a command line to run this module. You've run account server dot Pi. Just run the account server.py that I just showed you. And we know that when that's called it's going to create an MCP server, a fast MCP. It's going to call run. And that we've got some functions decorated as MCP tools. All right. So then we use the open AI agents SDK context manager with MCP Server Studio. And we pass in those parameters. Remember this timeout. And we then are going to call server list tools. So just by running this what's going to happen. It's going to create an MCP client. It's then going to spawn our MCP server by carrying out this instruction right here. And it's then going to ask it, what tools do you offer us? And we'll print them out. And we're hoping to see the things that we decorated. Uh, let's see if this works. Can it be that simple? Off it goes. Took 1.4 seconds. Let's just print that. And there we go. These are the functions that we just decorated. Get balance, get holdings. Buy shares. Sell shares. Change strategy. It's available. Wow. Um, so as you can see, it's not super easy, but it's also it's pretty easy. Okay, let's try and put this to action. Let's have some instructions. You're able to manage an account for a client and answer questions about the account. My name is Ed. My account is under the name Ed. What is my balance and my holdings? So it's going to need to make use of these tools. And we'll give it the latest model. Why not. Uh, let's take that. And now this again is the same code as before. We use the context manager with MCP Server Studio. We pass in our parameters. Let's put in that timeout client session timeout is 30. Um, and uh, we will pass in our instructions, our model, and we will then run a run and display the output. We're hoping that it's going to spawn the server, call the tools and be able to tell me about my six Amazon shares, uh, and the balance and everything like that. So enter your current cash balance is 8000. Your holdings includes six shares of Amazon. If you need any further details then let me know. So that is us successfully calling our very own MCP server that then calls our business logic. And now it's time to show you what it's like to write an MCP client. But I should first explain that it's not a common task to write an MCP client. You don't really need to do it anymore. When I when I first started working on this lab, the way that you worked with OpenAI agents SDK, they didn't sort of natively support MCP. You had to write your own client and then provide the tools into OpenAI agents SDK. And it was the day that I finished this project and checked it into GitHub and did the push that same day, about about 2 or 3 hours later, they released an update to OpenAI SDK that simplified everything and meant that all my code was useless. And you can, you can, you can look at the git timestamps if you don't believe me. It was crazy. I was I was infuriated, but it is a great thing. And basically it means that just with this, this context manager construct, you automatically get, uh, OpenAI SDK creating the client for you. But anyway, it's a good exercise to show you what it's like to make a client. And also this this construct works for tools. But if you want to use resources, I think you still need to write a client. And it's not so common. But but but we'll do it anyway and you'll see how it works. Okay. So this is in a Python module called Accounts clients. And this this is where the magic happens. That is no longer needed really. Uh. So this is this is a this is a an MCB client for use with my accounts, MCP server. So at the top here I specify these are the parameters that will be used to launch the MCP server. Now this could be something configurable. You could make a sort of generic MCP client that takes this as configuration and then and then spawns it. But for now this is just a fixed client for our accounts MCP server. And so this is an accounts MCP client. And so there are a few things we need to be able to do. We need to be able to list the tools. And so this first function here is an example of the function that lists tools. And we're basically using a bunch of anthropic code here for our client. And you can see that there's some context managers that we manage a session. We initialize the session. And then we call list tools on the session and return the tools. And so it's just sort of plumbing stuff that that you need to know about. If you want to write something that will contact your server and list the tools. This second one is actually calling a tool. This is how you go about reading a one of the resources, and this is reading the other resources. You can see I specify the resource name right there. Um, and then finally the types. The way that MCP returns tools is very similar to the JSON, the standard JSON that you use when you call a tool, as we did in week one. But it's not identical. There is some couple of slight differences. So if you're writing your own MCP client, you also have to know how to map between the MCP description of a tool and the kind of JSON description of a tool that's used generally when you're calling LMS. And that is what this this function here does that I painstakingly wrote. But all of this comes for free packaged in OpenAI agents SDK. So you don't need to do all of this, but you could look through it should you be interested and should you want to understand what does it mean to have your own MCP client? Okay, so now it remains for us to actually run the MCP client that we just created. So I import from that module. We just looked at some of the functions we just saw. And the first function I call is the list account tools function, which is the one that's going to spawn an MCP server and ask it for its tools that will come back as MCP tools. I'm then going to call the other function I showed you, where I take the MCP tools, and I reconstitute them as open AI tools. I turn them into these function tools, just as if I'd had a function and decorated it with app function tools. Same thing. And then I'm going to print that. So I'll print these two separately so you can see the results side by side. So this first line here, this is the actual MCP tools themselves with their description, the arguments of all the tools. And you can take a look at them should you wish. And underneath it is the same thing but reconstituted as a function tool, which is the kind of object that needs to be passed in to OpenAI. And indeed, that's what I'm going to do right here. So this is the way you had to do it before OpenAI built this, so that you can now you can just pass in the MCP servers. You don't have to build a client and do all of this stuff. But before that, in fact, very recently this was the way to do it. You'd actually have to pass in the tools themselves. So we've used our MCP client to find the tools and then we're passing them in here. So as I say, you just follow along for interest to see what this is doing. But it's not like you'd need to do this yourself. So as always, I've got a trace, and then I'll pass in my instructions and I will pass in the tools that when these tools are called, it's actually going to call the MCP client. That's going to call the MCP server that's going to actually run our business logic. Okay. So now we're going to run our MCP client like this. So we're passing in the tools we are calling the model and it has now responded. I asked what's my balance? And it's responded with the balance. And so just as a recap, what's going on there, we provided it with some tools right here. Those tools are in fact wrappers around MCP tools, which actually created an MCP client which launched our MCP server and which ran our function through the business logic on the MCP server and got back the results to the LM. So quite a lot was going on there behind the scenes with this. And as I say, you really shouldn't need to do this yourself because open AI agents SDK does it all for you. But it's useful to see how the plumbing works behind the scenes. Now here's another example of using the MCP client. This time we're using this read accounts resource client function that I wrote. And I think that you do need to do it this way. If you want to use resources, then that doesn't come for free with the OpenAI agents SDK package. So this is an example of reading the resource called editor. So let's see what happens if we run that. Let's run it through our client, through our server. And that's what comes back. This is a description of my account. And just to show you, I could also, of course, just import the business logic directly. And this is ending up just calling this function report. So this should give exactly the same answer. You could see the same stuff. So essentially what we've done is we've taken this piece of business logic. We've wrapped it in an MCP server to be available at a certain resource. We've written an MCP client that allows you to expose that resource. And that is what this function is. And so you can call this MCP client to get this. You could also just call the business logic directly. Um, and so why would you want to do this. Again it's if you wanted to share this resource with other people, it gives everyone a simple, streamlined way to access your resources rather than having to understand how your business logic works. Okay, that wraps up creating MCP servers and MCP clients with most of the emphasis on the servers. And I would now like to give you an exercise to go and create your own. So one super simple one to do would be to write an MCP server that can tell you the current date, give the current date, and you can expose it as a tool that you can equip OpenAI agent with so that it can find out the current date. And it can make sure that what it's the the content and the questions it's answering, it's doing so mindful of what the current date is. Um, and uh, yeah. Then then as a harder exercise, you could not only build that MCP server, but also build an MCP client to accompany it. Taking a look at exactly how I just did that for the accounts client compared with the accounts server, same approach. And then you could actually write something that looks like a native call to OpenAI, as we did in week one, like a simple native call with a tool as a JSON. And then you're getting really bare metal and you're seeing how you can call an MLM and have an MCP client and an MCP server where you're writing all the nuts and bolts. The only real use of that is to give you some direct exposure to it. So only do that if you're if you're interested. You won't have to do that day to day. But but it's a it's an interesting exercise. It's also worth pointing out that actually answering the current date isn't a great tool isn't super useful in the real world, because if you need to tell your agent about the current date, it's better just to state it in the prompt so that it always has it available so that the model doesn't have to go through the extra work of knowing to call your tool to collect the date. So if you feel like it, try make some more interesting tools. Have a shot at making a calculator that can do some calculation operation on two inputs or something like that. So. So try and make some interesting tool that appeals to you, and then write an MCP server and perhaps also explore writing an MCP client should you wish and enjoy that. And we will then go over to wrap up for today.
------
Video 116. Week 6 - Day 2 - Wrap-Up: Capabilities of Your Custom MCP Server
------
Well. I hope you enjoyed our adventure into the plumbing's and internals of MCP servers and clients. Next time we are going to now do what MCP is famous for, we are going to explore tons of MCP servers out there and just have fun equipping our agents with these new capabilities. I will see you then.
------
Video 117. Week 6 - Day 3 - Exploring Types of MCP Servers and Agent Memory
------
And welcome to week six, day three, and I've been looking forward to today for a long time, because this is the day when we go crazy exploring what MCP has to offer first. As usual, a quick recap of the core concepts behind MCP. Looking at the architecture diagram that we've got here, we're going to be looking at the three different configurations of MCP servers. First of all, and the simplest of all is when you use an MCP server that's simply is created, runs on your local computer, only, uses stuff on your computer, and is something that you're using right there, just like the accounts MCP server that we just used, which was actually one that we hand wrote ourselves, and also one like the local file system. Okay. And then secondly, and this is perhaps the most common of all, we're going to look at MCP servers that are run locally on your computer, but make remote calls to APIs that take advantage of stuff that can be available online. And that's number two on this diagram. And that is of course super common. And then thirdly, and we're not going to do this too much. We're just going to have a look at what it means to have a managed MCP server or a hosted MCP server, which is running remotely, as I say, not a common architecture, but we'll just look at it and see some examples and understand it. And it's worth pointing out that even numbers one and two, while it's possible to write your own most of the time when we're talking about MCP, these are MCP servers that have been made available online, and that through commands like like you are downloading it from an online public place and you are then running it locally on your box. So it's running on your box, but it's still something that's been shared online, made available for everyone to use. All right, that's enough. Intro. Let's go to the lab. Here we are back in cursor and we're going into the week six. Of course. Which other week would we want to be in and into the third lab, and we're going to be experimenting with a bunch more MCP servers. And yes, it's warning me I can actually take this out because we're not going to be using an MCP server. SS uh, okay. So the first type of MCP server that I mentioned is one that you run locally and everything it does is local. Again, remembering we have downloaded it from, from a public repo from somewhere public. But we're going to run it. The server will be applied locally and we're going to pick another JavaScript one, a node one. And I want to pick this one, which is great. It is a way to give our agent memory and to give it knowledge. Graph based memory, a special kind of memory that understands about entities and observations about those entities and relationships between those entities. It's worth memory is a is a hot topic, and people often ask me questions about memory, and they think of memory like it's kind of one construct, perhaps because Lang Chain thinks of it that way, but like there is one memory. But in this MCP age, we think of memory just as being another set of tools that we can equip our LM with. And in fact, it could have many different types of tools relating to memory. This in particular is giving it the ability to store information about things, observations and relationships. We could give it this and we could give it other kinds of context. All of this ends up just being information that it can request by tools, or that becomes available in its prompts. And so they're just different techniques to give the LM more context. Anyway, so these are the parameters. Remember you always start by specifying the parameters of your MCP server. And in this case it's mpc's because we're running a node version. And it's uh, we're going to be running this version called lib SQL, which is a version, a memory that stores to a SQLite database. I've actually I've given a link here to to one that this is closely based off, which is one that, uh, gives you access to a JSON file. Um, but doesn't actually store it to SQL. And I found that this is a little bit less stable. This is a better version of it that that I prefer. It also allows you to specify a directory and a name for your memory. And this allows you to have different memory stores for different agents. So I find that quite handy. So I'm saying that looking looking somewhere called memory and look for database called Ed. And I believe it might already exist. It does. So why don't we just delete this so that it doesn't have any memory to start with? We'll move that to trash starting from nothing. So if you have Ed in in a folder there, then delete it. If you don't have a folder then then create one called memory. All right. So first of all let's just find out what tools do we have uh associated with this. We have tools create entities search nodes read graph create Relations, delete entity and delete relation. So we're allowing it to build this kind of connectivity between the things it wants to remember. All right let's see this in action. Okay. So now we have instructions. You use your entity tools as persistent memory to store and recall information about your conversations. My name is Edgar. I'm an LLM engineer. I'm teaching a course about agents, including the incredible MCP protocol. And I say something about MCP and I give it a model and let's see how this gets on. So as before, this is hopefully a pattern that you're quite familiar with. We use this context manager with MCP Server Studio, which we know is going to create the MCP client. We pass in the parameters. We give this timeout of 30s. And then as usual, we create our agent with instructions with the model and now with the MCP server. And now we call runner run with the agent and our request. And let's see what happens. Armed with these tools is it able to take this piece of information about me and do something with it? So it says, nice to meet you, editor. It's great to know you're teaching a course about AI agents and amqp protocol, blah blah blah, blah blah. And if you look over on the left, it has created a file editor. So now for the follow up question. We're coming back. We're creating an agent. We're giving it instructions. We're giving it the same asking for the same model, and we're giving it the same MCP server with access to the memory. And we're saying, my name is Ed. What do you know about me? And we'll see what this MCP server is able to do. I know that you're ed an LM engineer. You're teaching a course about AI agents, and in this course, you're teaching about the MCP protocol. So there you go. It's a quick, simple example, but it shows you how we were so easily able to equip our agent with a memory that's able to handle relationships between things. And so this is just a great starting example, and we'll be using this memory in the future as well. And it could be handy for any of your projects too. And of course, it's always a good idea to check the trace, which we should do. Come on in and conversation with our agent is right here and you will see that it it called the search nodes. It looked it did a query for Ed and it got back here JSON structure that reflects the entity type, the observations and and so on. And so you can look through this and get a sense for how the memory works. And you can use this to try and build up some more sophisticated memory about the different, uh, parts of your conversation that you want your agent to remember. Okay. Now we're going to go on to another MCP tool, and this time it's going to be the second type, which is the type that runs locally but uses the internet, which we've already done with, with, with fetch and with playwright. But but let's do another and and a handy one.
------
Video 118. Week 6 - Day 3 - Brave Search API: MCP Server Calling the Web
------
So there's been many times in the last few weeks that we have used tools that do internet searches. You probably remember the hosted tool with OpenAI that we used right back in week two, which was quite expensive. Was it two and a half cents for every single web search? And we've used, I think, Tavileh and we've used, uh, Serpa. Well, we're going to do another one. So sorry to make you set up another key, but this one's a really great one. It's called Brave Search, and it's a company that specializes in an API driven search, including for use with AI. And it's free again, at least I think you get 2000 free searches a month. Uh, so it's very generous. And I have set it up. I've actually got a paid account because I've been using it a lot, but, uh, you very much can stay on the free version. I've got a link here for where you go to set up your account. If I go up here, uh, it brings up the brave search. You sign up, you go through and get an API key super easily without needing any credit card or anything like that. So once you've done that, you come back, you take your brave API key and you put it in your EMV file as usual, and then you'll have to rerun load MV up at the top. Okay. So then what we do is we collect our brave API key. And this time you'll notice that when we specify the params for our MCP server, we're passing in these environment variables, these settings as well. So that's a new twist for this one so that we can tell brave about our API key. And it's another node another JavaScript MCP server. So we run it this way. And in fact this is one of Anthropic's out of the box reference implementations. So this is part of Anthropic's offerings, the brave search. And so again even though we're running online searches, the MCP server is going to be running on my box. So the code for this is is remote is online. We are downloading that code and running it locally using npm. So it starts online, we bring it locally, we run it locally. And then of course, what that code actually does is it makes a call to a web service offered by the company brave, and it passes in our key to that service. So that's why this is architecture two MCP server running locally calling out to the cloud. And I'm probably belaboring this. You're probably like, yeah I get it I get it enough. Okay. So MCP server studio again is how we do this. We pass in the parameters. Let's just see what tools we get with this. Um, we get a brave web search tool which performs a web search with the API. Um, and then a local search searches for local businesses. I think I only get that because I'm paying the paid plan. I don't think that comes with the free plan, actually. Um, so again, the key difference with something like fetch is that with fetch, we're running a browser locally and we're just going to a web address. We're brave. We're making a web call to brave, to ask it to run like a Google search. But it's not using Google. It's using its own search engine and return the results of that query. Um, and so, uh, we're going to ask a question. You're able to search the web for information, research the latest news on the Amazon stock price and give it the current date. And then we run the server right here. Okay. Here we go. Running that you'll see. As usual we're passing in the MCP server. We've got the timeout set and we're calling run and run and back. We get of course, the results of running this web search with the current price of Amazon apparently 217 and more information about it. And as always, we should go in and check out the trace. Let's have a look here and make sure that we're happy. Yep, it's calling a brave web search. The query was Amazon Stock Price News May 2025. And it got back these results. And so it seems to be working nicely end to end. That is another. That is an example of type two of the calling out to the web. And so now to talk about the type three of the MCP servers, the ones where the server itself, the MCP server, is running remotely. And you must connect to it using the SSC approach. And now here's the thing. As I say, it's not that common. And it's also quite flaky because there's no guarantees that the people that you're connecting with are going to keep their server running. I used to have an example in here of an MCP server that I found, a hosted MCP server that you could connect to remotely, but it went down and now you can't connect to it. And that's why I had to take out that code at the top. So it's no longer running. So that example didn't work. And so I tried to look for some other examples. And the other examples that I found are all for paid business services where you are already paying for something, and it tends to be from sort of professional business plans. So let me show you that, that the anthropic docs have a section on remote MCP servers. That's what we're talking about here. And it says several companies have deployed remote servers that developers can connect to. Um, and so here are some examples of them. And you'll see why I'm not able to demo one of these because I'd have to be a paying client. So asana, if you are a client of asana, asana like project management tool, then you could interact with asana workspace by by using this remote, uh, MCP server at MCP. Uh, and then similarly for these others like intercom, like, uh, PayPal, if you use not not just PayPal for your personal, uh, PayPal, I have one of them, but I don't have commerce capabilities. I don't I'm not I'm not a seller. Um, and and the others at Square and Waikato and Zapier. I was going to try and set something up to show it, but you have to have a paid account. So it's it's it's honestly not common to be seeing this. It's only in these cases of, like, paid, uh, enterprise accounts where you don't want to run the MCP server yourself. You want to connect to it running on the cloud. And as I say, I really I haven't seen much traction with this in the community. It's not like this is something that you come across often. Um, and for many of these, uh, for many of those commercial examples, you can run an MCP server locally as well, which is what people typically do. Cloudflare, which was actually on that list, has some tools for you to create and run your own remote servers. If you're a Cloudflare Cloudflare customer and you use Cloudflare for your own website, your own deployments, uh, they have some really interesting sheets here which allow you to deploy, uh, a remote server that other people could connect to, and they actually make it really quite easy. They walk you through the code here and how you can check in their, uh, screens that it's running. And then they give you, of course, the way that you would then connect to it from, say, your desktop or indeed from your OpenAI agents SDK. Um, and this is then where you can add authentication, which is one of the hot new areas of MCP. If you are going to provide a hosted tool, then you would want to have a way that people can authenticate with it so that they can demonstrate that they are who they say they are. Um, but as I say one more time, then I'll shut up because you're fed up of me saying this. But but it's not a common way of doing things. And indeed, uh, if you're if you're running something like the brave search that we just did, it would be more common for you to set up an API key with something like brave and then pass in that API key when you call your local MCP server. That's the more common configuration. And of course, this might change any day that maybe, maybe managed hosted MCP servers will take off big time. And and I'll be eating my words. But but as of right now, uh, it's not terribly common, but you certainly could follow the Cloudflare instructions and set up your own should you wish. So that wraps up the third type. And we're now going to go back to the second type for a really cool example.
------
Video 119. Week 6 - Day 3 - Integrating Polygon API for Stock Market Data
------
So the main project for this week is to set up a trading floor, have autonomous agents that can buy and sell equities, and they're not going to really buy and sell equities. They're going to be using the simulated account management that we already built, but they will be using real market data. And there's a bunch of different MCP servers that can help provide you with market data. And after playing with a bunch of them, my favorite one is the one provided by the company, polygon. That's a very well known professional provider of financial markets. Uh, and uh, yeah, I'll bring you to their website. Here they are. It's, uh, they're super well known and, uh, they one of the big benefits that they have is that they have both a free plan and a paid plan. So I recommend you stick with the free plan. But if you're a sucker for this stuff like I am, then for sure you can. You can spend a little. It's somewhere between 20 and 30 bucks a month, uh, to get more accurate market data. So with the free plan, you always get data as of the previous day's business, close stock prices as of yesterday's close with the paid plan that I'm on, you get them on a 15 minute delay, but unlimited API use. And if you want to pay a lot more, you can get real time market data, but there's no need for that unless you're already a trader. So this is polygon. Uh, please come in and sign up. And once you're signed, signed in, select the keys button. In the left hand navigation. You press a blue new key button. And then you take that key and you put it in your EMV file as polygon API key. And uh, the, uh, here we go. Just run this. And the fact that it didn't say polygon API key is not set means that it was set. Um, and then you can, for example, directly use the polygon client. And they've got great docs on their websites and it's very easy to use. So this previous close, uh, AG for Apple's stock price, for example, will tell me that the previous close for Apple was $195.27. And that is how you simply use polygon code directly. Okay, so I have written a little Python module called market.py that wraps some of these calls to polygon. And I've done something a little bit sneaky. So the polygon API if you're on the free plan is what they call rate limited, which means you're only allowed to make that API call. I think it's five times a minute. It's quite, quite mean. Uh, and uh, so you would only be able to call for a share price that five times, but it turns out that that it also only counts it as one call. If you make a call and ask for all the share prices in the market, a snapshot of every share price. And so I've done something that if you ask for a share price like Apple and you're on the free plan, it will call for all share prices. It will use that one call and then it will cache that in memory and the subsequent times you call this, it will just return from the cache. Let me show you that for a second. If we look in Market Pi, you'll see here that there's this get share price. Um, I checked you got a polygon key and then I call this get share price polygon. And I see if you're on the paid plan or not if you're on the free plan, I call this function. If you're following me and this function is basically going to end up getting the full market for the prior business day, and it will cache that. So if this has been called before with the same date, it will just return the same full market. So it's just a nice way of making sure that we can always get that share price, and we won't hit the rate limits. Let me show you that if we get the share price for Apple, it's 195. And now I can put that in a tight loop and get it 1000 times, and we simply get 195. And I assure you that if this weren't being plucked from the cache, then we would have problems with that. We would hit the rate limit immediately. So that's just something to to understand. Just something that I've put in there. Okay. And I've also made this into an MCP server as well. Another little local MCP server called market underscore server. So if you look at market server you will see that this is again a very simple MCP server. It just the same as before. You remember, you create the MCP and then you use the MCP tool decorator. And then here is the function lookup share price. And that just calls get share price. And this is the way that you actually launch the server. So it's really nice to see such an such a simple MCP server here that created that means that we can now call that as uh, we can first of all, look at the tools that it gives us. And this should exactly give us the lookup share price. And now we can ask an agent to tell us the share price of Apple. So we'll say you answer questions about the stock market. What's the share price of Apple. Let's upgrade this to be the latest model. And then just as before we use the context manager we pass in that MCP server. This stuff is is simple to you. Now you're an expert at this. We call our MCP server. And sure enough it all worked great. It called the MCP tool and it told us the share price of Apple.
------
Video 120. Week 6 - Day 3 - Advanced Market Tools Using Paid Polygon Plan
------
But that's not the interesting part of using polygon. The interesting part is if you have a paid plan and you want to use their MCP server with all of the capabilities. And so I'm going to show this to you just so you get a sense for it and totally optional. But if you wish, you could sign up even just for the first month, to experience this for yourself. So here are the parameters that we're providing for our MCP tool. It is UVC, so it's a Python based MCP server. And you can see interestingly that the way it works is that you can you can provide a link to the GitHub repo which contains this MCP server. And you can use that. So we're calling MCP polygon from this repo. So it doesn't have to be something that's like pip installable. It can be something that is just straight from a repo. And obviously if you do something like this then you need to do your research. You need to go and check that this genuinely is the official GitHub repo for polygon, the company behind polygon IO, and make sure you're comfortable with the community traction, the number of stars, the kind of support it has, and so on. Just as you would do if you were going to clone somebody else's repo. The same kind of due diligence. Exactly. And so I have done that, of course. And I'm satisfied with this myself. But but you should too. And then I'm passing in my polygon key. And then I'm going to ask, what tools do you provide me this MCC server. So we'll run this. And wowza there's a lot of them. So there's a bunch of different tools. There's a lot that the model is going to be allowed to do. It includes, uh, doing things like getting the last trade. It has crypto data in here, as well as ethics data, getting market status, tickers, dividends, conditions, financials. There's there's a there's a lot to get here. Uh, and so this this is something which which equips our agent with lots of capabilities to analyse financial markets directly. Now, you can give all of these tools to your agent, even if you're on the free plan. But if you do that, most of these will respond and say this tool isn't available for people on the free plan. So it's a bit of a bore. Uh, what you have to do is, uh, is is either just only provide the one tool, as I did for the free plan example, uh, or sign up for the paid plan. Okay. So let's try these out. So, um, as I say, because if you're on the free plan, we have to be specific that it should only use the get snapshot ticker tool. And so I will run this and have it hopefully use the right tool. So so we're now equipping the agent with all of those tools that we just saw. And uh oh. And it hasn't used the right tool. That's super interesting. Actually let me also I'm going to upgrade the model while I'm doing it, even though I said get snapshot ticker. Let's try it a second time. It's always good to see her agents being unpredictable. Um, let's see whether it gets it right the second time and uses the right ticker. It does. Uh, so I'm tempted to, to go and rerecord this, but I won't, because I think it's important to see that with a genetic AI. You do get this, this sometimes ill behavior when it doesn't correctly follow the prompts, and you need to be able to account for that in what you do. And also, I don't know if this is what fixed it, but upgrading to a more recent model, uh, did did appear in this case to make it perform better, but now it did use the right tool, and it got the latest share price at $195. Okay, so if you do have a paid plan then you can now add this to your M file. You just have to in your env file say polygon plan equals paid. That makes sure that I switch it to use the right tools. And if you do decide to go all the way and have the premium real time plan, you can set this to real time. And I'll be sure to use all of the real time APIs. But I have got just this paid plan, so I'm going to go and update my M file right now. Okay, so now if I now run this cell, it should confirm that I'm on the right plan. Let's see. Yep. You've chosen to subscribe to the paid polygon plan. And so we'll be looking at prices on a 15 minute delay. And that for the time being is uh, is the, the uh, our foray into MCP servers. Um, but there are so many more and that's where I will get to with the exercises. Um, so I would suggest that you go on to the marketplaces and look for tools which interest you and experiment with them right here, try and use. I say here all three approaches. You'll probably find, like me, that there actually aren't easily available approaches with remote MCP servers, so you might not want to go there unless you are already paying user of something like asana. Um, but assuming that you're not, then then go with those first two approaches. Find examples of MCP servers that will run locally and only stay locally. Do things on your local computer. Find examples that call the web and find examples of MCP servers that are Python based and those that are JavaScript based, and try them all out. Experiment with them and have fun giving capabilities to your agents and seeing them taking advantage of them. Well, hopefully you took me up on that. You did some exercises and you've added more MCP servers. And that's a wrap on day three of week six. And that means that we're about to head into the capstone project of building a trading floor. And I can't wait to show you this. It's such a cool project. I'll see you next time.
------
Video 121. Week 6 - Day 4 - Whats Next: Launching Our Agent Trading Floor
------
It feels like just a second ago that we were starting this thing, and somehow we're already on week six, day four. We are heading into the final, final section with the capstone project, and let's make sure that we end this thing in style. This is going to be a great project. Welcome. Welcome to the Capstone project. Welcome to Autonomous Trading. We are going to build agents that can make their own decisions about analyzing financial markets, and that can make trades in a synthetic account that we have created ourselves in week three with our other team of agents. Okay, so autonomous traders, then let me tell you a few things about the project that I have in store for you. So first of all, it's something that is commercial. One of the things that I've noticed about a lot of the modern Agentic AI projects is that they're quite technical in nature, because they're so open to possibilities that one tends to build things like teams of coders and stuff like that. I really wanted our capstone project to be something that showed how you could apply it to a true commercial problem, like analyzing and understanding financial markets. And that's why I've gone in that direction. It's going to involve having five different MCP servers with tools and resources, and we will count them up. And there's going to be a lot of them. Look forward to it. There's going to be interactions between agents. And there's also going to be autonomy in this. We're going to allow our agents to choose their own adventure. They're going to be given the freedom to make various decisions themselves. And then most importantly, something that I may say once or twice, please don't use this for actual trading decisions. Uh, this is something which, uh, yes. I don't don't want to, uh, get in trouble if you go and put your all of your life savings into this. This is, of course, an experimental project and nothing more than that. Having said that, of course, if you make a massive fortune on this and you're off sailing your yacht, then I do expect an invitation. Uh, but no, do not, do not, do not use it. Do not get your yacht this way. Get your yacht by building autonomous agentic solutions for people. Not by having your traders run amok with financial markets. Anyway, with that introduction, we're going to spend most of our time sleeves rolled up in code. And let's get started now. Okay, here we go. Back in cursor. We're going into the folder for six. And we're going into lab for our Autonomous Traders project and Equity Trading Simulation to illustrate autonomous agents powered by tools and resources from MCP servers. Okay. So we are going to create a simulation. We are going to have four different traders eventually. And one researcher actually each trader will have their own researcher powered by a bunch of MCP servers. We're going to have our homemade accounts MCP server that you remember that we did in the second day. We will use fetch that we used in the first day we'll use memory. We'll use the SQL based relationship memory that we looked at. We'll use the brave search and we'll use the financial data courtesy of polygon AI. So what we're going to go through now is, is build this in the lab. And then we're going to look at the code, the module traders Pi. That will take the same thing pulled together. And it shows you a practice which I like to do, which is to work initially in the lab while you experiment. And it ties to a point that I know I make a million times that I'll make again at the end, which is how important it is to approach agent projects with a data scientists hat on. First and foremost, be looking to experiment and understand what you're doing. Uh, not just jumping straight into engineering and building building. It's important to start by investigating and understanding what you're doing. One other thing that's important to do is not use this for trading decisions. If I haven't already mentioned that, uh, so one more warning for that. Okay. Let's get going. Let's do our imports and set our EMV. And let's look at the polygon situation. Do we have an API key. And have you said whether or not we are in a paid plan? We are in paid plan. For me, you may be false there and we're not using the real time polygon. Okay. What we're going to be doing in the next few cells is, is collecting together the different MCP server parameters for all of the MCP servers that we're going to equip our model with. So first of all, I've got a little decision here. If we're using any of the paid polygon plans then we are going to directly use polygons MCP server with its whole set of different tools. If not, we're just going to use that tiny MCP server that I handcrafted in market server. And that's because if you're using the free plan, we don't want to overwhelm the model with lots of different tools. And there's another reason for it too, which is that here I'm using that trick of caching the previous day's data so that we don't exceed our rate limits with polygon on the free plan. So that's why we've got the little decision there. And then we're also going to include in our parameters the accounts server. So this is our homegrown. MCP server to read and write from accounts. And then there's a new one push server. I wonder what that could possibly be. Let's just run this. And then we're going to take a quick look at push server. It's another homegrown MCP server. What could it possibly be doing. Let's have a look. Here it is. Push server. Push server of course is going to send a push notification. You know how I like the push notifications because it makes it feel so autonomous when it's like messaging you suddenly. And so we're going to have it pushing. We're going to arm it with an MCP tool. Um, and this is an example of a case where we're writing this MCP server and we're not delegating on to some business logic, we're just putting all the logic right here in this single Python module. It's an MCP server. It has a single tool. It's called push. And it takes a little pedantic object that we've set up here to say that it is a brief message to push, and that is the message that we give. So this is a clear example of how you can write a little tool and expose it as an MCP server. Now, as I said before, really there's absolutely nothing stopping you from just having this as a tool. In fact, that would be the better way of doing it. There's no point in having an MCP server like this when we're just using the tool ourselves locally, but we want to get into the practice of making MCP servers, so why not? Okay, so with that, we have now also included our push notification as well as the accounts and the market data as our trader MCP servers. So that's set up the MCP servers that our trader will use. But we'll have another agent called the researcher that's able to do market research. And we also want to arm that agent with tools as well. And for that one we will use the brave key and we'll give it fetch as well. We'll give it the ability to fetch web pages. Um, and so both of these two together are going to be some, the tools that we will equip our model with. All right. And now it's going to be time to put these to use.
------
Video 122. Week 6 - Day 4 - Viewing the User Interface for Trading Activity
------
Okay, so we've gathered up our parameters into these params lists. What we're now going to do is create MCP servers. We're going to instantiate MCP servers with each of those params and with the 32nd timeout. And so we do all of that. And we've just built a bunch of these MCP servers ready to go okay. So we're going to have two different agents that we're going to define. We're going to have a trader that's able to make trading decisions, and a researcher that does market research, and the trader will use that researcher. And you may remember from week two in OpenAI agents SDK that when you want to have that kind of collaboration where one agent uses another, the best way to do it is to have that other agent, the research agent, be like a tool, convert it into a tool so that that agent can just be used as a tool by the trader agent. And that's exactly what we're going to do now. So we start by defining our researcher agent. So here's the system prompt the instruction. You're a financial researcher, you search the web for interesting news, and then you carry out deeper research and respond with your findings. And we tell it the current date. And you remember I said, it's better to do this than to have a tool to look up the date, because you might as well always pass it on and not and not add extra complexity to the agent to force it to come back and run a tool. So we just provide the current date right in there, and then we pass in, of course, our MCP servers and that defines our researcher. And then you'll remember this construct. This is how we say we want to use this as a tool. We simply call as tool on that researcher. And we give the tool a name and a description. And that means that this agent will be available for use by other agents that want to be able to treat this like it's a tool. And that is a very common pattern with open AI agents SDK. Well, before we use the research agent as a tool, let's just try calling it directly to check it works. Uh, so we're going to ask the question, what's the latest news on Amazon? And we're going to, first of all, go through and connect with all of our MCP servers. Now you'll notice this is a bit different. I normally have like with MCP studio uh ads. And then I do it that way with a context manager. So why am I doing it differently this time? Because if you've got a bunch of these, we'd have to have lots of widths all nested, and that would be quite clunky. And so this is just another way of doing it. You should also clean up if you if you do this. But because we're just running in a Jupyter lab anyway, it doesn't really matter. So we're connecting to the server here. Uh, we're then getting our research agent and then we're calling runner run for the research agent passing in the agent. And the question and this is new. I'm also passing in something called Max turns in here. And that's because if I the default is ten, which means that that it can by default do up to ten sets of tool calls. But if we wanted to do deep research, we might want it to take longer than that. So we might want to give it up to 30. Possible maximum turns. So that's why I'm I'm setting it that way. And it's good to know that that's another another thing that you can control. And you can also of course have that be a smaller number if you don't want to let your agents go off and potentially get into a loop of overthinking about things. But 30 seems to have done the trick for us, and probably we didn't need anything like that. And we got back a bunch of information about Amazon. Let's go in and look at the trace to see what happened behind the scenes. So if we come in to the trace and we come into the researcher, you'll see that it did a brave search. It did a bunch of fetching of web pages. It did another brave search and some more fetching of web pages before responding with its answer. And you should do this and go back and have a look and see what it does and what kind of searching and fetching the agent is doing as part of its research into Amazon. Okay. So now let's look at our trader. So I'm going to start by giving our trader a strategy because that's something that we store in the account. And the reason that I give the trader a strategy which is stored is that I want traders to be able to change that strategy, should they wish. We're going to give each of our traders a unique strategy to set them going, but we want to give them some autonomy to choose to evolve their strategy if they want to. But for me, Ed's initial strategy, I'm going to be a day trader that aggressively buys and sells shares based on news and market conditions. And I'm going to call this this, uh, this reset, uh, function to get Ed off to a, to a good start. And that's that read use these resources to read my account and my strategy at this starting position. So here we go. My starting position is I have $10,000 ready to invest there. There is my description and I have empty transactions and nothing in my portfolio and everything is, uh, ready, ready for business. It's now time to create our trader agent that is going to take this persona and be able to make trades as a result. Okay, so this is our trader agent right here. It's called editor. And, uh, the account details, it's reading in a resource. And the strategy, it's reading a resource. What is this reading a resource. This is of course, calling the MCP client that we created ourselves a couple of days ago. It's calling an MCP client that then calls the MCP server that provides the resource by calling our business logic. So it's kind of cool. We're using this, uh, resource side of MCP. The fact that you don't just need to use MCP for tools, you can use it for resources as well. And so what does it mean to use MCP for resources? What do you do with these resources? Well, it's just text that you shove in the prompt. You just add it to the prompt to give your agent more context to be able to make its decisions. And that's exactly what we do here. So we call these two resources. And then we put together our system prompt. You're a trader that manages a portfolio of shares. Your name's ed your accounts under your name. You have access to tools to do your job. Your investment strategy is and then we shove in the strategy from from this this call here your account, your current holdings and balance is and we just shove in the account details right here. And then we tell it to make decisions based on its tools. So if we run that and we print the instructions, we'll see that when it prints out, we get things like the current holdings and balance included in our prompt. This is our resource. And you can see I'm just shoving JSON in there because Llms love JSON. It's going to be great with this. It'll make total sense to it. And so with that we can kick off our trader. So set this running while I speak. So we connect to each of the MCP servers. We then turn our research agent into a tool, and we get our research agent as a tool. We then create our trader agent. We give it its instructions. We pass in as tools. We pass in our researcher tool, which is a wrapper around the agent for MCP servers. We pass in the full set of MCP servers. We're using GPT for our mini in here. You might want to update that to GPT for one mini if you're looking at that now. And then we call runner Dot run and we pass in the trader and the prompt. And again I'm adding to max turns. I'm not going with the default of ten. I'm giving it up to 30 turns to really go to town and be requesting across things. And now typically this takes a minute. So I'm drawing out this explanation as long as I possibly can in the hope that it finishes before. And it may well observe that the markets are closed right now so it can't make trading decisions. It's possible that it will do that, or it might decide that it wants to make some trading decisions anyway, even though the markets are closed. But we will soon find out. Uh, so here we go. It's. No, it has decided that it has. We gave it the tool to do it, and so it's decided to do it. It's got a bunch of different summary actions, the results of the research and the trades that were executed, that it bought shares and sold shares. And that's the current portfolio status. And it's got some next steps at the bottom. So that is the result of our trader agent running using the research agent and being able to execute its tools, and also being armed with the resources that we included in the prompt. And I wouldn't be doing my job if I didn't say that. We have to go and look at the at the traces. You always have to go and check out the traces and make sure you're happy with what's going on. And it shows here that this was Ed, the trader agent going to the researcher, the researcher agent that we're using as a tool. You can see that about half the time is spent in research mode. It used the tool that green shows it's using a tool called researcher. That's that's then using our agent we're listing the MCP tools. And we're then doing a brave search. We're doing the fetches. This is like before and that's coming back. We have to press load more because it's such a long, uh, thing here. We're then getting some tickers. We're getting the last trades, we're getting some quotes, we're getting some close of market prices. It's doing plenty. It's getting the previous close prices. Uh, and then it ends up buying some shares and selling some shares just like it told us it did. If we click here we can see that it's it's bought Disney. And uh, it's also sold 25 shares of Disney. So it's not hugely intelligent of it to buy shares and sell so quickly. But I guess we did tell it that I'm an aggressive trader that likes to buy and sell. I mean, that's super fast to do it instantly like that, but I guess that is what it wants to do. So it bought 50 and then it sold, and then it bought 30 Tesla and it sold 25 of the Disney shares that it just bought. So there you have it. That is our trace that is showing our trader in action. It's showing the collaboration between two different agents. One of them is a tool and it's showing the use of the MCP servers. Well let's just quickly look at the results of the trading by reading the resource again using the MCP client. And you can see that I've got less in my cash balance, uh, and that I have my remaining 25 shares of Disney. And that is the, uh, the various details we, we have there about what's going on. All right. We're now going to look at some Python modules.
------
Video 123. Week 6 - Day 4 - How Trading Agents Operate and Make Decisions
------
Wait, did you spot that intentional little hiccup there? I was it was just after I stopped recording. I was thinking, and I'm like, hang on a second, hang on a second. We saw that that, uh, in the traces that it bought Tesla stock and it didn't report that it bought Tesla stock. And we didn't see that in our holdings at the end. So let's go back and see what happened there. So I went back into the trace and I took a look and I scrolled down load more. Uh, and uh uh, sure enough, if we get down to the bottom here, you will see that there were indeed these two buying shares. First of all, it bought 50 Disney shares. And then as I remember it, bought 30 Tesla shares. What's going on? Well, then I scrolled down and saw that it got an error back from this error executing tool. Insufficient funds to buy shares. And what's kind of brilliant about this is that that error message is an error message that was written by our crew agents in week three. We asked it one of our business requirements is that it should put something in there that prevents the person from buying more shares than they can afford. And so that error message. Insufficient funds to buy shares was written by those crew agents. That's what's been surfaced back here. And that is why those shares weren't bought. And that's why it didn't include that on its summary. And indeed why why, although it does mention that it should be monitored in the next steps. And that's why, of course, it isn't in the holdings at the bottom. So there's a great explanation. And it's everything is working perfectly. So we're now going to go and look at some Python modules, some Python code. And this really brings again this very important point that a great way to work with building a genetic framework. So building a genetic solutions to business problems is to start in the lab. Start in a notebook like this experimenting with prompts and different agent configurations. And don't move to Python modules until you've done your experiments. I do get a lot of people asking me, how am I going to go about building this agent solution to the following commercial problem. And they want to dive straight into code and building lots of agents, and they want to build a diagram that shows boxes with agents all talking to each other. And I say to them, look, it's super important to begin by experimenting where that data science hat start in the lab, start by experimenting with prompts, understand what are the capabilities that you can give an agent, that it can stay coherent and it can follow instructions. And how can you give the right balance between autonomy and coherence and these kinds of decisions? They come through practice, and it doesn't come from designing a big picture and then coding for a long time, and then kicking it off and then complaining when it doesn't do what you want it to do because it won't the first time if you do it that way. And that's why the right way to approach this is little by little, starting small in the lab. So that's what we've done here. We've done that. We've been in the lab, we've built something, and it's now time for us to turn this into code. And we're going to do that. And I'm going to show you, piece by piece, three different files MCP servers, templates and traders. So first of all MCP servers is where we define our MCP servers. So here we go. We're looking for. It's actually sorry. It's it says MCP servers. It's MCP params dot pi. Change that right now MCP params dot pi is where the MCP servers are specified. So here we go. We're in MCP params dot pi. Now you don't need to have your MCP parameters specified in a python module separate to your other code, but I think this is an organized, tidy way of doing it to keep things nicely. The right concerns in the right file. So in this file, I'm setting up my different MCP parameters that I'll use later when I'm creating the MCP servers. So first of all, for market data I choose to either use the official polygon MCP server or I use the little sneaky one that we made market servers that caches results and makes sure that you can use free polygon IO without exceeding your limits. So we pick one or the other depending on how your EMV is set up. Okay. And then the full set of MCP servers for our trader is the accounts server, our homegrown MCP server for accounts, the push server, another little homegrown server sending push notifications because you know I love them. And then the server for the market data. Okay. And now we've got a separate set of MCP servers for our researcher. There are three of them. First of all fetch the ability to fetch a page using playwright browser behind the scenes. Then we're using the brave search again, our free brave search API for web searches. And then this the return of the memory. We're using our memory. We're using the SQL based memory. And you can see I'm actually going to try and have a separate memory file for each trader based on its name. So the name will be passed in. And that's what we will use here as the, the name of the of the memory file, so that every trader, every researcher has a separate set of memory. Okay. That is the MCP params module. So the second module I wanted to show you is called templates. And here's the thing templates.py is just where I have put any function which is returning like a prompt instruction or a string or something like that. And I've done this to be organized so that I don't have tons of text coded within the body of my functionality. And again, this is just a nice, good practice. It's good to separate concerns this way. It means that if I need to edit my prompts, I've got one place to go and do it. I've got effectively my my prompt templates. Uh, in many ways, if you if you think of some of the more opinionated frameworks like Lang chain or like crew I for for an agent framework, they sort of force you to separate stuff like this? Uh, well, they don't force you. But there are ways around it. But but, uh, crew, for example, of course, has the YAML files, which, uh, specifically takes away takes to one side your text. But we're not we're not using those kinds of frameworks, but there's nothing stopping us from, uh, putting us through the discipline of putting all of our texts like this into a separate module. And that's why I've got it here. So I've got, uh, my researcher instructions, my research tool definitions, my trader instructions, and my prompts all here in these different functions. And you'll see I do things like inserting the current date in the prompts, which I said before was a was a very good practice, uh, to avoid having to equip it with the tool that it needs to call. Just put the date in the prompt, and you'll also see how I'm putting the strategy and the account details straight in there in the prompt. So that's templates.py that separates out our uh text into its own module.
------
Video 124. Week 6 - Day 4 - Portfolio Management with Four Autonomous Agents
------
Okay. And with this, it's time for us to go and have a look at traders Dot Pi, the module for defining our traders. And just before I do this, I want to mention that there's some fancy python I've got in here, which looks a bit muddling the first time you see it. One of the slightly hokey things about working with OpenAI agents SDK is that that you. It's good to use the context managers to wrap creating a server like like like I've got right here this async with and then the server studio passing in the parameters as a server. But that can start to look super ugly if you have a bunch of servers, which we do, we have many. When you have many, you'd have to have a separate with statement, a separate context manager on each line indented, and it gets a little bit out of control. And there is a Python technique, which is quite an advanced technique for not having to have a stack like this, but just being able to do it, sort of iterate over lots of context managers, lots of widths, and it looks like this. and with async code as well. It's a little bit a little bit muddling, but you can have uh with an async exit stack. And then you can do this enter context for each in a list. So this construct that you're seeing here, this code is just taking the MCP server params and iterating over that and effectively doing a width for each of those in turn. So look into this if you wish. It's a construct. I use it to make the code a bit neater, but don't let it put you off. You could equally well just do it the manual way. Okay, so with that little caveat, let's go and have a look at this. This is the module. It's just taking what we had in the lab and turning it into a Python module. There is some cool stuff at the top here that just allows it to switch different models. So we're not only relying on OpenAI, we can use deep seq, grok, Google, or we can use open router uh, open router for for Americans, we can use open Router to connect to any model of our choosing. And if you look in the guides directory, there's a whole guide that talks more about this use of different models. So based on which one you choose it will pick a different model provider. And other than that, this is basically exactly what was in the lab in the notebook, turned into a class, into a Python module. And just here is that funky stuff I was just talking about with the stacks. So, you know, if you're suspicious of this, then look into it a little bit more. Read up about this, this construct here and why I've done it this way. And if you dislike it, you can always manually have the with with with with with. Um, but it will be quite, quite nested. It will look a bit bit ugly, but it would be a bit simpler to do it that way. But other than that, everything here simply involves creating a trader agent and using a research agent as a tool. And it's the same code that we had a second ago, and it's all packaged into a nice class trader so that we can create an instance of trader giving it a name and an agent and a model name, and off it will go. One other thing about it that's kind of fun is that I did have it, uh, alternating between making trading decisions and making decisions to rebalance its own portfolio. So I've given it that ability to, uh, either trade or rebalance as it wishes. So you can see here that that, uh, if the do trade flag is set, then it will, it will run a trade message. Otherwise it will run a rebalance message, which you can look in the templates is a message to say you should rebalance your portfolio, uh, to be optimized. And at the bottom here in in run this is of course the, the business function. This is what kicks it all off. It kicks off a run and then it will uh switch that do trade flag. It will flip it the other way. So that is the trader module. It's one thing about this I haven't told you yet, which I'm going to tell you about tomorrow. Uh, but this is basically showing you all of the stuff, all of the good stuff that we wanted to build around, having an autonomous trader that alternates between trading and balancing its portfolio and uses a researcher agent to help it. And using a bunch of MCP tools. Okay, so now back in the lab, let's actually see what this module does. So we're going to import that, uh, trader, the class we were just looking at a second ago. We're going to create a new instance called editor of that trader. So that's now called Add. And because the name is add, it's going to look up an accounts and have access to that same account. The thing that just bought Disney shares and then sold them again. Some of them. Uh, and now we're going to call the trader Dot run. This is the business. This is the thing that's actually going to kick it off and run this whole process. Let me get on with it. So I kick that going and while it's running, we'll let it let it run for a little bit. So it gets gets started with its activity. We'll open up the trace. Let's see what's happening here. Here it is. And so it's it's off and running. It's listing tools. And it's thinking, uh, we'll see if we can just refresh it like that. Yep. It's doing a brave web search and it's now going to take its time over this. So rather than sitting here thinking of what to say for for a couple of minutes, that will take, I will be right back with you. Okay. Well that just ran and it completed. And the result it took, it took 48 seconds. It didn't take too long. And it did end up by complaining that one of the tools had an error. Uh, so we can go and check that out. Um, but otherwise it seemed to do its thing. So let's go and have a look at the trace. Uh, so we'll come in here. Here we go. So it did. Listing tools. It starts with the researcher. The researcher does a brave web search, and then it comes back. And now we're with the trader. It does a bunch of of tickers. It does lots of tickers. It does some buying and some shares. What's it bought. It bought Amazon um, and and more and Apple and Microsoft and uh that's great. Uh, and then at the end here it does some selling and some shares as well. Again, I guess because it's so aggressive. Uh, so um, looking to, to, uh, sell some shares and then it ends with a push notification. And this is where our error happened. Uh, there was, uh, it said that there was an error running the tool, which is interesting because that looks fine to me. And it's also interesting because I did actually get a push notification on my phone there. Just, uh, it does indeed have, uh, have exactly the same, uh, message. So I got the push notification. Uh, it came through. It worked fine. I'm not sure why the error appeared here. It may be because that method didn't return anything, so I just. I just changed that just in case. But anyways, I'd say that that is a wholehearted success. Uh, agent just worked fine. Uh, and now we can call our read accounts resource and check what happened. And you'll see, indeed, that we do now have, uh, Amazon shares. And we've done a bunch of these transactions. If you look in here, which are, uh, consistent with that, uh, trading activity. So it looks like, uh, things were successful, we've been able to run our trader agent using this code, using our module, and it's called a slew of MCP servers. And it had collaborating a trader and also a researcher to do it. Okay. And finally, before we wrap up, let's just quickly count how many tools we just used in total. So from that MCP params module we can import the trader and researcher params, add them up and then iterate through them. And for each one, collect the tools and report on the total number of tools that we just made available to our two agents, our trader and researcher agent, in what we just did. And so we had a total of six MCP servers and 44 tools. That's a lot of tools. Uh, so yeah, that was great fun. And of course, you know what I'm going to say, the challenge for you is that you can add more tools. The more the merrier. We can give more capabilities to our traders to allow them to do more. And of course, if you work in finance, if you have business knowledge of this area, then I'm scratching the surface of what we could achieve with this. And you can add in tons more capabilities than our 44 tools. But again, one more time, I would urge you to start by adding in those tools, working in the lab to understand what the models are capable of and to experiment with it before you. Then go and put it into the modules and run it as part of the final architecture. And that's a wrap on day four. Obviously, we covered a lot of ground, and it's super important that you go through and replicate this yourself and look at things, look at this code to get a good sense of how you go about building these sorts of agent solutions. And of course, make it your own. As always, tweak the prompts, experiment. Remember, they're all in that one module template Test.py. So you can experiment with changing the prompts and see how that affects the characteristics of the trader. But of course, the big moment is coming tomorrow. And we then unveil the full platform. We put the finishing touches on everything and there's some really cool finishing touches where you bring it all together. And I also want to give some summaries on big picture thoughts on the big question like, okay, we've we've covered all these different ancient frameworks. So which one should I choose for my project. We'll answer that as well. All in our grand finale next time.
------
Video 125. Week 6 - Day 5 - Which Agent Framework Should You Pick?
------
While it's all been leading up to this. Welcome to the grand finale. Welcome to week six, day five, the conclusion of our capstone project, and also the time when we wrap up and answer the question of, okay, so which framework should I use for my project? But first, the capstone project, autonomous trading. We have some ingredients to add to the mix. We're going to turn it into a trading floor of four traders. We're going to give the traders autonomy to evolve their strategy. We're going to expand the number of models. You've already seen a little teaser of this, and we're going to build a user interface, which is going to have a surprise extra component, which is a super important piece of functionality extensibility that we can get through OpenAI agents SDK. So without further ado, let's go back to the lab for the final time. And here we are once more in cursor for the last time. We go into the sixth folder and we go to lab five, which is really more of a text thing, but let's just talk about what we have here. So we've, uh, actually now we've already taken a look at all of the MCP servers that we're going to be playing with, but it includes, of course, things like the push notification and the memory and everything else. But I want to be adding more in time. And I would like you to as well. And so if you go in there and you look in the MCP servers, you might find that there are more there already. Uh, because I do plan to be adding more and if not, you should keep adding them in. As of the time I record this, there are currently, as we just saw, six MCP servers with 44 different tools and two resources, and I do hope to be adding to them in time. Okay, so now to introduce you to the four traders that we have set up in our trading floor, we have four traders that are now known. They are named Warren, George, Ray and Cathy, and they are paying homage to four luminaries of the industry Warren to the legend that is Warren Buffett, George to George Soros. Ray for Ray Dalio and Kathy, perhaps an odd one out for a more modern investor for Cathie Wood of Ark Investments. And they each have investment strategies inspired by their namesakes. Uh, but also I've given them autonomy to be changing their strategy in time. So let's look at this. So first of all, we need to go to a Python module called Reset Pi. And this is where the strategies are laid out. There is a Warren strategy. Uh, you are Warren. You are named in homage to your role model, Warren Buffett, a value oriented investor prioritizing long term wealth creation. George is named for George Soros, an aggressive macro trader. Ray. Of course, systematic principles based approach, uh, with macro economic insights. And Kathy, who pursues disruptive innovation, particularly crypto. But I focused on crypto ETFs because this is all meant to be about equity trading. So ETFs is what we do. Um, and reset traders will reset these for people to have this strategy. Now I'm not going to run that reset method myself because I've already had this running for a couple of weeks. And I don't want to delete all that history. I want you to see how these traders have done for the last couple of weeks, because that's going to be fun. Um, so I won't reset, but you, of course should, so that you have the right strategies if you're looking at this. So you should uncomment this line right here and run reset traders so that everything is ready to begin. And then I need to talk to you about traces. Okay. So I now want to tell you about traces. One of the things you might have thought about OpenAI agents SDK is that whilst it is quite lightweight and simple, which is nice, it perhaps doesn't have the same level of resiliency and plumbing that something like Landgrave has with its connectivity to Lang Smith. The trace functionality in OpenAI. It's a little bit sort of cheap and cheerful. You have to log in to your OpenAI screen and then see the tracing in there. Well, it happens that they thought of that. They've made the tracing functionality in OpenAI agents SDK very extensible. You can, in fact connect it to Lang Smith to be able to look at your data in Lang Smith, you can also connect it to things like weights and biases, which I teach in my other course, which is super useful. And indeed, you can extend it yourself to record trace information however you would like, so that you can programmatically handle that with your agent flow and take actions as a result of tracing. And that is really cool and it's very easy to do. And I've done it here in this traces module. So you essentially make a subclass of an OpenAI class called Tracing Processor. And in that subclass you have to override four methods on trace start on trace end on span start and on span end, and you get passed in an object that describes what's going on, and you can then choose to do what you want, record it in any way. And what gets passed in could be one of a number of different types of data, and you can pluck out relevant information and decide what to do with it. And so what did I use this for? What have I decided to do with it? I've decided to have my own little logging mechanism. And so I write this to my own log with a name and a type and a message, and I actually store this in SQLite. I have this write log, just writes it to a database based on this person's name, so that I can keep track of these logs, and that allows me to take some actions as a result of the tracing activity happening in my agent framework. And what would I like to do with that? Well, I'd like to display it. I'd like to be able to see on the user interface, what are our traders actually doing? What are they thinking, what's going on? And I'd like to be able to reflect what is effectively being recorded in OpenAI's traces on the user interface, and that is what we will be doing. Okay, so we've talked about the user interface. Let's see what it is. Well this is the code for it. It's a Gradio app. And you probably know that I love Gradio. Now the purpose of this course is not to teach you about Gradio. So I'm not going to go through this in a lot of detail, but it is very simplistic. Uh, I've got like a class trader, which is going to to wrap my various calls to, to organize the business rules around a trader for this screen. And then I've got a sort of companion class, trader view, which deals with the visual elements associated with any trader. And it has something that periodically refreshes the screen. And then this ends with just the create UI that has the final gradio code to to create it, and something that launches the screen. So you can look at the UI code if you wish. It's not super important. What's important is what information it shows us. And let's take a look at that right now. So I press control and Backtick to open a terminal. I go into the sixth folder and now I type. You've run app Dot Pi to launch our Gradio app for our autonomous traders. And here it is. Here in a nutshell is the UI for our traders. And I gotta tell you, I love this. So the four columns here represent our four traders. This is the trader view object with the trader behind it Warren. And in this case Warren is controlled by GPT four one. Mini George is deep seek V3, Ray is Gemini two five flash and Cathie is using appropriately for crypto investor grok three mini. Um, and one thing that's kind of fun is to see that this has been running for a few weeks, and all four of my traders are profitable. They all started with $10,000. That is what happens when you call reset, and none of them have made a fortune, but they have all made a little bit of money. And the winner right now is Cathy. Uh, the, uh, Cathy seems to be doing quite well with her portfolio. And it does look like three out of the four have Nvidia stock in their portfolios. Um, and you can see here some fun looking stuff that we're seeing the trace information about these traders. And of course we're seeing this chart here that shows what's been going on with their portfolios. Um, and I haven't actually run them for a few days, which is why there's been this sort of jump down of, of their performance. Uh, so this is pretty cool. This is the user interface that's looking at our traders. But if you're following, you'll realize that we're not actually seeing anything dynamic here. We're just seeing a snapshot of the data in the database that reflects our. For traders, what we really want to see is what happens when they start trading, when we actually kick this off and run it. And that is what we're going to do right now.
------
Video 126. Week 6 - Day 5 - Key Settings and Launching the Trading System
------
So first we take a look at some code and then we go and run our trading floor. So I wanted to point out that there are a few settings in the EMV file that you can control. You need to put these in there to make sure that things work properly. Run every N minutes just literally like that. Equals 60 will make sure that it that it runs every 60 minutes. Actually if you if you don't include it in your M file it will default to 60. Anyway, run even when market is closed, is how you can make sure that it doesn't start trading all night overnight every every hour. And now I currently have this. True because it is Sunday right now. And I want to I want it to be trading and use many models. If it's false, it just uses Gpt4 mini. If it's true, it uses the models that we just saw in the user interface. And so obviously I have it as true. All right. And now I need to show you the class that brings it all together. That is wonderfully simple and is a fitting conclusion to the other material that we've covered. So we collect the variables that I just mentioned from the env file. These are the names of our agents. And then we simply look how short this is. We create our traders. Uh, so we create a collection of traders for each of these different traders. And then here is the run every n minutes function. This is the one that that does everything. It all happens here. We start by adding this trace. This is where we register that we want OpenAI agents SDK to record any traces using that log tracer that I created that I showed you a moment ago. We then create our traders as done here. And then there's a nice little whiletrue, uh, thing here. So this will keep going forever. Um, it will then call async gather, which you remember means that all these async methods will happen at the same time. Trader run for each of our traders so all four traders will be kicked off at the same time, and async IO will make sure that they all run as if as if it's multithreaded. But of course, what's going on is that anytime one of them is is locked on, IO is waiting for something to come back, another one will run that will allow them all to run. Uh, and uh, at the end of that, it will then sleep for the number of minutes that we specify. And finally there's this if name is main. And that's where we kick off this whole thing. So all that remains now is for me to actually kick this off. Let's do it. And so the time has come now to launch our trading floor. See these agents in action and see in particular this while true loop running. So I'm going to bring up a terminal. This is our running user interface. I'm going to open a new terminal window to run. At the same time, I'm going to change to the sixth directory and I'm going to do you've run Trading Floor dot Pi. And this is now kicking off our agent process and running our various agents. So it's running for different traders and their researchers. And right away you can see I hope that the information in these panels has started flying by. And this of course is our user interface tracking the traces that we have intercepted and written ourselves so that we can be managing the logs. It's being stored to a database and and showing here visibly on the user interface. And it is terrific fun to watch this and see autonomy in action. You'll see the different colors representing things like calling different MCP servers actually using tools. The red when you see it is when it's actually calling our accounts business logic, our own home grown MCP server and that noise, if I don't know if it comes through on this mic, but it just made a notification noise that one of our four traders of Warren or George or Ray or Cathy just made a trading decision and I got alerted about it, and they will now be off and running. It typically takes them a few minutes. Uh, thanks to the joys of async coding, they can run in parallel like this as they do their thinking. They will go through managing their portfolio, and down below you can see things like their current holdings and their recent transactions. So you can keep keep stock on what they're doing. Uh, and uh, yeah, as I say, it's uh, it's it's terrific. Terrific fun to see this. Here we go. We can just see here. Sold five AMD shares, retrieving account details and then sold three Qualcomm and bought four Nvidia shares as Warren. Uh, they do tend to love Nvidia. Uh, so um, which must be something to do with the researcher and some other share activity going on, and an update there to our to the values of the portfolios. Oh, and another text message I just got, which is and another one which is trading going on AMD trade apparently. Uh, and yeah, as I say, this is super addictive. I absolutely love watching this. Obviously the learning point here is to really get this very visual, very real, tangible sense of what it's like to have multiple agents working, uh, calling other agents and using a bunch of different MCP servers to achieve a commercial objective. Uh, and being able to use this as a way of inspecting what's going on and understanding the LM activity that's happening behind the scenes to allow for this trading activity. So hopefully when you run this yourself and get this sense, uh, you'll find it, uh, highly, highly educational and insightful as well as being quite a lot of fun.
------
Video 127. Week 6 - Day 5 - Advice for Selecting Agentic Frameworks
------
Well, as you can probably tell, I'm quite chuffed about that project. I think it came together so nicely, and I really do hope that there's a lot to learn from it and seeing it together like that. And I love the fact that it is actually a nod to a commercial use of agents. So a question that I get quite often at this point that people that have that have got to this point in the course, is that people feel a bit frazzled by everything that we've covered, like thinking back to the different variety of projects and frameworks and left with the question, okay, so so now that I'm embarking on my own project, which of these frameworks do I select? What do I choose? And I'm here to give you that answer. I want to tell you exactly which framework you should select. And the answer is the framework that you should select doesn't actually matter. It's not the most important question. And in a minute I'm going to take you through what I think does matter and the sorts of things you should be thinking about when you embark on your project. Look, the different frameworks have different pros and cons, and you should pick the one that just suits you. The one that that that that you jibe with the most that suits your kinds of of of strengths and the places where you need help and also the skills and capabilities of your team as well. I find that that, as you probably know, I like OpenAI agents SDK because it's lightweight, it stays out of the way, and it gives you a level of flexibility and control that I like to be able to code with. But I fully recognize that there are other frameworks like crew, that are much more batteries included, where you get a lot in the box, you get, you know, you just write your yamls and you can focus on your yamls and a lot comes along for the ride, and that can be super convenient and really attractive. And they even come with some of the low code tools that let you visualize this too. So there are lots of benefits to using the other frameworks too, but you can build an agent solution in any of them. So really you just pick the one that works for you the best that you enjoy using and that you have the best experience with. Sure, my go to is to use OpenAI agents SDK. I've used it a lot. I've used it to build projects which which are live. And of course I use MCP as well to bring in other tooling. Um, but I totally get that the batteries included ones work better for others and that there are pros and cons. Landgraf of course, famous for the way that it's so very repeatable, reproducible, and so tightly integrated with the long chain ecosystem and with things like Lange Smith for monitoring. So plenty of pros and cons. And there are also other frameworks too. Now I feel a bit like a DJ that that is up there trying to play the favorite tracks that everyone's going to like, and I always miss one that someone's favorite. And I've had some people message me and say, why didn't you cover whatever? And they're hurt by this and I'm sorry, I had to pick a selection. Some of the other, the sort of the ones that are that are almost made it Google Agent Development Kit, ADK that's becoming increasingly popular. It was it's very new and it was still in its infancy even a few weeks ago. But they've just released what they're calling the version one that's ready for production for the first time and it's starting to get more traction. They've got a lot of companies involved. ADK also comes with a protocol that Google has announced, called A to Agent to agent, which is meant to be a sort of companion protocol to MCP. But while MCP lets you connect to other tools, A to A would allow different agents to discover each other, to be able to exchange information about what they can do and agents to be able to call each other, even if they're running on different hardware in a completely different setting. Now, that second part of it, this, this connectivity piece that's a bit similar to Autogen core, if you remember that same kind of idea. So it's got that Autogen core angle to it, allow different heterogeneous agents to interact. And it's also got this discoverability idea that agents will be able to ask each other, okay, what are you capable of doing through a sort of agent card, which is a bit like a model card, but but for agents. But this is also still in its infancy. It hasn't yet got community traction, so it's a bit too early to call whether this is going to be as popular as something like MCP that has more immediate, more obvious role to play. So that's Google Agents development kit, which is which is somewhere in between OpenAI agents and and also it's got a some, some shades of crew because it has it's got a little bit more of batteries and it's got some user interface stuff as well. Hugging face, small agents, very popular. Uh, it was more popular a while ago and it's super simplistic. It's very much again, shades of opening agents SDK keep it simple and keep out of your way, which is great. Pedantic AI has has a big following. It's really fun to to work with. It's also very similar to OpenAI agents SDK. In fact, OpenAI agents SDK said that they were inspired by some of the pedantic AI work, and they gave a shout out to pedantic AI in their announcements. And, uh, pedantic AI also has some, some elements of of Landgraf in there too. But here's the thing. I chose the frameworks that I chose because I felt that it gives you a a great flavor of the gamut of different kinds of techniques that are used in a genetic framework. And I feel like it prepares you really well to adopt any of these. If you just bring up Google ADK and go through their tutorial and how to, you'll see so many familiar concepts that will be like, okay, sure. And in fact, you'll see that already a student has contributed in the community contributions. Google SDK solution for for one of the weeks, which is so cool. And so you can see that it's just very, very similar, the same same kind of techniques. And so I do encourage you, if one of these is one that you really want to explore as well, then then go and take a look at it. But, but and I think you'll find it's super easy to pick up same, same kinds of concepts as we've used tools, structured outputs and so on. But the overall message is don't get bogged down in framework selection. They all have their pros and cons. You can focus on the one that lands well for you. It's not the most important stuff. Let's talk about the important stuff. So I'm going to give you ten pieces of advice, ten things that I think matters. And this is personal advice, so you can feel free to disagree with me. Reject this. This is not necessarily the, you know, the gospel. This is simply my experiences, my advice as a practitioner in the field. First things first, and this is a general advice. In a lot of cases I do find, and perhaps particularly in Agentic I, that a lot of people come into this space with the solution in mind. They come in and say, I want to use agents for X, and it's super important to say that that is a bad way of thinking about it. You should really never you should always catch yourself with a little red flag if you're coming into building something. And the starting point is I want agents to do X. First of all, focus on the problem you're solving and what is X, what is wrong, and go with an agent solution. If you realize that that is in fact the right way to solve that problem, start with the problem. Go to the solution, not the other way around. I know it's the kind of thing that everyone says. You probably heard that a million times, but particularly with agents. There's so many people because of the agentic hype that are just jumping on. I need an agent for this. I need an agent for that. Start with the problem. Second piece of advice is that once you've got a problem, identify the metric that you will use to measure whether you have successfully solved that problem. This is like a standard piece of data science advice, but this can be your North Star for for as you're working to see whether you're getting closer or further from solving the problem and finding that metric can be really difficult. And that's something which is a important part of the task, which you have to start with and associated with that. It's not one of the ten, but but along with the metric, you have to be able to curate the data that will allow you to measure that metric. So understanding the data that you've got, the data that you need and making sure that you curate that data in order to measure that metric. This is all pretty standard data science stuff. But often again, I think particularly with agentic solutions, people sometimes bypass this because they're so focused on I want my agents to do blah.
------
128. Week 6 - Day 5 - 10 Essential Lessons for Building Agent Solutions
------
And moving on to the third piece of sage advice from me. It is to favor workflows over autonomy to start with. So when you're embarking on a new agent solution, it's tempting to rush into a fully autonomous solution. It's better to start by building something which is using simple workflows to take things step by step. Now, when you're working with open AI agents SDK, there are a number of different ways to do this. You remember, you can use tools as a way to have one agent call out to another, and you can also use handoffs as a way that an agent can pass control to a different one. But actually I would suggest starting using a third approach. You might wonder what's the third approach? I don't remember that. Well, this is actually what we did with our deep research agent. The third approach is simply to use Python code to make each agent call in isolation. Call runner run, make a call to an agent, get an output, and then call runner run again and do things step by step in this organized way with Python code later, you can turn this into Hand-offs or tools, and you can add autonomy and give more activities to one agent, more responsibility. But start simple with hard coded or Python coded workflows. So my fourth piece of advice is super important, particularly for people from a software engineering background such as myself, it's common to come into building agent solutions with a blank sheet of paper and draw some big agent architecture diagram. And in my view, that's not the best approach. It's better to work on these things. Bottoms up. Start with a simple agent. Take a small part of your problem and solve it well with one simple agent, one LM call and work on that until you've got that working well, and then add another agent and work bottoms up, building your platform as you discover what works well and what doesn't. Now, sometimes it's good to actually approach it from both angles. Do a bit of top down and a bit of bottoms up, but out of the two I would favor bottoms up in the case of a genetic workflows, because you need to first discover what's going to perform well with your LMS. And the next one is very similar, which is to say, it's best to always start simple and then make your platform more complicated when the simple solution is working. I can't tell you how many people have sent me these massive, great solutions, hundreds of lines of code, and it's not giving them the outcome that they want. And they're saying, I don't know, it's broken. Why help improve this? Fix it? And the answer is it doesn't work that way. You can't do that. It's too hard. You have to start small and simple, solve a simple problem and do it really well, and then gradually expand and with each of the building blocks, as you solve each problem, you put them together and you see the bigger agent workflow working well. But that's the right approach, because if you build everything and get complex and the answer doesn't match what you're looking for, it's impossible to know where to look. And so the simple key start simple. And a sort of opposite point to that, interestingly, is I recommend, when starting out, begin by using the highest end models to start with with with small data sets, such as using the GPT four one or using Claude for right now, Claude for sonnet, maybe not Claude for opus. That might be a little bit too too expensive, but start with the expensive ones so that you can make sure that in theory, what you're trying to accomplish will work. And then once you've got that working, you can then look to move to cheaper models like Claude for sonnet or GPT for one mini, or maybe GPT for one nano. And as you make your prompts more and more instructive and precise, you may be able to achieve similar performance with lighter models. But as you start with your simple solutions, start with the high powered models so you can prove out what works and what doesn't. And then the next piece of advice for you. This is a subtle one, so I see a lot of people getting caught up in trying to figure out different types of memory, which is a construct that I feel is like overworked. There's like short term memory, some people call it, which is usually just looking at the conversation so far. And then there's longer term memory, which is usually other words for for looking up like a rag retrieval in a database or using the kind of knowledge graph that we used in the trading project. The thing to keep in mind is that all of these different memory techniques are, in fact, just different ways of finding relevant context and shoving it in the prompt. It's all about what goes in the prompt at the end of the day. And so keep a clear mind about that and make sure that that whilst you might use various tools and techniques for memory at, at the end of the day, all you're doing is trying to find the right relevant material for the prompts. So look at the prompts, see what's being included there, look at the tools you're using and see what information is being retrieved. And make sure that you're giving the LLM the right context to be able to answer its question. And so rather than getting too bogged down in what kinds of memory you're using, focus on what kind of information does the LM need to answer the question? And are you providing that information in the prompts? Okay. And I'm almost done with this preachy advice for you. Let's go to number eight. It's very similar point again, which is that look, most of the difficulties that you come up against working with agent systems and with LMS generally are fixed by better prompting and by experimenting with your prompts. People often are message me and say, can I use fine tuning for this? Can I use a different encoder? LM for my rag? All sorts of sophisticated stuff, when often the right answer is to just focus on the prompts, make something a bit simpler, a bit more directive, a bit more instructive. If it's outputting one thing, then try and tell it not to give it some examples of what a good output looks like. You can get so far just by working on your prompts. And a similar point again is also look at the traces. Of course, this is a great discipline to get into, even if your agents seem to be working well and they're giving the answers that you expect. You should still be disciplined to go and check that everything is just as you want in the traces, just in case there's extra tool call happening. There's some weird stuff happening now. I'm often guilty of not going back and looking at the traces, particularly if things seem to be working, and then often when I do, I then discover that there is something in there, there is a gotcha that I can fix. And so it is so important, even if I don't take my own advice on this always you should you should go in and check, check the traces. Always. As you're building your agent system, make sure things are behaving the way that you expect. And I've left the most important piece of advice to the very end. And here it is. Look, being an AI engineer, being an LM engineer involves wearing two hats, a software engineering hat, and a data scientists hat. And when you're beginning your project, when you're at the starting point of building a new system, you need to be taking firmly off the software engineering hat and putting on that scientist hat and be a scientist. That means be comfortable with experimentation and R&D. People often come to me and say, which? Which model should I pick A, B, or C? Which technique should I use? This, this or this? Which tool should I use? And the answer is you should try them all. Try them. Experiment. Look at your overall business metric that you're using to gauge success, and use that to judge which of the different techniques is working better for you, and embrace the experimentation. It's absolutely key. I've put a whole guide about this. If you look in the guides, you'll see one. I think it's one of the last ones is about how to build your own projects. And I talk a lot about this. There's no shortcut to R&D. Embrace being a scientist and researching and exploring and understanding what works well. I often have an instinct about the right way to go and build something, but it turns out my instinct is often wrong. So don't trust my instinct, but instead experiment and discover for yourself.
------
Video 129. Week 6 - Day 5 - Course Recap and Final Goodbye  Keep Building!
------
Well, thank you for indulging me on my ten pieces of advice there. Again, these are just one practitioner's thoughts, but I do hope you find some of it helpful. Okay. So with that, I want to take you back to review the journey of the last six weeks and what an adventure it has been. Seems like a long time ago that we were looking at the foundations of a genetic eye, looking at what it's like to call multiple llms through the direct native APIs, and also different agentic design patterns, ending with the personal career agent that hopefully you deployed on Huggingface spaces. I know people have been looking at my one, and I know that some of you are ask cheeky questions to it because when you do, of course I get a notification. It hits my Apple Watch and it happens at the strangest of times. And I've actually cracked up in the middle of a meeting when someone asked a particularly silly question to it. So it's great fun, but hopefully you have your own and it's a bit more serious than you posted on it on LinkedIn, and it's getting some traction. And then in week two, we unveiled my favorite OpenAI agents SDK. Nice and easy. We built some really cool projects, we built the deep research, and a number of students have extended this and made meteor deep research apps that do things like asking clarifying questions. And that's all in the community Contributions project and in the folder. And you should put something in there too, because there's lots of great stuff. And then in week three, we went a different direction by looking at crew AI, more of a batteries included framework. And I think for me, the startling thing about week three was when we built the engineering team and I got to tell you that the the results of that I find it spellbinding. And just recently I tried running all of the test cases and they all run flawlessly. It's so amazing to see this come together, and I hope you had the same experience, and I hope you've actually put this to good use and built something with it, as we did in building the accounts stuff for for week six. And then week four was a surprise for me. I'd used Landgraf before, but not as in as much detail as I did in during week four building those projects, and I really enjoyed it and I was very impressed by it. It wasn't as heavyweight as I was expecting and it was very powerful. And the psychic that we built was great fun. I hope you've enjoyed that. I hope you've used it as well and extended it. I actually rebuilt it myself in OpenAI agents SDK to as a way to, to use it myself. Um, but it's really interesting to see that and to take advantage of the Landgraf and the Lang chain ecosystem in building that. And then week five was another bit of a surprise. Using Autogen, I found that Autogen agent chat was particularly lightweight and simple and easy to work with, although perhaps not as mature as OpenAI agent SDK. And then I really enjoyed Autogen core. As I say, there's there's some shades of some of the same things that Google has worked on with a to a um, in the way that Autogen core can allow different, different heterogeneous agents to talk to each other. So it's exciting stuff. And then building the agent creator was a lot of fun. I really like meta exercise and surreal and and exciting, so I hope I hope you enjoyed that that project, and I hope that it sparked some thoughts about different ways that you can take an agent creator to be building more agents. And then week six hardly needs a recap because we've done it. It's fresh in your minds. MCP servers, building them, using them 44 different tools in our capstone project for equity traders. That was such good fun and hopefully you found it, like me, a fitting conclusion, a really juicy project to wrap things up. And nice that it's a commercial project that applies it to a real business area. If you're in finance, I hope you're looking to extend it. If you're not in finance, I hope you're thinking about how could you apply a similar kind of agent framework to your business area, that that's the big idea. And then also at the end we went through some thoughts about overall picking agent frameworks. It doesn't matter. And some general points about things to look for as you're building your agent. Solution. Okay. We're going to head into our final farewell part. Don't go anywhere. There's only like a minute or two left of this whole adventure, and it's super important that I get to say to you, congratulations, you've made it through the full curriculum. You've now experienced so many different agent frameworks and patterns and designs and projects, and you should be equipped yourself. Just as we've equipped our agents, you are now equipped to be going off and building commercial, exciting projects that use autonomous agents. And you should be celebrating that on LinkedIn. And many, many congratulations for making it to that point. Let me now tell you the final to do's for you. And so my first request to you is to be building. You should go in and look at the community contributions from others, and then add some of your own community contributions. If you're interested in the trading simulator. I really love this project. I would love it if you added more MCP servers. Add some more agents to the mix, give it some more autonomy to do new things. I'd love to be able to add short selling crypto currency trading into the mix, But if if a different project appeals to you more, then work on that. I can't wait to see the stuff that you build and then share on LinkedIn. Share your success with this project with this course. Share your contributions, share any aha moment, anything that's been inspirational for you, or an insight that you'd like to highlight to other people and post that. And if you tag me, then I will weigh in and make sure that it gets amplified along my community as well, to make sure that that your expertise is shared wide and perhaps catches the attention of future clients of yours, or maybe future employers of yours as well. And then I may may have mentioned this maybe once or twice. Uh, please do connect with me on LinkedIn. I love connecting with people on LinkedIn. You don't need to, uh, make a comment if you don't want to. You can just connect. But if you do want to make a comment, then I will reply. And, uh, yeah, people joke that I have an agent running. It's not an agent. It's going to be me. I will reply to you. I do reserve the right to occasionally paste from my clipboard because, you know, I do sometimes say the same thing again and again, but I assure you that at least 50% of the content will be original content typed, but maybe half of it will be a sort of paste at the end of it there. But I'm sure you'll forgive me for that. That that must be allowed. But other than that, please do connect with me on LinkedIn. And I should also say my my editor would kill me if I didn't mention that. If you're able to rate this course on Udemy, it makes a huge difference. That's the main way that Udemy decides whether or not to recommend this course to other people. So as I say, it makes an enormous difference for me and for the course. If you have a moment to go in and rate the course, I would be super grateful. And that brings us to the final few seconds of this course. It remains for me to thank you so much for making it all the way through to the very, very, very end to say a big farewell. It's been great hanging out for the last six weeks. I hope to make many more courses so so I will be back. In the meantime, please do stay in touch and please do keep building.
------