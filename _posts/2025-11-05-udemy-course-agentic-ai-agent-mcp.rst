---
layout: post
title: "(Udemy Course) AI Engineer Agentic Track: The Complete Agent & MCP Course"
date: 2025-11-11 00:00:00 +0530
categories: [ai, agents, mcp]
tags: [agentic-ai, openai-agents, crewai, langgraph, autogen, mcp, ai-agents, tool-calling, structured-outputs]
pin: false
toc: true
comments: false
math: false
mermaid: true
description: "Master Agentic AI through an intensive 6-week program covering OpenAI Agents SDK, CrewAI, LangGraph, Autogen, and MCP. Build 8 real-world projects from Career Digital Twins to autonomous Trading Floors with hands-on tool integration and multi-agent orchestration."
image:

  path: /attachments/posts/2025-11-11-udemy-course-agentic-ai-agent-mcp/images/preview_art.png
  alt: "AI Engineer Agentic Track Overview"
allow_edit: true
---


AI Engineer Agentic Track: The Complete Agent & MCP Course
===========================================================


Preface: The Year Agents Enter the Workforce
---------------------------------------------

2025 marks a watershed moment for artificial intelligenceâ€”the year autonomous agents step into the professional arena. No longer confined to research labs or experimental sandboxes, AI agents now orchestrate complex workflows, make autonomous decisions, and collaborate across systems with unprecedented sophistication.

This transformation demands a new breed of expertise. Understanding how to prompt a single model no longer suffices. The frontier lies in designing systems where multiple agents collaborate, where LLMs wield tools to interact with the real world, where autonomy operates within carefully constructed guardrails.

This document captures a complete learning journey through the Udemy course *"AI Engineer Agentic Track: The Complete Agent & MCP Course"*â€”an intensive 6-week program designed to transform understanding into mastery. From foundational design patterns to production-ready multi-agent systems, each week builds capabilities systematically: first with native LLM orchestration, then through OpenAI Agents SDK, CrewAI, LangGraph, Autogen, and culminating with the groundbreaking Model Context Protocol (MCP).

The path ahead is intensely practical. Eight real-world projects awaitâ€”from building a Career Digital Twin to deploying an autonomous Trading Floor with 44 tools across 6 MCP servers. Each demonstrates not merely what agents can do, but what practitioners must master to unleash their commercial potential.

The architecture may be complex, but the destination is clear: expertise in designing, building, and deploying autonomous AI agents that solve meaningful problems in production environments.


.. code-block:: mermaid

   mindmap
     root((Agentic AI Mastery))
       Week 1 Foundations
         Agentic Workflows
         LLM Orchestration
         Resources vs Tools
         Tool Integration
         Structured Outputs
         Career Digital Twin
       Week 2 OpenAI SDK
         Agents Framework
         Guardrails
         Deep Research
         SDR Agent
       Week 3 CrewAI
         Low Code Framework
         Multi Agent Teams
         Stock Picker
         Engineering Team
       Week 4 LangGraph
         Computational Graphs
         Browser Sidekick
         Advanced Orchestration
       Week 5 Autogen
         Remote Collaboration
         Agent Creator
         Meta Agents
       Week 6 MCP
         Model Context Protocol
         Open Source Integration
         Trading Floor Capstone
         44 Tools Across 6 Servers


Course Navigation: Complete Learning Path
-----------------------------------------

This course consists of 14 interconnected postsâ€”5 framework deep dives and 8 hands-on project implementationsâ€”designed to build agentic AI mastery systematically.


Framework Deep Dives
~~~~~~~~~~~~~~~~~~~~

Master each framework through focused exploration of core concepts, installation, examples, and when-to-use guidance:

**Week 2: OpenAI Agents SDK** â€” `OpenAI Agents SDK Framework </posts/openai-agents-sdk-framework>`_
   Lightweight, Pythonic agent creation with native tool calling and structured outputs. Best for: Python-first teams, rapid prototyping, simple agent workflows.

**Week 3: CrewAI** â€” `CrewAI Framework </posts/crewai-framework-multi-agent-teams>`_
   Configuration-driven multi-agent teams with YAML-based definitions. Best for: Role-based collaboration, sequential workflows, non-Python teams.

**Week 4: LangGraph** â€” `LangGraph Framework </posts/langgraph-framework-computational-graphs>`_
   Computational graphs for complex stateful workflows with checkpointing and conditional routing. Best for: Complex decision trees, human-in-the-loop systems, resumable workflows.

**Week 5: Autogen** â€” `Autogen Framework </posts/autogen-framework-agent-collaboration>`_
   Remote agent collaboration with code execution and distributed runtimes. Best for: Code generation tasks, distributed systems, meta-agent architectures.

**Week 6: MCP** â€” `Model Context Protocol </posts/mcp-model-context-protocol>`_
   Universal standard connecting AI models to tools and data sources. Best for: Framework-agnostic tools, open-source ecosystems, reusable server architecture.


Project Implementations by Week & Complexity
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Build production-grade applications progressing from beginner-friendly to expert-level complexity:

**Beginner-Friendly Projects (Weeks 1-2)**

1. **Career Digital Twin** â€” `Building Your Professional AI Assistant </posts/project-career-digital-twin-agentic-chatbot>`_

   *Week 1 | Foundation Project | ~2 hours*

   Create conversational agent representing your professional expertise. Learn: structured outputs (Pydantic), tool integration, Gradio interfaces, HuggingFace Spaces deployment.

2. **SDR Agent** â€” `Automated Sales Development Representative </posts/project-sdr-agent-sales-automation>`_

   *Week 2 | OpenAI SDK | ~3 hours*

   Build autonomous sales agent researching prospects, crafting personalized emails, sending via SendGrid. Learn: multi-agent collaboration, async orchestration, quality evaluation loops.

**Intermediate Projects (Weeks 2-3)**

3. **Deep Research Agent** â€” `Multi-Source Web Investigation </posts/project-deep-research-agent-web-search>`_

   *Week 2 | OpenAI SDK | ~3 hours*

   Autonomous research agent planning queries, executing parallel searches, synthesizing comprehensive reports. Learn: planner-executor-synthesizer pattern, Serper API integration, HTML report generation.

4. **Stock Picker** â€” `Multi-Agent Investment Analysis </posts/project-stock-picker-agent-crewai>`_

   *Week 3 | CrewAI | ~4 hours*

   Investment analysis team with researcher, analyst, and risk assessor agents. Learn: CrewAI YAML configuration, structured outputs, sequential workflows, financial API integration.

**Advanced Projects (Weeks 3-4)**

5. **Engineering Team** â€” `Four-Agent Software Development System </posts/project-engineering-team-four-agent-system>`_

   *Week 3 | CrewAI | ~5 hours*

   Complete engineering team (PM, Engineer, QA, DevOps) generating production-ready applications. Learn: agent delegation, code generation, multi-specialist collaboration, GitHub integration.

6. **Browser Sidekick** â€” `Web Automation with LangGraph </posts/project-browser-sidekick-langgraph-operator>`_

   *Week 4 | LangGraph | ~5 hours*

   Autonomous browser automation with human-in-the-loop control. Learn: LangGraph state graphs, Playwright integration, operator pattern, checkpointing, visual reasoning.

**Expert Projects (Weeks 5-6)**

7. **Agent Creator** â€” `Meta-Agent System Design </posts/project-agent-creator-meta-agents>`_

   *Week 5 | Autogen | ~6 hours*

   Meta-agent that creates other agents dynamically. Learn: Autogen code execution (Docker), distributed runtime, meta-programming, safe code validation, dynamic orchestration.

8. **Trading Floor** â€” `Autonomous Trading with MCP </posts/project-trading-floor-capstone-mcp>`_

   *Week 6 | MCP | ~8 hours | Capstone*

   Complete trading system with 44 tools across 6 MCP servers. Learn: MCP server development, real-time market data, multi-agent trading workflows, risk management, production safety.


Project Comparison Matrix
~~~~~~~~~~~~~~~~~~~~~~~~~~

Choose projects matching your learning goals and experience level:

.. list-table::
   :header-rows: 1
   :widths: 25 15 15 20 25

   * - Project
     - Complexity
     - Framework
     - Key Technologies
     - Commercial Applications
   * - Career Digital Twin
     - Beginner
     - Foundation
     - Pydantic, Gradio
     - Personal branding, consulting
   * - SDR Agent
     - Beginner
     - OpenAI SDK
     - SendGrid, Async
     - Sales automation, lead gen
   * - Deep Research
     - Intermediate
     - OpenAI SDK
     - Serper, HTML
     - Market research, due diligence
   * - Stock Picker
     - Intermediate
     - CrewAI
     - Financial APIs, YAML
     - Robo-advisors, investment tools
   * - Engineering Team
     - Advanced
     - CrewAI
     - Code gen, GitHub
     - Rapid prototyping, scaffolding
   * - Browser Sidekick
     - Advanced
     - LangGraph
     - Playwright, Vision
     - Web scraping, testing, automation
   * - Agent Creator
     - Expert
     - Autogen
     - Docker, Meta-agents
     - Dynamic systems, AI research
   * - Trading Floor
     - Expert
     - MCP
     - Polygon, 44 tools
     - Algorithmic trading, fintech


Recommended Learning Pathways
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Path 1: Rapid Foundations (1 week)**
   Career Digital Twin â†’ SDR Agent â†’ Deep Research â†’ Stock Picker

   *Focus:* Core patterns across simple-to-moderate complexity. Build practical skills quickly.

**Path 2: Framework Mastery (2-3 weeks)**
   Read all 5 framework posts â†’ Build 1-2 projects per framework â†’ Compare architectures

   *Focus:* Deep framework understanding. Know when to use each framework commercially.

**Path 3: Production Engineer (3-4 weeks)**
   Foundation posts â†’ All beginner/intermediate projects â†’ Trading Floor capstone

   *Focus:* Production deployment skills. Safety, monitoring, real-world integration.

**Path 4: Agent Architect (4-6 weeks, complete curriculum)**
   Sequential: Week 1 foundations â†’ Week 2-6 frameworks + projects â†’ Trading Floor

   *Focus:* Comprehensive mastery. Design complex multi-agent systems from scratch.


Course Overview
---------------

**Course Title:** AI Engineer Agentic Track: The Complete Agent & MCP Course

**Structure:** 6-week intensive program with 8 production-grade projects

**Last Updated:** November 2025

**One-Sentence Summary:** A hands-on journey from foundational LLM orchestration to advanced multi-agent systems, mastering OpenAI Agents SDK, CrewAI, LangGraph, Autogen, and MCP through eight commercial-grade projects.

**Course Resources:**

- GitHub Repository: https://github.com/ed-donner/agents
- Course Website: https://edwarddonner.com/2025/04/21/the-complete-agentic-ai-engineering-course/


Who This Course Serves
~~~~~~~~~~~~~~~~~~~~~~~

The curriculum welcomes practitioners across the experience spectrum, though those with Python familiarity and basic LLM exposure will find the smoothest path forward.

**For Beginners:** Self-study labs cover foundational programming concepts, command-line proficiency, and Python essentials. Patience proves the only prerequisiteâ€”every concept builds from first principles.
**For AI Engineers:** Immediate practical value emerges through production patterns, framework comparisons, and commercial applications. The focus shifts from toy examples to deployable systems.
**For Agent Engineers:** Advanced material awaits in guardrail implementation, multi-agent orchestration, MCP integration, and architectural decisions that separate prototype from production.

The hands-on philosophy pervades every lecture. Watching alone yields understanding; building alongside transforms that understanding into capability.


What You'll Learn
~~~~~~~~~~~~~~~~~

**Core Competencies:**

- Native LLM orchestration without frameworksâ€”understanding the foundational layer
- Tool calling and function integrationâ€”bridging language models to real-world actions
- Structured outputs and validation patternsâ€”ensuring robust, predictable responses
- Multi-LLM workflows and design patternsâ€”orchestrating complexity with clarity
- Framework-specific expertise across five major platforms
- Production deployment with proper guardrails and monitoring
- MCP protocol integration for open-source collaboration

**Eight Production Projects:**

1. **Career Digital Twin:** A conversational agent representing professional experience, deployable on personal websites
2. **SDR Agent:** Autonomous sales development representatives crafting personalized outreach
3. **Deep Research:** Multi-agent teams conducting comprehensive topic investigation
4. **Stock Picker Agent:** CrewAI-powered investment research and recommendation
5. **Engineering Team:** Four-agent software development squad (manager, frontend, backend, tester)
6. **Browser Sidekick:** LangGraph-powered agent collaborating within browser environments
7. **Agent Creator:** Meta-agent system that designs and launches new agents autonomously
8. **Trading Floor Capstone:** Four autonomous traders with 44 tools across 6 MCP servers


Course Structure
~~~~~~~~~~~~~~~~

**Week 1 - Foundations:** Direct LLM orchestration, agentic patterns, tools, and resources culminating in the Career Digital Twin project
**Week 2 - OpenAI Agents SDK:** Lightweight, elegant framework introduction with guardrails implementation
**Week 3 - CrewAI:** Low-code agent orchestration through configuration-driven design
**Week 4 - LangGraph:** Computational graphs for sophisticated agent workflows
**Week 5 - Autogen:** Remote agent collaboration and meta-agent capabilities
**Week 6 - MCP:** Model Context Protocol integration and the Trading Floor capstone


Prerequisites & Requirements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Technical Foundation:**

- Python programming capability (guides provided for self-study)
- Command-line comfort (tutorials included)
- Basic LLM familiarity helpful but not required
- Intermediate concepts: async/await, generators, debugging patterns

**Development Environment:**

- Cursor IDE (AI-powered development environment)
- UV package manager (fast, Rust-based Python environment management)
- Git for version control
- Terminal proficiency

**API Access (Flexible Options):**

The course supports multiple pathways, respecting budget constraints while enabling full feature exploration:

- **Recommended:** OpenAI API ($5 minimum, course usage ~$3-5 total)
- **Budget-Friendly:** DeepSeek ($2 minimum, exceptionally low per-call costs)
- **Free Tier:** Google Gemini (within usage limits), Groq (inference platform)
- **Completely Free:** Ollama (local open-source models, reduced performance)

The capstone project optionally uses real-time market data ($20 one-time), though free alternatives exist. Course design ensures complete viability without paid APIs, though frontier model performance provides optimal learning value.

**Cost Context:** Running trillions of floating-point calculations across massive neural networks requires substantial compute infrastructure. API pricing reflects genuine costs while remaining orders of magnitude cheaper than local hardware capable of similar performance. The typical $3-5 investment enables full course completion with frontier models.


Week 1: Foundations of Agentic AI
----------------------------------

The journey begins not with frameworks, but with understanding what frameworks abstract away. Building agentic workflows through direct LLM interaction reveals the mechanics that sophisticated tooling later conceals.


Day 1: First Contact with Autonomous Agents
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Understanding emerges through action. The opening demonstration uses n8nâ€”a no-code workflow platformâ€”to control physical smart home devices through conversational AI. An agent receives the instruction "pick a color, either deep red or deep blue, and change the lights to that color." The system makes an autonomous choice and executes the action.

This simple demonstration encapsulates the core promise: language models transcending pure text generation to make decisions and manipulate the physical world.


What Defines an Agent?
^^^^^^^^^^^^^^^^^^^^^^^

The term "agent" carries ambiguity across the AI landscape, used variously to describe any system involving multiple LLM calls, tool-enabled models, or truly autonomous decision-makers. Hugging Face offers clarity: **AI agents are programs where LLM outputs control the workflow.**

Five hallmarks commonly identify agentic systems:

1. **Multiple LLM calls** orchestrated in sequence
2. **Tool use** enabling interaction beyond text generation
3. **Orchestration environments** allowing inter-model communication
4. **Planning capabilities** where models coordinate task execution
5. **Autonomy** granting models discretion over workflow paths

The most powerful characteristic remains autonomyâ€”the delegation of control that allows models to "choose their own adventure" within defined boundaries.

.. code-block:: mermaid

   graph TB
       A[User Query] --> B{Agentic System}
       B --> C[LLM 1: Analyze Task]
       C --> D[LLM 2: Execute Subtask]
       D --> E{Tool Required?}
       E -->|Yes| F[Execute Tool]
       E -->|No| G[Generate Response]
       F --> H[Process Results]
       H --> I{Complete?}
       I -->|No| C
       I -->|Yes| J[Final Output]


Environment Setup: Tools for Modern Agent Development
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Professional agent development demands modern tooling. Three components form the foundation:

**Cursor IDE** transforms development through AI-assisted coding. Built on VSCode, it integrates LLM-powered suggestions, contextual understanding, and intelligent completions. Installation requires simple download and credential configuration.

**UV Package Manager** revolutionizes Python environment management. Written in Rust, UV achieves environment creation in minutes rather than the hour-plus typical of conda. Installation executes in a single command:

.. code-block:: bash

   # Install UV (Unix/macOS)
   curl -LsSf https://astral.sh/uv/install.sh | sh

Creating project environments becomes trivial:

.. code-block:: bash

   # Navigate to project root
   cd /path/to/agents

   # Create and sync environment
   uv sync

UV automatically installs the correct Python version (3.12 for this course), creates isolated virtual environments, and installs dependenciesâ€”all significantly faster than traditional approaches.

**API Configuration** follows standard patterns, storing credentials in `.env` files:

.. code-block:: bash

   # .env file structure
   OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxx
   ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxx
   GOOGLE_API_KEY=xxxxxxxxxxxxxxxxxxxx
   DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
   GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxx

The `python-dotenv` package loads these securely:

.. code-block:: python

   from dotenv import load_dotenv
   import os

   # Load environment variables, overriding system defaults
   load_dotenv(override=True)

   # Verify key presence
   api_key = os.getenv("OPENAI_API_KEY")
   if api_key:
       print(f"âœ“ OpenAI key exists, begins: {api_key[:7]}...")
   else:
       print("âœ— OpenAI key not set. Check .env file.")


First Agentic Workflow
^^^^^^^^^^^^^^^^^^^^^^

The inaugural hands-on exercise constructs a multi-step workflow: one LLM generates a challenging question, a second LLM answers it. Simple in concept, profound in implicationâ€”this pattern of decomposition underlies sophisticated agent architectures.

.. code-block:: python

   from openai import OpenAI

   client = OpenAI()  # Automatically reads OPENAI_API_KEY from environment

   # Step 1: Generate a challenging question
   messages = [
       {
           "role": "user",
           "content": "Please propose a hard, challenging question to assess "
                      "someone's IQ. Respond only with the question."
       }
   ]

   response = client.chat.completions.create(
       model="gpt-4o-mini",
       messages=messages
   )

   question = response.choices[0].message.content
   print(f"Generated question: {question}")

   # Step 2: Answer the question with a second LLM call
   messages = [
       {
           "role": "user",
           "content": question
       }
   ]

   response = client.chat.completions.create(
       model="gpt-4o-mini",
       messages=messages
   )

   answer = response.choices[0].message.content
   print(f"\nAnswer: {answer}")

This patternâ€”sequential LLM calls where outputs become subsequent inputsâ€”represents **prompt chaining**, the first of five core agentic design patterns.

**Lesson to Remember:** Agentic AI begins not with complex frameworks but with recognizing that LLM outputs can drive subsequent actions. Even simple multi-step workflows demonstrate autonomy when models choose content that shapes downstream behavior.


Day 2: Agentic Patterns and Architecture
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Theory grounds practice. Understanding canonical design patterns provides vocabulary and mental models for recognizing opportunities and avoiding pitfalls.


Workflows vs. Agents: A Critical Distinction
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Anthropic's seminal post "Building Effective Agents" distinguishes two categories under the umbrella term "agentic systems":

**Workflows:** Models and tools orchestrated through predefined paths. The sequence remains fixed, though model outputs vary. Predictability trades against flexibility.

**Agents:** Models dynamically direct their own processes and tool usage, maintaining control over task accomplishment. Flexibility trades against predictability.

The boundary blurs in practice. Even "fixed" workflows grant models autonomy in content generation, while "dynamic" agents operate within guardrailed constraints. The distinction offers useful framing rather than rigid categorization.

.. code-block:: mermaid

   graph LR
       A[Agentic Systems] --> B[Workflows]
       A --> C[Agents]
       B --> D[Predefined Paths]
       B --> E[Consistent Behavior]
       B --> F[Lower Costs]
       C --> G[Dynamic Routing]
       C --> H[Greater Flexibility]
       C --> I[Higher Uncertainty]


Five Workflow Design Patterns
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

**Pattern 1: Prompt Chaining**

Sequential LLM calls where each processes output from its predecessor. Decomposition allows specialized prompting per subtask, maximizing effectiveness while maintaining control.

.. code-block:: mermaid

   flowchart LR
       Input --> LLM1[LLM 1]
       LLM1 --> Code1[Optional Code]
       Code1 --> LLM2[LLM 2]
       LLM2 --> Code2[Optional Code]
       Code2 --> LLM3[LLM 3]
       LLM3 --> Output

Example: Generate business sector â†’ Identify pain point in that sector â†’ Propose AI solution for that pain point.

**Pattern 2: Routing**

An LLM classifier routes requests to specialist models based on task type. Separation of concerns enables targeted expertise while a lightweight router maintains efficiency.

.. code-block:: mermaid

   flowchart TB
       Input --> Router[Router LLM]
       Router --> Specialist1[Code Specialist]
       Router --> Specialist2[Creative Writing]
       Router --> Specialist3[Data Analysis]
       Specialist1 --> Output
       Specialist2 --> Output
       Specialist3 --> Output

**Pattern 3: Parallelization**

Code decomposes tasks into concurrent subtasks executed by multiple LLMs, then aggregates results. Reduces latency for embarrassingly parallel workloads.

.. code-block:: mermaid

   flowchart TB
       Input --> Decompose[Code: Task Decomposition]
       Decompose --> LLM1[LLM 1]
       Decompose --> LLM2[LLM 2]
       Decompose --> LLM3[LLM 3]
       LLM1 --> Aggregate[Code: Aggregate Results]
       LLM2 --> Aggregate
       LLM3 --> Aggregate
       Aggregate --> Output

Tasks need not differâ€”sending identical requests to multiple models and averaging responses (ensemble methods) improves reliability.

**Pattern 4: Orchestrator-Worker**

Similar to parallelization, but an LLM performs decomposition and synthesis. The orchestrator model intelligently divides work and interprets combined resultsâ€”approaching true agent behavior within workflow constraints.

.. code-block:: mermaid

   flowchart TB
       Input --> Orchestrator1[Orchestrator LLM:<br/>Task Decomposition]
       Orchestrator1 --> Worker1[Worker LLM 1]
       Orchestrator1 --> Worker2[Worker LLM 2]
       Orchestrator1 --> Worker3[Worker LLM 3]
       Worker1 --> Orchestrator2[Orchestrator LLM:<br/>Result Synthesis]
       Worker2 --> Orchestrator2
       Worker3 --> Orchestrator2
       Orchestrator2 --> Output

**Pattern 5: Evaluator-Optimizer**

A generator LLM produces output; an evaluator LLM validates quality. Rejection triggers regeneration with feedback. This pattern proves crucial for production systems requiring reliability guarantees.

.. code-block:: mermaid

   flowchart TB
       Input --> Generator[Generator LLM]
       Generator --> Evaluator{Evaluator LLM}
       Evaluator -->|Acceptable| Output
       Evaluator -->|Rejected| Feedback[Feedback Loop]
       Feedback --> Generator

Commercial deployments depend on this pattern. Validation agents significantly improve output quality and consistency, building confidence in system behavior.


Agent Patterns: Embracing Dynamic Behavior
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

True agent architectures abandon fixed paths. Models interact with environments through tools, receiving feedback that informs subsequent actions in open-ended loops.

.. code-block:: mermaid

   flowchart TB
       Human[Human/Environment] --> LLM[Agent LLM]
       LLM --> Action[Execute Action/Tool]
       Action --> Env[Environment Response]
       Env --> LLM
       LLM --> Decision{Task Complete?}
       Decision -->|No| LLM
       Decision -->|Yes| Output[Final Response]
       Output --> Human

The LLM continuously decides: execute another tool, gather more information, or conclude with final output. This autonomy enables tackling complex, unpredictable problems beyond workflow capacity.

**Tradeoffs emerge clearly:**

- **Unpredictable Paths:** Unknown task sequences complicate debugging and optimization
- **Variable Quality:** No guarantee of optimal output or approach
- **Uncertain Costs:** Unknown tool calls make expense forecasting difficult
- **Unknown Duration:** Completion time varies dramatically by problem and model decisions

**Essential Mitigations:**

**Monitoring:** Observability into agent decision-making, tool usage, and inter-agent communication. Platforms like LangSmith (LangGraph), built-in tracing (OpenAI SDK), and custom logging prove essential.

**Guardrails:** Programmatic constraints ensuring safe, bounded behavior. OpenAI's SDK emphasizes: "Guardrails ensure agents behave safely, consistently, and within boundaries you define." Week 2 demonstrates implementation.

**Lesson to Remember:** Agentic architectures trade predictability for capability. Understanding design patterns provides vocabulary for recognizing when to accept that trade and how to mitigate its risks through monitoring and guardrails.


Day 3: Orchestrating Multiple Language Models
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Theory meets practice. Calling multiple models reveals both API mechanics and architectural opportunities.


The Model Landscape
^^^^^^^^^^^^^^^^^^^

Five categories dominate the current landscape:

**OpenAI:** `gpt-4o-mini` (fast, affordable), `gpt-4o` (flagship), `o1`/`o3-mini` (reasoning models)

**Anthropic:** `claude-3-7-sonnet` (recommended), `claude-3-haiku` (budget option)

**Google:** `gemini-2.0-flash` (currently free within limits)

**DeepSeek:** `deepseek-chat` (extremely affordable), `deepseek-reasoner` (R1 reasoning model)

**Groq:** Fast inference platform for `llama-3.3-70b` and other open models

**Ollama:** Local execution of small models (`llama-3.2-3b`, `qwen`, `phi`, `gemma`)

The Vellum AI leaderboard (https://www.vellum.ai/llm-leaderboard) compares costs, context windows, and benchmark performance across modelsâ€”an essential bookmark for practitioners.


API Patterns: OpenAI as Universal Standard
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

OpenAI's chat completions API established patterns now adopted industry-wide:

.. code-block:: python

   from openai import OpenAI

   client = OpenAI()

   messages = [
       {"role": "user", "content": "Explain agentic AI in one sentence."}
   ]

   response = client.chat.completions.create(
       model="gpt-4o-mini",
       messages=messages
   )

   answer = response.choices[0].message.content

**Anthropic** maintains distinct API patterns:

.. code-block:: python

   import anthropic

   claude = anthropic.Anthropic()

   response = claude.messages.create(
       model="claude-3-7-sonnet-latest",
       max_tokens=1024,  # Required by Anthropic
       messages=[
           {"role": "user", "content": "Explain agentic AI in one sentence."}
       ]
   )

   answer = response.content[0].text

**Google Gemini**, **DeepSeek**, and **Groq** all support OpenAI-compatible endpoints:

.. code-block:: python

   # Google Gemini using OpenAI client
   gemini = OpenAI(
       api_key=os.getenv("GOOGLE_API_KEY"),
       base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
   )

   # DeepSeek using OpenAI client
   deepseek = OpenAI(
       api_key=os.getenv("DEEPSEEK_API_KEY"),
       base_url="https://api.deepseek.com"
   )

   # Groq using OpenAI client
   groq = OpenAI(
       api_key=os.getenv("GROQ_API_KEY"),
       base_url="https://api.groq.com/openai/v1"
   )

Each then accepts standard `chat.completions.create()` calls. This convergence on OpenAI's patterns simplifies multi-model orchestration dramatically.

**Ollama** runs locally, exposing OpenAI-compatible endpoints on `localhost`:

.. code-block:: python

   ollama = OpenAI(
       api_key="ollama",  # Placeholder, not validated
       base_url="http://localhost:11434/v1"
   )

   # Must have pulled model locally first
   # Terminal: ollama pull llama-3.2-3b

   response = ollama.chat.completions.create(
       model="llama-3.2-3b",
       messages=[{"role": "user", "content": "Hello!"}]
   )


Multi-Model Competition: A Practical Workflow
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Combining models implements multiple design patterns simultaneously:

1. **Orchestrator generates a challenging question** (autonomy in topic selection)
2. **Routing to multiple specialist models** for diverse answers (parallelization)
3. **Evaluator judges responses** (evaluator pattern)

.. code-block:: python

   from openai import OpenAI
   import anthropic
   import os

   # Initialize clients
   openai_client = OpenAI()
   claude_client = anthropic.Anthropic()
   gemini_client = OpenAI(
       api_key=os.getenv("GOOGLE_API_KEY"),
       base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
   )

   # Step 1: Generate question
   question_response = openai_client.chat.completions.create(
       model="gpt-4o-mini",
       messages=[{
           "role": "user",
           "content": "Propose a challenging question to assess AI intelligence. "
                      "Respond only with the question, no explanation."
       }]
   )
   question = question_response.choices[0].message.content
   print(f"Question: {question}\n")

   # Step 2: Collect answers from multiple models
   competitors = []
   answers = []

   messages = [{"role": "user", "content": question}]

   # GPT-4o-mini
   response = openai_client.chat.completions.create(
       model="gpt-4o-mini",
       messages=messages
   )
   competitors.append("GPT-4o-mini")
   answers.append(response.choices[0].message.content)

   # Claude 3.7 Sonnet
   response = claude_client.messages.create(
       model="claude-3-7-sonnet-latest",
       max_tokens=1024,
       messages=messages
   )
   competitors.append("Claude 3.7 Sonnet")
   answers.append(response.content[0].text)

   # Gemini 2.0 Flash
   response = gemini_client.chat.completions.create(
       model="gemini-2.0-flash",
       messages=messages
   )
   competitors.append("Gemini 2.0 Flash")
   answers.append(response.choices[0].message.content)

   # Step 3: Format for evaluation
   evaluation_text = f"You are judging a competition between {len(competitors)} AI models.\n\n"
   evaluation_text += f"Question: {question}\n\n"

   for i, (competitor, answer) in enumerate(zip(competitors, answers), 1):
       evaluation_text += f"Response from Competitor {i}:\n{answer}\n\n"

   evaluation_text += """Please rank the responses from best to worst.

   Respond in JSON format only:
   {
       "results": {
           "1": <best competitor number>,
           "2": <second best competitor number>,
           "3": <third best competitor number>
       }
   }

   Do not include markdown formatting or code blocks."""

   # Step 4: Judge with a reasoning model
   judge_response = openai_client.chat.completions.create(
       model="o3-mini",
       messages=[{
           "role": "user",
           "content": evaluation_text
       }]
   )

   import json
   rankings = json.loads(judge_response.choices[0].message.content)

   print("Rankings:")
   for rank, competitor_num in rankings["results"].items():
       print(f"{rank}. {competitors[int(competitor_num) - 1]}")

This exercise demonstrates:

- **Prompt chaining** (question generation â†’ answering â†’ evaluation)
- **Parallelization** (multiple models answering concurrently, if async implemented)
- **Routing** (selecting appropriate models for generation vs. evaluation tasks)
- **Orchestrator-worker** (evaluator synthesizing multiple outputs)

The pattern applies commercially to ensemble methods, quality assurance, and multi-perspective analysis.

**Lesson to Remember:** Direct API calls to multiple models require minimal code but maximum insight. Understanding native orchestration reveals what frameworks abstract and when that abstraction helps or hinders.


Day 4: Resources, Tools, and Structured Outputs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Models gain power through external knowledge (resources) and real-world interaction capabilities (tools). Understanding both unlocks practical agent construction.


Resources: Augmenting Model Knowledge
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Resources simply mean providing relevant context in promptsâ€”the foundation of Retrieval-Augmented Generation (RAG). Rather than relying on parametric knowledge alone, models receive documents, data, or instructions pertinent to the task.

**Simple Resource Pattern:**

.. code-block:: python

   import pypdf
   from openai import OpenAI

   # Load PDF resource
   reader = pypdf.PdfReader("linkedin_profile.pdf")
   linkedin_text = "\n".join([page.extract_text() for page in reader.pages])

   # Load additional context
   with open("summary.txt", "r") as f:
       summary = f.read()

   # Construct resource-enriched system prompt
   system_prompt = f"""You are acting as a professional assistant.
   You answer questions about career background, skills, and experience.

   # Summary
   {summary}

   # LinkedIn Profile
   {linkedin_text}

   With this context, please respond professionally to user questions."""

   # Use in conversation
   client = OpenAI()
   messages = [
       {"role": "system", "content": system_prompt},
       {"role": "user", "content": "What are the main career accomplishments?"}
   ]

   response = client.chat.completions.create(
       model="gpt-4o-mini",
       messages=messages
   )

   print(response.choices[0].message.content)

This pattern underlies chatbots, document Q&A systems, and specialized assistants. Advanced implementations employ vector databases, semantic search, and re-rankingâ€”topics covered extensively in LLM Engineering coursework but applied directly here.


Tools: The Heart of Agentic Behavior
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Tools enable models to transcend text generation, executing functions in response to understanding user intent. The mechanism proves surprisingly straightforwardâ€”more orchestration pattern than deep magic.

**The Tool Calling Mechanism:**

1. **Define function signatures in JSON** describing available tools
2. **Send JSON to model** alongside user query
3. **Model responds** either with answer OR request to call function
4. **Execute requested function** if model chose tool use
5. **Return results to model** for final response generation

The revelation: **tool calling reduces to JSON responses and conditional logic**. No remote execution occursâ€”models simply indicate desired actions through structured output.

**Tool Definition Example:**

.. code-block:: python

   # Define a tool: record user contact details
   def record_user_details(email: str, name: str = None, notes: str = None):
       """Record that a user wants to be contacted."""
       print(f"ðŸ“§ Recording interest from {email}")
       if name:
           print(f"   Name: {name}")
       if notes:
           print(f"   Notes: {notes}")
       return f"Recorded details for {email}"

   # Describe tool to model in JSON format
   tool_definition = {
       "type": "function",
       "function": {
           "name": "record_user_details",
           "description": "Use this tool to record that a user is interested in "
                          "being contacted and provided an email address.",
           "parameters": {
               "type": "object",
               "properties": {
                   "email": {
                       "type": "string",
                       "description": "User's email address"
                   },
                   "name": {
                       "type": "string",
                       "description": "User's name, if provided"
                   },
                   "notes": {
                       "type": "string",
                       "description": "Any additional context about the user's interest"
                   }
               },
               "required": ["email"],
               "additionalProperties": False
           }
       }
   }

**Using Tools in Conversations:**

.. code-block:: python

   from openai import OpenAI
   import json

   client = OpenAI()

   messages = [
       {"role": "system", "content": "You are a helpful career assistant."},
       {"role": "user", "content": "I'd like to get in touch. My email is user@example.com"}
   ]

   # Call model with tool availability
   response = client.chat.completions.create(
       model="gpt-4o-mini",
       messages=messages,
       tools=[tool_definition]
   )

   # Check if model wants to use tool
   message = response.choices[0].message

   if message.tool_calls:
       # Model requested tool execution
       for tool_call in message.tool_calls:
           function_name = tool_call.function.name
           function_args = json.loads(tool_call.function.arguments)

           print(f"Model requested: {function_name}({function_args})")

           # Execute the function
           if function_name == "record_user_details":
               result = record_user_details(**function_args)

               # Add function result to conversation
               messages.append({
                   "role": "tool",
                   "tool_call_id": tool_call.id,
                   "content": result
               })

       # Call model again with function results
       final_response = client.chat.completions.create(
           model="gpt-4o-mini",
           messages=messages,
           tools=[tool_definition]
       )

       print(final_response.choices[0].message.content)
   else:
       # Model provided direct answer
       print(message.content)

The pattern generalizes: define functions Python-side, describe them JSON-side, conditionally execute based on model responses. Frameworks automate this choreography, but understanding the underlying dance proves essential.


Structured Outputs: Validating Responses
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Structured outputs ensure models return data in precise formatsâ€”critical for programmatic consumption and validation workflows.

**Pydantic Models Define Schemas:**

.. code-block:: python

   from pydantic import BaseModel
   from openai import OpenAI

   # Define response structure
   class Evaluation(BaseModel):
       is_acceptable: bool
       feedback: str

   client = OpenAI()

   # Request structured output
   response = client.beta.chat.completions.parse(
       model="gpt-4o-mini",
       messages=[
           {"role": "system", "content": "You evaluate answer quality."},
           {"role": "user", "content": "Evaluate: 'The sky is blue because of molecules.'"}
       ],
       response_format=Evaluation
   )

   # Response automatically parsed into Pydantic model
   evaluation = response.choices[0].message.parsed

   print(f"Acceptable: {evaluation.is_acceptable}")
   print(f"Feedback: {evaluation.feedback}")

Structured outputs prove analogous to tool callsâ€”both involve JSON schemas, model responses conforming to specifications, and programmatic handling. This symmetry becomes apparent when implementing evaluator-optimizer patterns.

**Evaluator-Optimizer with Structured Outputs:**

.. code-block:: python

   from pydantic import BaseModel
   from openai import OpenAI

   class Evaluation(BaseModel):
       is_acceptable: bool
       feedback: str

   def generate_answer(question: str) -> str:
       """Generator LLM creates initial answer."""
       client = OpenAI()
       response = client.chat.completions.create(
           model="gpt-4o-mini",
           messages=[{"role": "user", "content": question}]
       )
       return response.choices[0].message.content

   def evaluate_answer(answer: str, question: str) -> Evaluation:
       """Evaluator LLM validates answer quality."""
       client = OpenAI()

       messages = [
           {"role": "system", "content": "You evaluate answer professionalism and accuracy."},
           {"role": "user", "content": f"Question: {question}\n\nAnswer: {answer}\n\nEvaluate this response."}
       ]

       response = client.beta.chat.completions.parse(
           model="gpt-4o-mini",
           messages=messages,
           response_format=Evaluation
       )

       return response.choices[0].message.parsed

   def regenerate_with_feedback(question: str, previous_answer: str, feedback: str) -> str:
       """Generator LLM retries with evaluator feedback."""
       client = OpenAI()

       messages = [{
           "role": "user",
           "content": f"Previous answer was rejected.\n\n"
                      f"Question: {question}\n"
                      f"Rejected answer: {previous_answer}\n"
                      f"Feedback: {feedback}\n\n"
                      f"Please provide an improved answer."
       }]

       response = client.chat.completions.create(
           model="gpt-4o-mini",
           messages=messages
       )

       return response.choices[0].message.content

   # Complete workflow with retry logic
   def generate_validated_answer(question: str, max_retries: int = 2) -> str:
       """Generate answer with evaluation loop."""
       answer = generate_answer(question)

       for attempt in range(max_retries):
           evaluation = evaluate_answer(answer, question)

           if evaluation.is_acceptable:
               print(f"âœ“ Answer accepted on attempt {attempt + 1}")
               return answer
           else:
               print(f"âœ— Answer rejected: {evaluation.feedback}")
               answer = regenerate_with_feedback(question, answer, evaluation.feedback)

       # Return final attempt even if not validated
       return answer

This pattern appears throughout production systemsâ€”validation gates ensuring quality before expensive downstream processing or user exposure.

**Lesson to Remember:** Resources provide knowledge, tools provide capabilities, and structured outputs provide guarantees. Together, they transform language models from text generators into reliable system components.


Day 5: Building a Production Agent
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Theory crystallizes in deployment. The week culminates in a fully functional Career Digital Twinâ€”a conversational agent representing professional experience, deployed to production and embeddable in personal websites.


Project Architecture
^^^^^^^^^^^^^^^^^^^^

The system combines multiple techniques into cohesive functionality:

**Resources:** LinkedIn profile PDF and career summary provide knowledge base

**Tools:** Two functions enable real-world interaction:
- `record_user_details(email, name, notes)` - captures contact information
- `record_unknown_question(question)` - logs unanswerable queries

**Structured Outputs:** Evaluation system validates response professionalism

**Gradio UI:** Web interface with minimal code

**Pushover Integration:** Real-time mobile notifications for tool executions

**Deployment:** Hugging Face Spaces for production hosting


Implementing Tool Execution
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The complete tool-calling implementation requires handling the request-response cycle:

.. code-block:: python

   import os
   import json
   from openai import OpenAI
   from typing import List, Dict
   import requests

   # Tool functions
   def record_user_details(email: str, name: str = None, notes: str = None) -> str:
       """Record user contact information with push notification."""
       message = f"ðŸ“§ Contact request from {email}"
       if name:
           message += f"\nName: {name}"
       if notes:
           message += f"\nNotes: {notes}"

       # Send push notification via Pushover
       requests.post("https://api.pushover.net/1/messages.json", data={
           "token": os.getenv("PUSHOVER_TOKEN"),
           "user": os.getenv("PUSHOVER_USER"),
           "message": message
       })

       return f"Thank you! Contact details recorded for {email}."

   def record_unknown_question(question: str) -> str:
       """Log question that couldn't be answered."""
       message = f"â“ Question I couldn't answer:\n{question}"

       requests.post("https://api.pushover.net/1/messages.json", data={
           "token": os.getenv("PUSHOVER_TOKEN"),
           "user": os.getenv("PUSHOVER_USER"),
           "message": message
       })

       return "I've noted your question. I'll provide an answer soon."

   # Tool definitions for OpenAI
   TOOLS = [
       {
           "type": "function",
           "function": {
               "name": "record_user_details",
               "description": "Use this tool to record that a user is interested in "
                              "being contacted and provided an email address.",
               "parameters": {
                   "type": "object",
                   "properties": {
                       "email": {"type": "string", "description": "User's email address"},
                       "name": {"type": "string", "description": "User's name, if provided"},
                       "notes": {"type": "string", "description": "Context about user's interest"}
                   },
                   "required": ["email"],
                   "additionalProperties": False
               }
           }
       },
       {
           "type": "function",
           "function": {
               "name": "record_unknown_question",
               "description": "Use this tool to record questions you cannot answer.",
               "parameters": {
                   "type": "object",
                   "properties": {
                       "question": {"type": "string", "description": "The question that couldn't be answered"}
                   },
                   "required": ["question"],
                   "additionalProperties": False
               }
           }
       }
   ]

   # Dynamic function execution
   def handle_tool_calls(tool_calls) -> List[Dict]:
       """Execute requested tools and return results."""
       results = []

       for tool_call in tool_calls:
           function_name = tool_call.function.name
           function_args = json.loads(tool_call.function.arguments)

           print(f"ðŸ”§ Calling tool: {function_name}")

           # Dynamic function lookup and execution
           function_result = globals()[function_name](**function_args)

           results.append({
               "role": "tool",
               "tool_call_id": tool_call.id,
               "content": function_result
           })

       return results

The `handle_tool_calls` function demonstrates dynamic dispatchâ€”using `globals()` to look up functions by name eliminates verbose if-elif chains. This pattern scales to arbitrary tool counts.


The Complete Chat Loop
^^^^^^^^^^^^^^^^^^^^^^^

Bringing together resources, tools, and conversation flow:

.. code-block:: python

   import pypdf
   from openai import OpenAI

   class CareerAgent:
       def __init__(self, linkedin_pdf_path: str, summary_path: str, name: str):
           """Initialize agent with resource documents."""
           # Load resources
           reader = pypdf.PdfReader(linkedin_pdf_path)
           self.linkedin_text = "\n".join([page.extract_text() for page in reader.pages])

           with open(summary_path, "r") as f:
               self.summary = f.read()

           self.name = name
           self.client = OpenAI()

           # Construct system prompt
           self.system_prompt = f"""You are acting as {self.name}.
           You answer questions on their website, particularly related to career
           background, skills, and experience.

           Your responsibility is to represent {self.name} faithfully and professionally.

           # Summary
           {self.summary}

           # LinkedIn Profile
           {self.linkedin_text}

           If you don't know the answer to any question, use your tool to record it.
           If the user engages positively, encourage them to get in touch via email
           and record their details using your tool.
           """

       def chat(self, message: str, history: List[Dict]) -> str:
           """Handle one conversation turn with tool support."""
           # Build message list
           messages = [{"role": "system", "content": self.system_prompt}]
           messages.extend(history)
           messages.append({"role": "user", "content": message})

           # Loop until model provides final answer (not tool calls)
           done = False
           while not done:
               response = self.client.chat.completions.create(
                   model="gpt-4o-mini",
                   messages=messages,
                   tools=TOOLS
               )

               response_message = response.choices[0].message
               finish_reason = response.choices[0].finish_reason

               if finish_reason == "tool_calls":
                   # Model wants to execute tools
                   messages.append(response_message)

                   # Execute tools and add results
                   tool_results = handle_tool_calls(response_message.tool_calls)
                   messages.extend(tool_results)

                   # Loop continues for final response
               else:
                   # Model provided final answer
                   done = True
                   return response_message.content

The `while not done` loop handles iterative tool callsâ€”models may execute multiple tools before generating final responses.


Gradio Deployment Interface
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Gradio transforms Python functions into web applications with minimal code:

.. code-block:: python

   import gradio as gr

   # Initialize agent
   agent = CareerAgent(
       linkedin_pdf_path="me/linkedin.pdf",
       summary_path="me/summary.txt",
       name="Your Name"
   )

   # Create web interface
   interface = gr.ChatInterface(
       fn=agent.chat,
       title="Career Conversation",
       description="Ask me about my professional background and experience.",
       examples=[
           "What is your current role?",
           "Tell me about your biggest accomplishment.",
           "What technologies do you specialize in?"
       ],
       theme=gr.themes.Soft()
   )

   # Launch locally
   interface.launch()

Running `uv run app.py` starts a local server. Gradio provides a shareable URL for temporary public access.


Production Deployment to Hugging Face Spaces
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Hugging Face Spaces offers free hosting for Gradio applications. Deployment requires a single command:

.. code-block:: bash

   # Navigate to project directory
   cd week-1-foundations

   # Deploy to Hugging Face
   gradio deploy

The CLI prompts for:

- **Space title:** `career-conversation`
- **App file:** `app.py`
- **Hardware:** `cpu-basic` (free tier)
- **Secrets:**
  - `OPENAI_API_KEY`: Your OpenAI API key
  - `PUSHOVER_USER`: Your Pushover user ID
  - `PUSHOVER_TOKEN`: Your Pushover application token

The application deploys to `https://huggingface.co/spaces/{username}/career-conversation`, fully functional and embeddable via iframe in personal websites.


**Embedding in Personal Websites:**

.. code-block:: html

   <iframe
       src="https://huggingface.co/spaces/{username}/career-conversation"
       width="100%"
       height="600px"
       frameborder="0"
   ></iframe>

Visitors interact with the agent directly on the personal website, with tool executions notifying via mobile push notifications.


Extensions and Improvements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The baseline implementation invites numerous enhancements:

**1. Enhanced Resources:**
- Multiple documents (projects, publications, testimonials)
- Structured data (JSON-formatted achievements, skills taxonomy)
- Vector database integration for semantic retrieval (Chroma, Pinecone)

**2. Additional Tools:**
- `schedule_meeting()` - Calendar API integration
- `query_portfolio()` - SQL database of project details
- `fetch_blog_post()` - Dynamic content retrieval
- `record_feedback()` - Visitor sentiment tracking

**3. Evaluation System:**
Reintegrate the evaluator-optimizer pattern from Day 4:

.. code-block:: python

   from pydantic import BaseModel

   class ResponseEvaluation(BaseModel):
       is_professional: bool
       is_accurate: bool
       feedback: str

   def evaluate_response(response: str, question: str) -> ResponseEvaluation:
       """Validate response quality before sending."""
       evaluator_client = OpenAI(
           api_key=os.getenv("GOOGLE_API_KEY"),
           base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
       )

       messages = [
           {"role": "system", "content": "You evaluate career-related responses for "
                                          "professionalism, accuracy, and engagement."},
           {"role": "user", "content": f"Question: {question}\n\nResponse: {response}"}
       ]

       result = evaluator_client.beta.chat.completions.parse(
           model="gemini-2.0-flash",
           messages=messages,
           response_format=ResponseEvaluation
       )

       return result.choices[0].message.parsed

**4. Streaming Responses:**

.. code-block:: python

   def chat_stream(self, message: str, history: List[Dict]):
       """Stream response tokens for improved UX."""
       messages = self._build_messages(history, message)

       stream = self.client.chat.completions.create(
           model="gpt-4o-mini",
           messages=messages,
           tools=TOOLS,
           stream=True
       )

       full_response = ""
       for chunk in stream:
           if chunk.choices[0].delta.content:
               token = chunk.choices[0].delta.content
               full_response += token
               yield token

**5. Analytics Dashboard:**
- Track question categories
- Monitor tool usage patterns
- Identify knowledge gaps requiring resource expansion

**Lesson to Remember:** Production systems emerge from combining simple patternsâ€”resources, tools, validation, and deployment. Each component proves straightforward individually; their integration creates commercial value. The Career Digital Twin demonstrates how foundational techniques scale to deployable applications.


Week 1 Key Takeaways
---------------------

The foundation week establishes core competencies without framework dependencies:

**Conceptual Mastery:**

- Agentic systems exist on a spectrum from rigid workflows to dynamic agents
- Five design patterns (prompt chaining, routing, parallelization, orchestrator-worker, evaluator-optimizer) cover most architectures
- Autonomy means granting models discretion over workflow pathsâ€”powerful but requiring guardrails

**Technical Implementation:**

- Native LLM orchestration through direct API calls reveals what frameworks abstract
- Tool calling reduces to JSON schemas and conditional logicâ€”mystique dissolves into mechanism
- Structured outputs provide type-safe, validated responses essential for production reliability
- Resources augment model knowledge without fine-tuning or RAG complexity

**Production Considerations:**

- Monitoring and guardrails mitigate unpredictability inherent in agent systems
- Evaluation loops significantly improve output quality and consistency
- Modern tooling (Cursor, UV, Gradio) dramatically accelerates development velocity
- Multiple deployment paths exist (Hugging Face Spaces, cloud platforms, local hosting)

**Commercial Patterns:**

- Career assistants, support agents, and research tools all follow similar architectures
- Tool-enabled agents transcend information provision to execute actions
- Multi-model ensembles balance cost, capability, and reliability
- Validation patterns build confidence required for customer-facing deployments

The subsequent weeks layer frameworks atop this foundationâ€”OpenAI's elegant SDK, CrewAI's low-code approach, LangGraph's computational power, Autogen's collaborative environment, and MCP's protocol-based integration. Each abstracts these core patterns differently, and understanding the underlying mechanics enables informed framework selection.


Looking Ahead: Framework Exploration
-------------------------------------

**Week 2 - OpenAI Agents SDK:**

Lightweight, Pythonic agent creation with built-in tracing and guardrail support. The SDR Agent project demonstrates professional email generation with brand consistency enforcement.

**Week 3 - CrewAI:**

Configuration-driven multi-agent systems enabling rapid prototyping. The Engineering Team project showcases specialized agents (manager, developers, tester) collaborating on software development.

**Week 4 - LangGraph:**

Computational graph abstraction for complex, stateful workflows. The Browser Sidekick project implements an agent operating alongside users in web environments.

**Week 5 - Autogen:**

Microsoft's framework for agent collaboration and meta-agent patterns. The Agent Creator project demonstrates agents that design and deploy other agents.

**Week 6 - MCP & Capstone:**

Anthropic's Model Context Protocol enables open-source tool and data sharing. The Trading Floor capstone synthesizes all frameworks: four autonomous trading agents with 44 tools across 6 MCP servers, making real-time market decisions.


Resources & Further Exploration
--------------------------------

**Official Course Materials:**

- GitHub Repository: https://github.com/ed-donner/agents

  - `/guides/` - Self-study materials for Python, async, debugging
  - `/week-1-foundations/` - Complete code for all Day 1-5 projects
  - `/community-contributions/` - Student project submissions
  - `/troubleshooting.md` - Common issues and solutions

- Course Website: https://edwarddonner.com/2025/04/21/the-complete-agentic-ai-engineering-course/

**Key Documentation:**

- Anthropic: "Building Effective Agents" - https://www.anthropic.com/research/building-effective-agents
- OpenAI: Function Calling Guide - https://platform.openai.com/docs/guides/function-calling
- Vellum AI Leaderboard - https://www.vellum.ai/llm-leaderboard
- Gradio Documentation - https://www.gradio.app/docs
- UV Package Manager - https://github.com/astral-sh/uv

**Development Tools:**

- Cursor IDE - https://cursor.sh
- Pushover Push Notifications - https://pushover.net
- Hugging Face Spaces - https://huggingface.co/spaces
- Ollama Local Models - https://ollama.ai

**Recommended Background:**

For practitioners new to LLM engineering, the companion course "Open-Source LLMs: LLM Engineering" provides comprehensive coverage of prompt engineering, RAG systems, embeddings, vector databases, and fine-tuningâ€”complementary skills that enhance agent development.


Community & Support
-------------------

Learning proves most effective within supportive communities. Multiple channels exist for questions, collaboration, and showcase:

**Direct Support:**

- Instructor email for technical questions and roadblock resolution
- LinkedIn connection for professional networking and project amplification
- GitHub Issues for bug reports and feature requests

**Contribution Pathways:**

- Pull requests to `/community-contributions/` with novel implementations
- LinkedIn posts demonstrating projects (tag instructor for visibility)
- GitHub Stars and documentation improvements

**Collaboration Opportunities:**

- Peer code review through pull request comments
- Shared debugging in discussion forums
- Joint projects building on course frameworks

The agentic AI field evolves rapidly. Active engagement ensures continued learning beyond course completion, access to emerging patterns, and visibility within the growing practitioner community.


Conclusion: Foundation Established
----------------------------------

Week 1 establishes the bedrockâ€”direct experience with LLM orchestration absent framework magic. The temptation exists to leap immediately into high-level abstractions, trading understanding for velocity. Resistance proves worthwhile.

Frameworks come and go. New platforms emerge, APIs change, abstractions shift with technological evolution. Underlying principles endure: models generate text probabilistically, tools execute through conditional logic, validation improves reliability, resources provide context.

Mastery begins with fundamentals. The Career Digital Twin, built without frameworks using resources and tools, stands as proofâ€”a production application derived from first principles. This foundation supports all subsequent framework learning, enabling critical evaluation of when abstractions help versus hinder.

The journey now advances to specialization: five frameworks, seven remaining projects, and the capstone synthesis. Each week builds capability methodically, expanding the toolkit for commercial agent deployment.

The first week concludes. The path forward beckons. Autonomous agents await construction.


---

*Note: This document represents active course notes, continuously updated as new content, troubleshooting guidance, and student contributions emerge. Bookmark the GitHub repository for latest materials and community projects.*
